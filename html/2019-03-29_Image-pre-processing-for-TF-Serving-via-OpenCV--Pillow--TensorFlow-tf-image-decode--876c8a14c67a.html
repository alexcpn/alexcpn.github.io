<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Image pre-processing for TF Serving via OpenCV, Pillow, TensorFlow tf.image.decode*</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Image pre-processing for TF Serving via OpenCV, Pillow, TensorFlow tf.image.decode*</h1>
</header>
<section data-field="subtitle" class="p-summary">
Example Input Images
</section>
<section data-field="body" class="e-content">
<section name="198a" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="eb1e" id="eb1e" class="graf graf--h3 graf--leading graf--title">Image pre-processing for TF Serving via OpenCV, Pillow, TensorFlow tf.image.decode* , Keras-Retinanet Example</h3><p name="8de3" id="8de3" class="graf graf--p graf-after--h3">Example Input Images</p><p name="e61a" id="e61a" class="graf graf--p graf-after--p">(all images are attributed to their owners, just showing here for technical demo)</p><figure name="b8e8" id="b8e8" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 640px; max-height: 480px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 75%;"></div><img class="graf-image" data-image-id="1*G1brFgvQ7g13wHtl54pvqg.jpeg" data-width="640" data-height="480" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*G1brFgvQ7g13wHtl54pvqg.jpeg"></div><figcaption class="imageCaption">640*480</figcaption></figure><figure name="2b54" id="2b54" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 464px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 66.3%;"></div><img class="graf-image" data-image-id="1*00tv7Orh6I0MQF0_WoWQWA.jpeg" data-width="1234" data-height="818" src="https://cdn-images-1.medium.com/max/800/1*00tv7Orh6I0MQF0_WoWQWA.jpeg"></div><figcaption class="imageCaption">1234*818</figcaption></figure><figure name="ffd1" id="ffd1" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 502px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 71.7%;"></div><img class="graf-image" data-image-id="1*KCJq8L6-lQDUWRQRcJT2vw.jpeg" data-width="1280" data-height="918" src="https://cdn-images-1.medium.com/max/800/1*KCJq8L6-lQDUWRQRcJT2vw.jpeg"></div><figcaption class="imageCaption">1280*918</figcaption></figure><figure name="e2a7" id="e2a7" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 565px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 80.7%;"></div><img class="graf-image" data-image-id="1*RDLG7v_mTiCfiCwqsgmMUg.jpeg" data-width="8400" data-height="6777" src="https://cdn-images-1.medium.com/max/800/1*RDLG7v_mTiCfiCwqsgmMUg.jpeg"></div><figcaption class="imageCaption">8400*6777</figcaption></figure><h4 name="d7c5" id="d7c5" class="graf graf--h4 graf-after--figure">Loaded via retinanet_helper</h4><pre name="853e" id="853e" class="graf graf--pre graf-after--h4"># for reitnanet image processing<br>from keras.preprocessing import image<br>from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image<br>from keras_retinanet.utils.visualization import draw_box, draw_caption<br>from keras_retinanet.utils.colors import label_color</pre><pre name="6fd8" id="6fd8" class="graf graf--pre graf-after--pre">def decode_image_retinanet(img_path):<br>  ## Going to read the image via retinanet helper- the original way<br>  start = timer()<br>  image = read_image_bgr(img_path)<br>  # preprocess image for network<br>  image = preprocess_image(image)<br>  image, scale = resize_image(image)<br>  end = timer()<br>  print(&quot;decode time=&quot;,end - start)<br>  #(&#39;decode time=&#39;, 0.028119802474975586)<br>  # these are the best scores<br>  #(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([409, 167, 728, 603]), &#39; Score &#39;, 0.9681119)  <br>  #(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([  0, 426, 512, 785]), &#39; Score &#39;, 0.8355836)<br>  #(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([ 723,  475, 1067,  791]), &#39; Score &#39;, 0.72344124)<br>  #(&#39;Label&#39;, &#39;tie&#39;, &#39; at &#39;, array([527, 335, 569, 505]), &#39; Score &#39;, 0.525432)<br>  return image,image</pre><figure name="9d15" id="9d15" class="graf graf--figure graf-after--pre"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 525px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 75%;"></div><img class="graf-image" data-image-id="1*846aAa_t7wBBEWY2dcyaIA.png" data-width="1067" data-height="800" src="https://cdn-images-1.medium.com/max/800/1*846aAa_t7wBBEWY2dcyaIA.png"></div></figure><p name="ddc7" id="ddc7" class="graf graf--p graf-after--figure">Output</p><pre name="a1e6" id="a1e6" class="graf graf--pre graf-after--p">(‘Number of test=’, 1)<br>(‘decode time=’, 0.028119802474975586)<br>(‘in image shape’, (800, 1067, 3))<br>(‘Input shape=’, (1, 800, 1067, 3))<br>(‘in tf shape’, (1, 800, 1067, 3))<br>(‘result no’, 0)<br>(‘boxes output’, (1, 300, 4))<br>(‘scores output’, (1, 300))<br>(‘labels output’, (1, 300))<br>(‘Label’, ‘person’, ‘ at ‘, array([409, 167, 728, 603]), ‘ Score ‘, 0.9681119)<br>(‘Label’, ‘person’, ‘ at ‘, array([ 0, 426, 512, 785]), ‘ Score ‘, 0.8355836)<br>(‘Label’, ‘person’, ‘ at ‘, array([ 723, 475, 1067, 791]), ‘ Score ‘, 0.72344124)<br>(‘Label’, ‘tie’, ‘ at ‘, array([527, 335, 569, 505]), ‘ Score ‘, 0.525432)<br>(‘Time for ‘, 1, ‘ is ‘, 0.8878581523895264)</pre><figure name="25ab" id="25ab" class="graf graf--figure graf-after--pre"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 502px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 71.7%;"></div><img class="graf-image" data-image-id="1*2FXdsCndKJar_0FJj9TzwQ.png" data-width="1115" data-height="800" src="https://cdn-images-1.medium.com/max/800/1*2FXdsCndKJar_0FJj9TzwQ.png"></div></figure><figure name="9471" id="9471" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 464px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 66.3%;"></div><img class="graf-image" data-image-id="1*ZvEaJ7FS0nrRAjAwFrwRnw.png" data-width="1207" data-height="800" src="https://cdn-images-1.medium.com/max/800/1*ZvEaJ7FS0nrRAjAwFrwRnw.png"></div></figure><figure name="157a" id="157a" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 565px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 80.60000000000001%;"></div><img class="graf-image" data-image-id="1*UgdYX99Phneld9_KCHNzCw.png" data-width="992" data-height="800" src="https://cdn-images-1.medium.com/max/800/1*UgdYX99Phneld9_KCHNzCw.png"></div></figure><h4 name="ac22" id="ac22" class="graf graf--h4 graf-after--figure">Load via OpenCV</h4><p name="df5a" id="df5a" class="graf graf--p graf-after--h4"><strong class="markup--strong markup--p-strong">Code</strong></p><p name="f7f6" id="f7f6" class="graf graf--p graf-after--p">Note OpenCV reads in BRG format — with swapRB — it changes to RGB format. This causes variations — found this setting good for Keras Retinanet model overall</p><p name="75fa" id="75fa" class="graf graf--p graf-after--p"><a href="http://answers.opencv.org/question/174139/is-the-swaprb-value-in-the-example-googlenet-dnn-code-wrong/" data-href="http://answers.opencv.org/question/174139/is-the-swaprb-value-in-the-example-googlenet-dnn-code-wrong/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">http://answers.opencv.org/question/174139/is-the-swaprb-value-in-the-example-googlenet-dnn-code-wrong/</a></p><pre name="5b9c" id="5b9c" class="graf graf--pre graf-after--p">def decode_image_opencv(img_path):<br>  ### Going to create image vector via OpenCV<br>  #todo <a href="https://docs.nvidia.com/deeplearning/sdk/dali-developer-guide/docs/examples/getting%20started.html" data-href="https://docs.nvidia.com/deeplearning/sdk/dali-developer-guide/docs/examples/getting%20started.html" class="markup--anchor markup--pre-anchor" rel="nofollow noopener" target="_blank">https://docs.nvidia.com/deeplearning/sdk/dali-developer-guide/docs/examples/getting%20started.html</a><br>  start = timer()<br>  image = cv2.imread(img_path,1)<br>  image = image_resize(image,height=800)<br>  org  = image<br>  image = cv2.dnn.blobFromImage(image, scalefactor=1.0,mean=(103.939, 116.779, 123.68), swapRB=<strong class="markup--strong markup--pre-strong">True</strong>)<br>  # this gives   shape as  (1, 3, 480, 640))<br>  # we need it as (&#39;Input shape=&#39;, (1, 480, 640, 3))<br>  image = np.transpose(image, (0, 2, 3, 1))<br>  #image = image.astype(&#39;f&#39;)<br>  #image = image -127.5<br>  end = timer()<br>  print(&quot;decode time=&quot;,end - start)<br>  #&#39;decode time=&#39;, 0.007803916931152344)<br>  return image,org</pre><pre name="44b6" id="44b6" class="graf graf--pre graf-after--pre">#<a href="https://stackoverflow.com/a/44659589/429476" data-href="https://stackoverflow.com/a/44659589/429476" class="markup--anchor markup--pre-anchor" rel="nofollow noopener" target="_blank">https://stackoverflow.com/a/44659589/429476</a><br>def image_resize(image, width = None, height = None, inter = cv2.INTER_AREA):<br>    # initialize the dimensions of the image to be resized and<br>    # grab the image size<br>    dim = None<br>    (h, w) = image.shape[:2]</pre><pre name="bc3e" id="bc3e" class="graf graf--pre graf-after--pre"># if both the width and height are None, then return the<br>    # original image<br>    if width is None and height is None:<br>        return image</pre><pre name="ed55" id="ed55" class="graf graf--pre graf-after--pre"># check to see if the width is None<br>    if width is None:<br>        # calculate the ratio of the height and construct the<br>        # dimensions<br>        r = height / float(h)<br>        dim = (int(w * r), height)</pre><pre name="ffad" id="ffad" class="graf graf--pre graf-after--pre"># otherwise, the height is None<br>    else:<br>        # calculate the ratio of the width and construct the<br>        # dimensions<br>        r = width / float(w)<br>        dim = (width, int(h * r))</pre><pre name="f451" id="f451" class="graf graf--pre graf-after--pre"># resize the image<br>    resized = cv2.resize(image, dim, interpolation = inter)</pre><pre name="3f59" id="3f59" class="graf graf--pre graf-after--pre"># return the resized image<br>    return resized</pre><p name="b12a" id="b12a" class="graf graf--p graf-after--pre">Output</p><pre name="f57b" id="f57b" class="graf graf--pre graf-after--p">&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([590, 368, 649, 501]), &#39; Score &#39;, 0.8739606)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([457, 362, 503, 504]), &#39; Score &#39;, 0.8183074)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([532, 352, 568, 445]), &#39; Score &#39;, 0.804338)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([500, 354, 535, 439]), &#39; Score &#39;, 0.5898186)<br>(&#39;Label&#39;, &#39;dog&#39;, &#39; at &#39;, array([375, 455, 394, 490]), &#39; Score &#39;, 0.46877873)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([897, 335, 916, 371]), &#39; Score &#39;, 0.43948865)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([926, 335, 943, 375]), &#39; Score &#39;, 0.4195389)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([529, 301, 544, 344]), &#39; Score &#39;, 0.30523297)<br>(&#39;Time for &#39;, 1, &#39; is &#39;, 13.235145092010498)</pre><p name="8849" id="8849" class="graf graf--p graf-after--pre">With RGB switched</p><figure name="d2bb" id="d2bb" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 464px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 66.3%;"></div><img class="graf-image" data-image-id="1*6s1T3ayOsHuMzWUXqgqpnQ.png" data-width="1206" data-height="800" src="https://cdn-images-1.medium.com/max/800/1*6s1T3ayOsHuMzWUXqgqpnQ.png"></div></figure><p name="28cd" id="28cd" class="graf graf--p graf-after--figure">With RGB not switched</p><figure name="9725" id="9725" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 502px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 71.7%;"></div><img class="graf-image" data-image-id="1*rb5qtW_nZswyN_4_AiwCkw.png" data-width="1115" data-height="800" src="https://cdn-images-1.medium.com/max/800/1*rb5qtW_nZswyN_4_AiwCkw.png"></div></figure><p name="156b" id="156b" class="graf graf--p graf-after--figure">With RGB switched</p><figure name="b398" id="b398" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 502px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 71.7%;"></div><img class="graf-image" data-image-id="1*s7GtYU8vyemTa0QBY7seQg.png" data-width="1115" data-height="800" src="https://cdn-images-1.medium.com/max/800/1*s7GtYU8vyemTa0QBY7seQg.png"></div></figure><p name="4a5b" id="4a5b" class="graf graf--p graf-after--figure">With RGB not switched</p><figure name="1341" id="1341" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 565px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 80.7%;"></div><img class="graf-image" data-image-id="1*4OOXXFGSH_tOKQR5_PxcWQ.png" data-width="991" data-height="800" src="https://cdn-images-1.medium.com/max/800/1*4OOXXFGSH_tOKQR5_PxcWQ.png"></div></figure><p name="e7d9" id="e7d9" class="graf graf--p graf-after--figure">With RGB switched</p><figure name="f897" id="f897" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 565px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 80.7%;"></div><img class="graf-image" data-image-id="1*iW3_nGpsWJCZWLvHsPVSwQ.png" data-width="991" data-height="800" src="https://cdn-images-1.medium.com/max/800/1*iW3_nGpsWJCZWLvHsPVSwQ.png"></div></figure><h4 name="0e7a" id="0e7a" class="graf graf--h4 graf-after--figure">With TF</h4><pre name="9551" id="9551" class="graf graf--pre graf-after--h4">def decode_image_tf_reader(img_path):<br>  ## Going to read the image via TF helper</pre><pre name="2a6e" id="2a6e" class="graf graf--pre graf-after--pre">img_raw = tf.read_file(img_path)<br>  start = timer()<br>  img_tensor = tf.image.decode_jpeg(img_raw, channels=0,<br>        dct_method=&quot;INTEGER_FAST&quot;) #not much effect here decode time 30ms<br>  print(&quot;img_tensor.shape=&quot;,img_tensor.shape)<br>  image = tf.cast(img_tensor, tf.float32)<br>  <br>  #image = tf.image.resize_images(img_tensor, [800,1067])<br>  <br>  smallest_side = 480.0 # will losse some info<br>  height, width = tf.shape(image)[0], tf.shape(image)[1]<br>  height = tf.to_float(height)<br>  width = tf.to_float(width)<br>  <br>  scale = tf.cond(tf.greater(height, width),<br>                          lambda: smallest_side / width,<br>                          lambda: smallest_side / height)<br>  new_height = tf.to_int32(height * scale)<br>  new_width = tf.to_int32(width * scale)</pre><pre name="fe53" id="fe53" class="graf graf--pre graf-after--pre">image = tf.image.resize_images(image, [new_height, new_width])</pre><pre name="552f" id="552f" class="graf graf--pre graf-after--pre">#image = tf.image.resize_images(image, [800,1200])</pre><pre name="63ce" id="63ce" class="graf graf--pre graf-after--pre">#<a href="https://forums.fast.ai/t/how-is-vgg16-mean-calculated/4577/19" data-href="https://forums.fast.ai/t/how-is-vgg16-mean-calculated/4577/19" class="markup--anchor markup--pre-anchor" rel="nofollow noopener noopener" target="_blank">https://forums.fast.ai/t/how-is-vgg16-mean-calculated/4577/19</a><br>  VGG_MEAN = [123.68, 116.78, 103.94] # This is R-G-B for Imagenet<br>  #means = tf.reshape(tf.constant(VGG_MEAN), [1, 1, 3])<br>  image = image - means <br>  # swap to BGR    <br>  img_channel_swap = image[..., ::-1]<br>  image = tf.reverse(image, axis=[-1])<br>  #without the above preprocessing there is a miss of detection and change in weight<br>  image = tf.Session().run(image)<br>  #image = image[:, :, [2,1,0]] # swap channel from RGB to BGR<br>  end = timer()<br>  print(&quot;decode time=&quot;,end - start)<br>  #(&#39;decode time=&#39;, 0.032473087310791016)<br>  #(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([ 0, 252, 306, 470]), &#39; Score &#39;, 0.8479492)<br>  #(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([241, 97, 435, 365]), &#39; Score &#39;, 0.7183717)<br>  #(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([429, 287, 635, 475]), &#39; Score &#39;, 0.67711633)<br>  #(&#39;Label&#39;, &#39;tie&#39;, &#39; at &#39;, array([313, 199, 340, 311]), &#39; Score &#39;, 0.5820301<br>  return image,image</pre><figure name="169b" id="169b" class="graf graf--figure graf-after--pre"><div class="aspectRatioPlaceholder is-locked" style="max-width: 640px; max-height: 480px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 75%;"></div><img class="graf-image" data-image-id="1*CK3bcIc2KzhgCwICKMA-cQ.png" data-width="640" data-height="480" src="https://cdn-images-1.medium.com/max/800/1*CK3bcIc2KzhgCwICKMA-cQ.png"></div></figure><pre name="3703" id="3703" class="graf graf--pre graf-after--figure">(‘Number of test=’, 1)<br>(‘img_tensor.shape=’, TensorShape([Dimension(None), Dimension(None), Dimension(None)]))<br>2019–03–27 09:17:00.288596: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA<br>(‘decode time=’, 0.032473087310791016)<br>(‘in image shape’, (480, 640, 3))<br>(‘Input shape=’, (1, 480, 640, 3))<br>(‘in tf shape’, (1, 480, 640, 3))<br>(‘result no’, 0)<br>(‘boxes output’, (1, 300, 4))<br>(‘scores output’, (1, 300))<br>(‘labels output’, (1, 300))<br>(‘Label’, ‘person’, ‘ at ‘, array([ 0, 252, 306, 470]), ‘ Score ‘, 0.8479492)<br>(‘Label’, ‘person’, ‘ at ‘, array([241, 97, 435, 365]), ‘ Score ‘, 0.7183717)<br>(‘Label’, ‘person’, ‘ at ‘, array([429, 287, 635, 475]), ‘ Score ‘, 0.67711633)<br>(‘Label’, ‘tie’, ‘ at ‘, array([313, 199, 340, 311]), ‘ Score ‘, 0.5820301)<br>(‘Time for ‘, 1, ‘ is ‘, 0.7225308418273926)</pre><h4 name="a29a" id="a29a" class="graf graf--h4 graf-after--pre">Load via Pillow</h4><pre name="8d08" id="8d08" class="graf graf--pre graf-after--h4">### Going to create image vector via PIL</pre><pre name="00bd" id="00bd" class="graf graf--pre graf-after--pre">start = timer()</pre><pre name="d7e1" id="d7e1" class="graf graf--pre graf-after--pre">image = np.asarray(Image.open(img_path).convert(&#39;RGB&#39;))</pre><pre name="1fd1" id="1fd1" class="graf graf--pre graf-after--pre">image = image.astype(&#39;f&#39;)</pre><pre name="694f" id="694f" class="graf graf--pre graf-after--pre">end = timer()</pre><pre name="91b3" id="91b3" class="graf graf--pre graf-after--pre">print(&quot;decode time=&quot;,end - start)</pre><p name="5398" id="5398" class="graf graf--p graf-after--pre">Ouput</p><p name="499c" id="499c" class="graf graf--p graf-after--p">and without conversion as in</p><div name="c8ad" id="c8ad" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://github.com/fizyr/keras-retinanet/blob/master/keras_retinanet/utils/image.py" data-href="https://github.com/fizyr/keras-retinanet/blob/master/keras_retinanet/utils/image.py" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/fizyr/keras-retinanet/blob/master/keras_retinanet/utils/image.py"><strong class="markup--strong markup--mixtapeEmbed-strong">fizyr/keras-retinanet</strong><br><em class="markup--em markup--mixtapeEmbed-em">Keras implementation of RetinaNet object detection. - fizyr/keras-retinanet</em>github.com</a><a href="https://github.com/fizyr/keras-retinanet/blob/master/keras_retinanet/utils/image.py" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="cfbb018155bcbf2b3653cd5cbb12f878" data-thumbnail-img-id="0*5Z80h1MYsjvuqkKh" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*5Z80h1MYsjvuqkKh);"></a></div><pre name="d8e8" id="d8e8" class="graf graf--pre graf-after--mixtapeEmbed">#(&#39;decode time=&#39;, 0.023459911346435547)</pre><pre name="d341" id="d341" class="graf graf--pre graf-after--pre"># it is not able to detect one person object</pre><pre name="8f67" id="8f67" class="graf graf--pre graf-after--pre">#Label&#39;, &#39;person&#39;, &#39; at &#39;, array([  0, 258, 308, 473]), &#39; Score &#39;, 0.8666027)</pre><pre name="7e86" id="7e86" class="graf graf--pre graf-after--pre">#(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([239,  98, 427, 378]), &#39; Score &#39;, 0.745802)</pre><pre name="d2a2" id="d2a2" class="graf graf--pre graf-after--pre graf--trailing">#(&#39;Label&#39;, &#39;tie&#39;, &#39; at &#39;, array([312, 199, 340, 314]), &#39; Score &#39;, 0.7037207)</pre></div></div></section><section name="0d70" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="48f9" id="48f9" class="graf graf--h3 graf--leading">How increase in image size aids detection (and takes more resources and time)</h3><p name="5515" id="5515" class="graf graf--p graf-after--h3">Original image 8400*6777</p><figure name="a701" id="a701" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 565px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 80.7%;"></div><img class="graf-image" data-image-id="1*RDLG7v_mTiCfiCwqsgmMUg.jpeg" data-width="8400" data-height="6777" src="https://cdn-images-1.medium.com/max/800/1*RDLG7v_mTiCfiCwqsgmMUg.jpeg"></div></figure><p name="9774" id="9774" class="graf graf--p graf-after--figure">Detection by Retinanet loader Pillow + image normalization</p><figure name="d6ef" id="d6ef" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 565px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 80.60000000000001%;"></div><img class="graf-image" data-image-id="1*eDPg3SDLFP56Jx9TJtVwHA.png" data-width="992" data-height="800" src="https://cdn-images-1.medium.com/max/800/1*eDPg3SDLFP56Jx9TJtVwHA.png"></div></figure><pre name="3561" id="3561" class="graf graf--pre graf-after--figure">(&#39;Number of test=&#39;, 1)<br>(&#39;decode time=&#39;, 2.9874989986419678)<br>(&#39;in image shape&#39;, (800, 992, 3))<br>(&#39;Input shape=&#39;, (1, 800, 992, 3))<br>(&#39;in tf shape&#39;, (1, 800, 992, 3))<br>(&#39;result no&#39;, 0)<br>(&#39;boxes output&#39;, (1, 300, 4))<br>(&#39;scores output&#39;, (1, 300))<br>(&#39;labels output&#39;, (1, 300))<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([726, 234, 796, 424]), &#39; Score &#39;, 0.939078)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([418, 232, 464, 425]), &#39; Score &#39;, 0.9093691)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([521, 234, 578, 430]), &#39; Score &#39;, 0.90926725)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([466, 670, 621, 798]), &#39; Score &#39;, 0.89821684)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([ 44, 229, 112, 424]), &#39; Score &#39;, 0.89342)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([607, 227, 703, 428]), &#39; Score &#39;, 0.88139987)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([148, 458, 270, 664]), &#39; Score &#39;, 0.88060725)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([885, 233, 987, 401]), &#39; Score &#39;, 0.87992036)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([742, 454, 865, 665]), &#39; Score &#39;, 0.87926996)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([482,   8, 612, 219]), &#39; Score &#39;, 0.8729649)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([171, 241, 246, 430]), &#39; Score &#39;, 0.859805)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([  7,   2, 129, 218]), &#39; Score &#39;, 0.851859)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([868, 454, 965, 662]), &#39; Score &#39;, 0.8461512)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([  9, 450, 126, 665]), &#39; Score &#39;, 0.8399973)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([823, 234, 870, 417]), &#39; Score &#39;, 0.8194807)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([745,  20, 861, 216]), &#39; Score &#39;, 0.8122773)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([605, 457, 725, 665]), &#39; Score &#39;, 0.8076674)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([289, 444, 414, 665]), &#39; Score &#39;, 0.79930824)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([133,   8, 254, 215]), &#39; Score &#39;, 0.7968467)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([619,   7, 742, 216]), &#39; Score &#39;, 0.7953634)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([619, 680, 787, 799]), &#39; Score &#39;, 0.7879201)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([251,   3, 351, 223]), &#39; Score &#39;, 0.78193235)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([189, 687, 297, 794]), &#39; Score &#39;, 0.7808079)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([783, 664, 971, 794]), &#39; Score &#39;, 0.7797159)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([449, 453, 553, 668]), &#39; Score &#39;, 0.7750649)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([301, 252, 345, 432]), &#39; Score &#39;, 0.77126116)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([365,   8, 471, 212]), &#39; Score &#39;, 0.7604503)<br>(&#39;Label&#39;, &#39;cell phone&#39;, &#39; at &#39;, array([697, 668, 747, 706]), &#39; Score &#39;, 0.70097476)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([871,   8, 985, 218]), &#39; Score &#39;, 0.6951452)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([363, 680, 477, 785]), &#39; Score &#39;, 0.6740273)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([ 47, 676, 148, 793]), &#39; Score &#39;, 0.6373141)<br>(&#39;Time for &#39;, 1, &#39; is &#39;, 2.5029850006103516)</pre><p name="2c5e" id="2c5e" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Via TensorFLow</strong></p><p name="9dce" id="9dce" class="graf graf--p graf-after--p">at resolution</p><pre name="8930" id="8930" class="graf graf--pre graf-after--p">img_tensor = tf.image.decode_jpeg(img_raw, channels=0,</pre><pre name="41ec" id="41ec" class="graf graf--pre graf-after--pre">dct_method=&quot;INTEGER_FAST&quot;) #not much effect here decode time 30ms</pre><pre name="edd0" id="edd0" class="graf graf--pre graf-after--pre">print(&quot;img_tensor.shape=&quot;,img_tensor.shape)</pre><pre name="5598" id="5598" class="graf graf--pre graf-after--pre">image = tf.cast(img_tensor, tf.float32)</pre><pre name="fbac" id="fbac" class="graf graf--pre graf-after--pre">#image = tf.image.resize_images(img_tensor, [800,1067])</pre><pre name="e673" id="e673" class="graf graf--pre graf-after--pre">image = tf.image.resize_images(image, [800,1600])</pre><pre name="735d" id="735d" class="graf graf--pre graf-after--pre">VGG_MEAN = [123.68, 116.78, 103.94]    # This is R-G-B for Imagenet</pre><pre name="7da0" id="7da0" class="graf graf--pre graf-after--pre">#VGG_MEAN = [103.94,116.78,123.68]</pre><pre name="b1ff" id="b1ff" class="graf graf--pre graf-after--pre">means = tf.reshape(tf.constant(VGG_MEAN), [1, 1, 3])</pre><pre name="2f99" id="2f99" class="graf graf--pre graf-after--pre">image = image - means</pre><figure name="a32c" id="a32c" class="graf graf--figure graf-after--pre"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 350px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 50%;"></div><img class="graf-image" data-image-id="1*FCZA2UAgJ7ow3I1rLnci8g.png" data-width="1600" data-height="800" src="https://cdn-images-1.medium.com/max/800/1*FCZA2UAgJ7ow3I1rLnci8g.png"></div></figure><p name="ec85" id="ec85" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Via OpenCV</strong></p><p name="6365" id="6365" class="graf graf--p graf-after--p">Detection are resolution 800, 1067</p><figure name="ddcc" id="ddcc" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 525px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 75%;"></div><img class="graf-image" data-image-id="1*gcytLFz7I3PJk-sicblFXw.png" data-width="1067" data-height="800" src="https://cdn-images-1.medium.com/max/800/1*gcytLFz7I3PJk-sicblFXw.png"></div></figure><pre name="cc14" id="cc14" class="graf graf--pre graf-after--figure">(&#39;Number of test=&#39;, 1)<br>(&#39;decode time=&#39;, 0.6910791397094727)<br>(&#39;in image shape&#39;, (800, 1067, 3))<br>(&#39;Input shape=&#39;, (1, 800, 1067, 3))<br>(&#39;in tf shape&#39;, (1, 800, 1067, 3))<br>(&#39;result no&#39;, 0)<br>(&#39;boxes output&#39;, (1, 300, 4))<br>(&#39;scores output&#39;, (1, 300))<br>(&#39;labels output&#39;, (1, 300))<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([507, 667, 671, 798]), &#39; Score &#39;, 0.891104)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([565, 238, 621, 428]), &#39; Score &#39;, 0.8876671)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([186, 243, 264, 429]), &#39; Score &#39;, 0.8755632)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([794, 450, 929, 669]), &#39; Score &#39;, 0.86985874)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([311, 444, 448, 662]), &#39; Score &#39;, 0.8531823)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([270,   7, 380, 214]), &#39; Score &#39;, 0.8526345)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([779, 230, 855, 422]), &#39; Score &#39;, 0.8257129)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([450, 232, 496, 423]), &#39; Score &#39;, 0.81420606)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([524,  10, 658, 217]), &#39; Score &#39;, 0.7964738)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([ 844,  663, 1003,  796]), &#39; Score &#39;, 0.7255119)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([409,   8, 514, 201]), &#39; Score &#39;, 0.7211455)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([172, 452, 284, 673]), &#39; Score &#39;, 0.71296626)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([ 942,    8, 1058,  219]), &#39; Score &#39;, 0.67450273)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([643, 452, 768, 654]), &#39; Score &#39;, 0.6541213)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([325, 245, 368, 430]), &#39; Score &#39;, 0.60505646)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([673,   8, 790, 218]), &#39; Score &#39;, 0.5901652)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([209, 686, 320, 792]), &#39; Score &#39;, 0.56278116)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([ 957,  235, 1015,  401]), &#39; Score &#39;, 0.5500602)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([804,  16, 925, 283]), &#39; Score &#39;, 0.53584856)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([ 942,  457, 1026,  650]), &#39; Score &#39;, 0.5299792)<br>(&#39;Label&#39;, &#39;person&#39;, &#39; at &#39;, array([485, 450, 591, 667]), &#39; Score &#39;, 0.515951)<br>(&#39;Time for &#39;, 1, &#39; is &#39;, 1.6651248931884766)<br>Response Received Exiting</pre><p name="5b2a" id="5b2a" class="graf graf--p graf-after--pre graf--trailing">Full code — <a href="https://gist.github.com/alexcpn/c43e8980d32348fce1fd04f1a52d5ea7" data-href="https://gist.github.com/alexcpn/c43e8980d32348fce1fd04f1a52d5ea7" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://gist.github.com/alexcpn/c43e8980d32348fce1fd04f1a52d5ea7</a></p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@alexcpn" class="p-author h-card">Alex Punnen</a> on <a href="https://medium.com/p/876c8a14c67a"><time class="dt-published" datetime="2019-03-29T06:12:30.285Z">March 29, 2019</time></a>.</p><p><a href="https://medium.com/@alexcpn/image-pre-processing-for-tf-serving-via-opencv-pillow-tensorflow-tf-image-decode-876c8a14c67a" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on August 22, 2019.</p></footer></article></body></html>