<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>backpropogation_matrix_calculus – The Maths of Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-617dda62c7243b6972367a47cbfd8072.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">The Maths of Deep Learning</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-end">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./5_backpropogation_matrix_calculus.html">Backpropagation with Matrix Calculus</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1_vectors_dot_product_and_perceptron.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Vectors &amp; Perceptron</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2_perceptron_training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Perceptron Training</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3_gradient_descent.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Gradient Descent</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4_backpropogation_chainrule.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Backpropagation with Scalar Calculus</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./5_backpropogation_matrix_calculus.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Backpropagation with Matrix Calculus</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./6_backpropogation_softmax.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Backpropagation with Softmax and Cross Entropy</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./7_neuralnetworkimpementation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Neural Network Implementation</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#backpropagation-with-matrix-calculus" id="toc-backpropagation-with-matrix-calculus" class="nav-link active" data-scroll-target="#backpropagation-with-matrix-calculus">Backpropagation with Matrix Calculus</a>
  <ul class="collapse">
  <li><a href="#some-math-intuition" id="toc-some-math-intuition" class="nav-link" data-scroll-target="#some-math-intuition">Some Math Intuition</a>
  <ul class="collapse">
  <li><a href="#gradient-vector" id="toc-gradient-vector" class="nav-link" data-scroll-target="#gradient-vector">Gradient Vector</a></li>
  </ul></li>
  <li><a href="#jacobian-matrix" id="toc-jacobian-matrix" class="nav-link" data-scroll-target="#jacobian-matrix">Jacobian Matrix</a>
  <ul class="collapse">
  <li><a href="#the-chain-rule-with-matrices" id="toc-the-chain-rule-with-matrices" class="nav-link" data-scroll-target="#the-chain-rule-with-matrices">The Chain Rule with Matrices</a></li>
  <li><a href="#backpropagation-trick---vjp-vector-jacobian-product-and-jvp-jacobian-vector-product" id="toc-backpropagation-trick---vjp-vector-jacobian-product-and-jvp-jacobian-vector-product" class="nav-link" data-scroll-target="#backpropagation-trick---vjp-vector-jacobian-product-and-jvp-jacobian-vector-product">Backpropagation Trick - VJP (Vector Jacobian Product) and JVP (Jacobian Vector Product)</a></li>
  <li><a href="#hadamard-product" id="toc-hadamard-product" class="nav-link" data-scroll-target="#hadamard-product">Hadamard Product</a></li>
  </ul></li>
  <li><a href="#backpropagation-derivation" id="toc-backpropagation-derivation" class="nav-link" data-scroll-target="#backpropagation-derivation">Backpropagation Derivation</a>
  <ul class="collapse">
  <li><a href="#the-2-layer-neural-network-model" id="toc-the-2-layer-neural-network-model" class="nav-link" data-scroll-target="#the-2-layer-neural-network-model">The 2-Layer Neural Network Model</a></li>
  <li><a href="#gradient-vector2d-tensor-of-loss-function-in-last-layer" id="toc-gradient-vector2d-tensor-of-loss-function-in-last-layer" class="nav-link" data-scroll-target="#gradient-vector2d-tensor-of-loss-function-in-last-layer">Gradient Vector/2D-Tensor of Loss Function in Last Layer</a></li>
  </ul></li>
  <li><a href="#jacobian-of-loss-function-in-inner-layer" id="toc-jacobian-of-loss-function-in-inner-layer" class="nav-link" data-scroll-target="#jacobian-of-loss-function-in-inner-layer">Jacobian of Loss Function in Inner Layer</a>
  <ul class="collapse">
  <li><a href="#summary-of-backpropagation-equations" id="toc-summary-of-backpropagation-equations" class="nav-link" data-scroll-target="#summary-of-backpropagation-equations">Summary of Backpropagation Equations</a></li>
  <li><a href="#summary-of-backpropagation-equations-in-terms-of-numpy" id="toc-summary-of-backpropagation-equations-in-terms-of-numpy" class="nav-link" data-scroll-target="#summary-of-backpropagation-equations-in-terms-of-numpy">Summary of Backpropagation Equations in Terms of Numpy</a></li>
  </ul></li>
  <li><a href="#using-gradient-descent-to-find-the-optimal-weights-to-reduce-the-loss-function" id="toc-using-gradient-descent-to-find-the-optimal-weights-to-reduce-the-loss-function" class="nav-link" data-scroll-target="#using-gradient-descent-to-find-the-optimal-weights-to-reduce-the-loss-function">Using Gradient Descent to Find the Optimal Weights to Reduce the Loss Function</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="backpropagation-with-matrix-calculus" class="level1">
<h1>Backpropagation with Matrix Calculus</h1>
<p>The previous chapters we used a Scalar derivation of the Back Propagation formula to implement it in a simple two layer neural network. What we have done is is to use Hadamard product and matrix transposes with scalar derivation alignment.</p>
<p>But we have not really explained why we use Hadamard product and matrix transposes with scalar derivation alignment.</p>
<p>This is due to Matrix Calculus which is the real way in which we should be deriving the Back Propagation formula.</p>
<p>Lets explore this in this chapter. Note that we are still not using a Softmax activation function in the output layer as is usually the case with Deep Neural Networks. Deriving the Back Propagation formula with Softmax activation function is bit more complex and we will do that in a later chapter.</p>
<p>Let’s take the previous two layered simple neural network,with a Mean Square Error Loss function, and derive the Back Propagation formula with Matrix Calculus now.</p>
<p>Let’s write the equation of the following neural network</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>x <span class="kw">is</span> the Input</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>y <span class="kw">is</span> the Output.</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>l <span class="kw">is</span> the number of layers of the Neural Network.</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>a <span class="kw">is</span> the activation function ,(we use sigmoid here)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<p><span class="math display">\[
x \rightarrow a^{l-1} \rightarrow  a^{l} \rightarrow  y
\]</span></p>
<p>Where the activation <span class="math inline">\(a^l\)</span> is <span class="math display">\[
  a^{l} = \sigma(W^l a^{l-1}+b^l).
\]</span></p>
<p>and</p>
<p><span class="math display">\[
a^{l} = \sigma(z^l) \quad where \quad
z^l =W^l a^{l-1} +b^l
\]</span></p>
<p>Our two layer neural network can be written as</p>
<p><span class="math display">\[
\mathbf { a^0 \rightarrow a^{1} \rightarrow  a^{2} \rightarrow  y }
\]</span></p>
<p>(<span class="math inline">\(a^2\)</span> does not denote the exponent but just that it is of layer 2)</p>
<section id="some-math-intuition" class="level2">
<h2 class="anchored" data-anchor-id="some-math-intuition">Some Math Intuition</h2>
<p>The concept of a Neural Network as a composition of functions remains central.</p>
<p>In our network, most layers represent functions that map a <strong>Vector to a Vector</strong> (<span class="math inline">\(\mathbb{R}^n \to \mathbb{R}^m\)</span>). For example, the hidden layers take an input vector and produce an activation vector.</p>
<p>However, the final step—calculating the Loss—is different. It maps the final output vector <span class="math inline">\(a^L\)</span> (and the target <span class="math inline">\(y\)</span>) to a single <strong>Scalar</strong> value, the Cost <span class="math inline">\(C\)</span> (<span class="math inline">\(\mathbb{R}^n \to \mathbb{R}\)</span>).</p>
<section id="gradient-vector" class="level3">
<h3 class="anchored" data-anchor-id="gradient-vector">Gradient Vector</h3>
<p>When we take the derivative of a scalar-valued function (like the Cost <span class="math inline">\(C\)</span>) with respect to a vector (like the weights <span class="math inline">\(w\)</span>), the result is a vector of the same size as <span class="math inline">\(w\)</span>. This is called the <strong>Gradient Vector</strong>.</p>
<p><span class="math display">\[
\nabla_w C = \begin{bmatrix} \frac{\partial C}{\partial w_1} \\ \vdots \\ \frac{\partial C}{\partial w_n} \end{bmatrix}
\]</span></p>
<p>Why is it called a “gradient”?</p>
<p>Because the gradient points in the direction of steepest increase of the function.</p>
<p>Moving a tiny step along <span class="math inline">\(+\nabla_w C\)</span> increases the cost the fastest.</p>
<p>Moving a tiny step against it, i.e.&nbsp;along <span class="math inline">\(-\nabla_w C\)</span>, decreases the cost the fastest.</p>
<p>That’s exactly why gradient descent updates parameters like this:</p>
<p><span class="math display">\[
w \leftarrow w - \eta \nabla_w C
\]</span></p>
<p>where <span class="math inline">\(\eta\)</span> is the learning rate.</p>
<p>So the gradient vector is more than a list of derivatives—it’s the local direction that tells us how to change parameters to reduce the loss.</p>
<p>See the Colab <a href="https://colab.research.google.com/drive/1sMODrDCdR7lKF9cWcNNhhdLglxJRzmgK?usp=sharing">7_1</a> for a generated visualization of this.</p>
<p>The first image is the plotting of the Cost function</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/gradientvec1.png" class="img-fluid figure-img"></p>
<figcaption>gradient_vector</figcaption>
</figure>
</div>
<p>The second image where you see the cones are the gradient vector of the Cost function wrto weights plotted in 3D space.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/gradientvec2.png" class="img-fluid figure-img"></p>
<figcaption>gradient_vector</figcaption>
</figure>
</div>
</section>
</section>
<section id="jacobian-matrix" class="level2">
<h2 class="anchored" data-anchor-id="jacobian-matrix">Jacobian Matrix</h2>
<p>The second key concept is the <strong>Jacobian Matrix</strong>.</p>
<p>As mentioned earlier, in our network, most layers represent functions that map a <strong>Vector to a Vector</strong> (<span class="math inline">\(\mathbb{R}^n \to \mathbb{R}^m\)</span>). For example, a hidden layer takes an input vector <span class="math inline">\(x\)</span> and produces an activation vector <span class="math inline">\(a\)</span>.</p>
<p>What is the derivative of a vector-valued function with respect to a vector input? This is where the Jacobian comes in.</p>
<p>For a function <span class="math inline">\(f: \mathbb{R}^n \to \mathbb{R}^m\)</span> that maps an input vector <span class="math inline">\(x\)</span> of size <span class="math inline">\(n\)</span> to an output vector <span class="math inline">\(y\)</span> of size <span class="math inline">\(m\)</span>, the derivative is an <span class="math inline">\(m \times n\)</span> matrix called the Jacobian Matrix <span class="math inline">\(J\)</span>.</p>
<p>The entry <span class="math inline">\(J_{ij}\)</span> is the partial derivative of the <span class="math inline">\(i\)</span>-th output component with respect to the <span class="math inline">\(j\)</span>-th input component:</p>
<p><span class="math display">\[
J_{ij} = \frac{\partial f_i}{\partial x_j}
\]</span></p>
<section id="the-chain-rule-with-matrices" class="level3">
<h3 class="anchored" data-anchor-id="the-chain-rule-with-matrices">The Chain Rule with Matrices</h3>
<p>The beauty of the Jacobian is that it allows us to generalize the chain rule.</p>
<p>For scalar functions, the chain rule is just multiplication: <span class="math inline">\((f \circ g)'(x) = f'(g(x)) \cdot g'(x)\)</span>.</p>
<p>For vector functions, the chain rule becomes <strong>Matrix Multiplication</strong> of the Jacobians:</p>
<p>If we have a composition of functions <span class="math inline">\(y = f(g(x))\)</span>, and we let <span class="math inline">\(A\)</span> be the Jacobian of <span class="math inline">\(f\)</span> and <span class="math inline">\(B\)</span> be the Jacobian of <span class="math inline">\(g\)</span>, then the Jacobian of the composition is simply the matrix product <span class="math inline">\(A \cdot B\)</span>.</p>
<p><span class="math display">\[
(A \cdot B)_{ij} = \sum_{k=1}^m A_{ik} \cdot B_{kj}
\]</span></p>
<p>So, the Jacobian is a matrix of partial derivatives that represents the local linear approximation of a vector function. When we say the Jacobian represents a “local linear approximation,” we mean:</p>
<p><span class="math display">\[ \text{Change in Output} \approx \text{Jacobian Matrix} \cdot \text{Change in Input} \]</span></p>
<p><span class="math display">\[ \Delta y \approx J \cdot \Delta x \]</span></p>
<p>It tells us: “If I nudge the input vector by a tiny vector <span class="math inline">\(\Delta x\)</span>, the output vector will change by roughly the matrix-vector product <span class="math inline">\(J \cdot \Delta x\)</span>.”</p>
</section>
<section id="backpropagation-trick---vjp-vector-jacobian-product-and-jvp-jacobian-vector-product" class="level3">
<h3 class="anchored" data-anchor-id="backpropagation-trick---vjp-vector-jacobian-product-and-jvp-jacobian-vector-product">Backpropagation Trick - VJP (Vector Jacobian Product) and JVP (Jacobian Vector Product)</h3>
<p>There is one more trick that we can use to make backpropogation more efficient.</p>
<p>Let me explain with an example.</p>
<p>Suppose we have a chain of functions: <span class="math inline">\(y = f(g(h(x)))\)</span>. To find the derivative <span class="math inline">\(\frac{\partial y}{\partial x}\)</span>, the chain rule tells us to multiply the Jacobians: <span class="math display">\[ J_{total} = J_f \cdot J_g \cdot J_h \]</span></p>
<p>If <span class="math inline">\(x, h, g, f\)</span> are all vectors of size 1000, then each Jacobian is a <span class="math inline">\(1000 \times 1000\)</span> matrix. Multiplying them is expensive (<span class="math inline">\(O(N^3)\)</span>).</p>
<p><strong>However, in Backpropagation, we always start with a scalar Loss function.</strong> The final derivative <span class="math inline">\(\frac{\partial C}{\partial y}\)</span> is a row vector (size <span class="math inline">\(1 \times N\)</span>).</p>
<p>So we are computing: <span class="math display">\[ \nabla C = \underbrace{\frac{\partial C}{\partial y}}_{1 \times N} \cdot \underbrace{J_f}_{N \times N} \cdot \underbrace{J_g}_{N \times N} \cdot \underbrace{J_h}_{N \times N} \]</span></p>
<p>Notice the order of operations matters! 1. <strong>Jacobian-Matrix Product</strong>: If we multiply the matrices first (<span class="math inline">\(J_f \cdot J_g\)</span>), we do expensive matrix-matrix multiplication. 2. <strong>Vector-Jacobian Product (VJP)</strong>: If we multiply from left to right: * <span class="math inline">\(v_1 = \frac{\partial C}{\partial y} \cdot J_f\)</span> (Vector <span class="math inline">\(\times\)</span> Matrix <span class="math inline">\(\to\)</span> Vector) * <span class="math inline">\(v_2 = v_1 \cdot J_g\)</span> (Vector <span class="math inline">\(\times\)</span> Matrix <span class="math inline">\(\to\)</span> Vector) * <span class="math inline">\(v_3 = v_2 \cdot J_h\)</span> (Vector <span class="math inline">\(\times\)</span> Matrix <span class="math inline">\(\to\)</span> Vector)</p>
<p>We <strong>never</strong> explicitly compute or store the full Jacobian matrix. We only compute the product of a vector with the Jacobian. This is much faster (<span class="math inline">\(O(N^2)\)</span>) and uses less memory.</p>
<p>This is the secret sauce of efficient Backpropagation!</p>
</section>
<section id="hadamard-product" class="level3">
<h3 class="anchored" data-anchor-id="hadamard-product">Hadamard Product</h3>
<p>Another important operation we use is the <strong>Hadamard Product</strong> (denoted by <span class="math inline">\(\odot\)</span> or sometimes <span class="math inline">\(\circ\)</span>). This is simply <strong>element-wise multiplication</strong> of two vectors or matrices of the same size.</p>
<p><span class="math display">\[
\begin{bmatrix} a_1 \\ a_2 \end{bmatrix} \odot \begin{bmatrix} b_1 \\ b_2 \end{bmatrix} = \begin{bmatrix} a_1 \cdot b_1 \\ a_2 \cdot b_2 \end{bmatrix}
\]</span></p>
<p>It is different from the dot product (which sums the results to a scalar) and matrix multiplication. In backpropagation, it often appears when we apply the chain rule through an activation function that operates element-wise (like sigmoid or ReLU).</p>
<hr>
</section>
</section>
<section id="backpropagation-derivation" class="level2">
<h2 class="anchored" data-anchor-id="backpropagation-derivation">Backpropagation Derivation</h2>
<section id="the-2-layer-neural-network-model" class="level3">
<h3 class="anchored" data-anchor-id="the-2-layer-neural-network-model">The 2-Layer Neural Network Model</h3>
<p>For this derivation, we use a simple 2-layer network (one hidden layer, one output layer):</p>
<p><span class="math display">\[
x \xrightarrow{W^1, b^1} a^1 \xrightarrow{W^2, b^2} a^2
\]</span></p>
<p><strong>Forward Pass Equations:</strong> 1. <strong>Hidden Layer:</strong> <span class="math display">\[ z^1 = W^1 x + b^1 \]</span> <span class="math display">\[ a^1 = \sigma(z^1) \]</span> 2. <strong>Output Layer:</strong> <span class="math display">\[ z^2 = W^2 a^1 + b^2 \]</span> <span class="math display">\[ a^2 = \sigma(z^2) \]</span></p>
<p>We use the <strong>Mean Squared Error (MSE)</strong> loss function: <span class="math display">\[ C = \frac{1}{2} \|y - a^2\|^2 \]</span></p>
</section>
<section id="gradient-vector2d-tensor-of-loss-function-in-last-layer" class="level3">
<h3 class="anchored" data-anchor-id="gradient-vector2d-tensor-of-loss-function-in-last-layer">Gradient Vector/2D-Tensor of Loss Function in Last Layer</h3>
<p><span class="math display">\[
C = \frac{1}{2} \|y - a^2\|^2 = \frac{1}{2} \sum_j (y_j-a^2_j)^2
\]</span></p>
<p>Where: <span class="math display">\[
a^2 = \sigma(z^2) \quad \text{and} \quad z^2 = W^2 a^1 + b^2
\]</span></p>
<p>We want to find <span class="math inline">\(\frac{\partial C}{\partial W^2}\)</span>. Using the Chain Rule:</p>
<p><span class="math display">\[
\frac{\partial C}{\partial W^2} = \frac{\partial C}{\partial z^2} \cdot \frac{\partial z^2}{\partial W^2}
\]</span></p>
<p>Let’s define the <strong>error term</strong> <span class="math inline">\(\delta^2\)</span> as the derivative of the cost with respect to the pre-activation <span class="math inline">\(z^2\)</span>:</p>
<p><span class="math display">\[
\delta^2 \equiv \frac{\partial C}{\partial z^2} = \frac{\partial C}{\partial a^2} \odot \frac{\partial a^2}{\partial z^2}
\]</span></p>
<ol type="1">
<li><span class="math inline">\(\frac{\partial C}{\partial a^2} = (a^2 - y)\)</span></li>
<li><span class="math inline">\(\frac{\partial a^2}{\partial z^2} = \sigma'(z^2)\)</span></li>
</ol>
<p>So, using the Hadamard product (<span class="math inline">\(\odot\)</span>) for element-wise multiplication:</p>
<hr>
<p>Note that none of these terms are exponents but super scripts.!</p>
<p><strong>Hadamard product or Element-wise multiplication</strong></p>
<p>The confusion usually lies in this term:<span class="math display">\[\frac{\partial a}{\partial z}\]</span></p>
<p>Since <span class="math inline">\(a\)</span> is a vector and <span class="math inline">\(z\)</span> is a vector, the derivative of one with respect to the other is technically a Jacobian Matrix, not a vector.However, because the activation function <span class="math inline">\(\sigma\)</span> is applied element-wise (i.e., <span class="math inline">\(a_i\)</span> depends only on <span class="math inline">\(z_i\)</span>, not on <span class="math inline">\(z_j\)</span> - That is - activation function in one layer is just dependent of the output of only the previous layer and no other layers), all off-diagonal elements of this Jacobian are zero.</p>
<p><span class="math display">\[
J = \frac{\partial a}{\partial z} = \begin{bmatrix}
\sigma'(z_1) &amp; 0 &amp; \dots \\
0 &amp; \sigma'(z_2) &amp; \dots \\
\vdots &amp; \vdots &amp; \ddots
\end{bmatrix}\]</span></p>
<p>When you apply the chain rule, you are multiplying the gradient vector <span class="math inline">\(\nabla_a C\)</span> by this diagonal matrix <span class="math inline">\(J\)</span> - VJP (Vector-Jacobian Product).</p>
<p>Key Identity: Multiplying a vector by a diagonal matrix is mathematically identical to taking the Hadamard product of the vector and the diagonal elements and this is why we use Hadamard product in backpropogation.</p>
<p><span class="math display">\[
\delta^2 = (a^2 - y) \odot \sigma'(z^2)
\]</span></p>
<hr>
<p>Now for the second part, we need to find how the Cost changes with respect to the weights <span class="math inline">\(W^2\)</span>.</p>
<p>We know that <span class="math inline">\(z^2 = W^2 a^1\)</span>. In index notation, for a single element <span class="math inline">\(z^2_i\)</span>: <span class="math display">\[ z^2_i = \sum_k W^2_{ik} a^1_k \]</span></p>
<p>We want to find <span class="math inline">\(\frac{\partial C}{\partial W^2_{ik}}\)</span>. Using the chain rule: <span class="math display">\[ \frac{\partial C}{\partial W^2_{ik}} = \frac{\partial C}{\partial z^2_i} \cdot \frac{\partial z^2_i}{\partial W^2_{ik}} \]</span></p>
<ol type="1">
<li>We already defined <span class="math inline">\(\frac{\partial C}{\partial z^2_i} = \delta^2_i\)</span>.</li>
<li>From the linear equation <span class="math inline">\(z^2_i = \dots + W^2_{ik} a^1_k + \dots\)</span>, the derivative with respect to <span class="math inline">\(W^2_{ik}\)</span> is simply <span class="math inline">\(a^1_k\)</span>.</li>
</ol>
<p>So: <span class="math display">\[ \frac{\partial C}{\partial W^2_{ik}} = \delta^2_i \cdot a^1_k \]</span></p>
<p>If we organize these gradients into a matrix, the element at row <span class="math inline">\(i\)</span> and column <span class="math inline">\(k\)</span> is the product of the <span class="math inline">\(i\)</span>-th element of <span class="math inline">\(\delta^2\)</span> and the <span class="math inline">\(k\)</span>-th element of <span class="math inline">\(a^1\)</span>.</p>
<p>Let’s visualize the matrix of gradients <span class="math inline">\(\nabla W\)</span>:<span class="math display">\[\nabla W =
\begin{bmatrix}
\frac{\partial C}{\partial W_{11}} &amp; \frac{\partial C}{\partial W_{12}} \\
\frac{\partial C}{\partial W_{21}} &amp; \frac{\partial C}{\partial W_{22}}
\end{bmatrix}\]</span>Substitute the result from step 3 (<span class="math inline">\(\delta_i \cdot a_k\)</span>):<span class="math display">\[\nabla W =
\begin{bmatrix}
\delta_1 a_1 &amp; \delta_1 a_2 \\
\delta_2 a_1 &amp; \delta_2 a_2
\end{bmatrix}\]</span></p>
<p>This is exactly the definition of the <strong>Outer Product</strong> <span class="math inline">\(\otimes\)</span> of two vectors:</p>
<p><span class="math display">\[
\frac{\partial C}{\partial W^2} =  \delta^2 \otimes a^1 = \delta^2  (a^1)^T \quad \rightarrow (Eq \; 3)
\]</span></p>
<p>This gives us the gradient matrix for the last layer weights.</p>
<p>&nbsp;</p>
</section>
</section>
<section id="jacobian-of-loss-function-in-inner-layer" class="level2">
<h2 class="anchored" data-anchor-id="jacobian-of-loss-function-in-inner-layer">Jacobian of Loss Function in Inner Layer</h2>
<p>Now let’s do the same for the inner layer (<span class="math inline">\(W^1\)</span>).</p>
<p><span class="math display">\[
\frac{\partial C}{\partial W^1} = \frac{\partial C}{\partial z^1} \cdot \frac{\partial z^1}{\partial W^1} = \delta^1 (a^0)^T
\]</span></p>
<p>We need to find <span class="math inline">\(\delta^1 = \frac{\partial C}{\partial z^1}\)</span>. We can backpropagate the error <span class="math inline">\(\delta^2\)</span> from the next layer:</p>
<p><span class="math display">\[
\delta^1 = \frac{\partial C}{\partial z^1} = \left( (W^2)^T \delta^2 \right) \odot \sigma'(z^1)
\]</span></p>
<p><strong>Explanation:</strong> 1. We propagate <span class="math inline">\(\delta^2\)</span> backwards through the weights <span class="math inline">\((W^2)^T\)</span>. 2. We multiply element-wise by the derivative of the activation function <span class="math inline">\(\sigma'(z^1)\)</span>.</p>
<p>Putting it all together:</p>
<p><span class="math display">\[
\frac{\partial C}{\partial W^1} = \left( (W^2)^T \delta^2 \odot \sigma'(z^1) \right) (a^0)^T \quad \rightarrow (Eq \; 5)
\]</span></p>
<section id="summary-of-backpropagation-equations" class="level3">
<h3 class="anchored" data-anchor-id="summary-of-backpropagation-equations">Summary of Backpropagation Equations</h3>
<ol type="1">
<li><strong>Compute Output Error:</strong> <span class="math display">\[ \delta^L = (a^L - y) \odot \sigma'(z^L) \]</span></li>
<li><strong>Backpropagate Error:</strong> <span class="math display">\[ \delta^l = ((W^{l+1})^T \delta^{l+1}) \odot \sigma'(z^l) \]</span></li>
<li><strong>Compute Gradients:</strong> <span class="math display">\[ \frac{\partial C}{\partial W^l} = \delta^l (a^{l-1})^T \]</span></li>
</ol>
<p>&nbsp;</p>
</section>
<section id="summary-of-backpropagation-equations-in-terms-of-numpy" class="level3">
<h3 class="anchored" data-anchor-id="summary-of-backpropagation-equations-in-terms-of-numpy">Summary of Backpropagation Equations in Terms of Numpy</h3>
<p>Here is how these equations translate to Python code using NumPy, assuming standard column vectors (shape <code>(N, 1)</code>).</p>
<div class="code-copy-outer-scaffold"><div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Forward pass context:</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># x, a1, a2 are column vectors</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co"># W1, W2 are weight matrices</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># sigmoid_prime(z) is the derivative of activation</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 1. Compute Output Error (Hadamard Product)</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="co"># '*' operator in numpy is element-wise multiplication (Hadamard)</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>delta2 <span class="op">=</span> (a2 <span class="op">-</span> y) <span class="op">*</span> sigmoid_prime(z2) </span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 2. Gradient for W2 (Outer Product)</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co"># We need shape (n_out, 1) @ (1, n_hidden) -&gt; (n_out, n_hidden)</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>dC_dW2 <span class="op">=</span> np.matmul(delta2, a1.T)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Alternative using einsum for outer product:</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="co"># dC_dW2 = np.einsum('i,j-&gt;ij', delta2.flatten(), a1.flatten())</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="co"># 3. Backpropagate Error to Hidden Layer</span></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="co"># Matrix multiplication (W2.T @ delta2) followed by Hadamard product</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>delta1 <span class="op">=</span> np.matmul(W2.T, delta2) <span class="op">*</span> sigmoid_prime(z1)</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="co"># 4. Gradient for W1 (Outer Product)</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a>dC_dW1 <span class="op">=</span> np.matmul(delta1, x.T)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</section>
</section>
<section id="using-gradient-descent-to-find-the-optimal-weights-to-reduce-the-loss-function" class="level2">
<h2 class="anchored" data-anchor-id="using-gradient-descent-to-find-the-optimal-weights-to-reduce-the-loss-function">Using Gradient Descent to Find the Optimal Weights to Reduce the Loss Function</h2>
<p>&nbsp;</p>
<p>With equations (3) and (5) we can calculate the gradient of the Loss function with respect to weights in any layer - in this example</p>
<p><span class="math display">\[\frac {\partial C}{\partial W^1},\frac {\partial C}{\partial W^2}\]</span></p>
<p>&nbsp;</p>
<p>We now need to adjust the previous weight, by gradient descent.</p>
<p>&nbsp;</p>
<p>So using the above gradients we get the new weights iteratively like below. If you notice this is exactly what is happening in gradient descent as well; only chain rule is used to calculate the gradients here. Backpropagation is the algorithm that helps calculate the gradients for each layer.</p>
<p>&nbsp;</p>
<p><span class="math display">\[\mathbf {
  W^{l-1}_{new} = W^{l-1}_{old} - \eta \cdot \frac{\partial C}{\partial W^{l-1}}
}\]</span></p>
<p>&nbsp;</p>
<p>Where <span class="math inline">\(\eta\)</span> is the learning rate.</p>
<p>Next: <a href="./6_backpropogation_softmax.html">Backpropagation with Softmax and Cross Entropy</a></p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li><a href="https://cedar.buffalo.edu/~srihari/CSE574/Chap5/Chap5.3-BackProp.pdf">Back Propagation - Srihari (SUNY Buffalo)</a></li>
<li><a href="http://neuralnetworksanddeeplearning.com/chap2.html">Neural Networks and Deep Learning - Michael Nielsen</a></li>
<li><a href="https://colab.research.google.com/drive/1sMODrDCdR7lKF9cWcNNhhdLglxJRzmgK?usp=sharing">Colab Visualization</a></li>
</ul>



</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>