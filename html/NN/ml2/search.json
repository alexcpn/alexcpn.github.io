[
  {
    "objectID": "8_backpropogation_full.html",
    "href": "8_backpropogation_full.html",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "",
    "text": "Alex Punnen\n© All Rights Reserved\n\nContents",
    "crumbs": [
      "Backprop With Softmax and Cross Entropy"
    ]
  },
  {
    "objectID": "8_backpropogation_full.html#back-propagation-in-full---with-softmax-crossentropy-loss",
    "href": "8_backpropogation_full.html#back-propagation-in-full---with-softmax-crossentropy-loss",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Back Propagation in Full - With Softmax & CrossEntropy Loss",
    "text": "Back Propagation in Full - With Softmax & CrossEntropy Loss\nLet’s think of a \\(l\\) layered neural network whose input is \\(x=a^0\\) and output is \\(a^l\\).In this network we will be using the sigmoid (\\(\\sigma\\) ) function as the activation function for all layers except the last layer \\(l\\). For the last layer we use the Softmax activation function. We will use the Cross Entropy Loss as the loss function.\nThis is how a proper Neural Network should be.",
    "crumbs": [
      "Backprop With Softmax and Cross Entropy"
    ]
  },
  {
    "objectID": "8_backpropogation_full.html#the-neural-network-model",
    "href": "8_backpropogation_full.html#the-neural-network-model",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "The Neural Network Model",
    "text": "The Neural Network Model\nI am writing this out, without index notation, and with the super script representing just the layers of the network.\n\\[\n\\mathbf {\n\\begin{aligned}\na^0 \\rightarrow\n      \\underbrace{\\text{hidden layers}}_{a^{l-2}}\n      \\,\\rightarrow\n      \\underbrace{W^{l-1} a^{l-2}+b^{l-1}}_{z^{l-1} }\n      \\,\\rightarrow\n      \\underbrace{\\sigma(z^{l-1})}_{a^{l-1}}\n    \\,\\rightarrow\n     \\underbrace{W^l a^{l-1}+b^l}_{z^{l}/logits }\n    \\,\\rightarrow\n    \\underbrace{P(z^l)}_{\\vec P/ \\text{softmax} /a^{l}}\n    \\,\\rightarrow\n    \\underbrace{L ( \\vec P, \\vec Y)}_{\\text{CrossEntropyLoss}}\n\\end{aligned}\n}\n\\]\n\\(Y\\) is the target vector or the Truth vector. This is a one hot encoded vector, example \\(Y=[0,1,0]\\), here the second element is the desired class.The training is done so that the CrossEntropyLoss is minimized using Gradient Descent algorithm.\n\\(P\\) is the Softmax output and is the activation of the last layer \\(a^l\\). This is a vector. All elements of the Softmax output add to 1; hence this is a probability distribution unlike a Sigmoid output.The Cross Entropy Loss \\(L\\) is a Scalar.\nNote the Index notation is the representation an element of a Vector or a Tensor, and is easier to deal with while deriving out the equations.\nSoftmax (in Index notation)\nBelow I am skipping the superscript part, which I used to represent the layers of the network.\n\\[\n\\begin{aligned}\np_j = \\frac{e^{z_j}}{\\sum_k e^{z_k}}\n\\end{aligned}\n\\]\nThis represent one element of the softmax vector, example \\(\\vec P= [p_1,p_2,p_3]\\)\nCross Entropy Loss (in Index notation)\nHere \\(y_i\\) is the indexed notation of an element in the target vector \\(Y\\).\n\\[\n\\begin{aligned}\nL = -\\sum_j y_j \\log p_j\n\\end{aligned}\n\\]\n\nThere are too many articles related to Back propagation, many of which are very good.However many explain in terms of index notation and though it is illuminating, to really use this with code, you need to understand how it translates to Matrix notation via Matrix Calculus and with help form StackOverflow related sites.\n\nCrossEntropy Loss with respect to Weight in last layer\n\\[\n\\mathbf {\n\\frac {\\partial L}{\\partial W^l}\n=  \\color{red}{\\frac {\\partial L}{\\partial z^l}} \\cdot \\color{green}{\\frac {\\partial z^l}{\\partial W^l}} \\rightarrow \\quad EqA1\n}\n\\]\nWhere \\[\nL = -\\sum_k y_k \\log {\\color{red}{p_k}} \\quad \\text{and} \\quad p_j = \\frac {e^{\\color{red}{z_j}}} {\\sum_k e^{z_k}}\n\\]\nIf you are confused with the indexes, just take a short example and substitute. Basically i,j,k etc are dummy indices used to illustrate in index notation the vectors.\nI am going to drop the superscript \\(l\\) denoting the layer number henceforth and focus on the index notation for the softmax vector \\(P\\) and target vector \\(Y\\)\nFrom Derivative of Softmax Activation -Alijah Ahmed\n$$ {\n\\[\\begin{aligned}\n\n    \\frac {\\partial L}{\\partial z_i} = \\frac {\\partial ({-\\sum_k y_k \\log {p_k})}}{\\partial z_i}\n   \\\\ \\\\ \\text {taking the summation outside} \\\\ \\\\\n   = -\\sum_k y_k\\frac {\\partial ({ \\log {p_k})}}{\\partial z_i}\n  \\\\ \\\\ \\color{grey}{\\text {since }\n  \\frac{d}{dx} (f(g(x))) = f'(g(x))g'(x) }\n  \\\\ \\\\\n  = -\\sum_k y_k \\cdot \\frac {1}{p_k} \\cdot \\frac {\\partial { p_k}}{\\partial z_i}\n  \n\\end{aligned}\\]\n} $$\nThe last term \\(\\frac {\\partial { p_k}}{\\partial z_i}\\) is the derivative of Softmax with respect to its inputs also called logits. This is easy to derive and there are many sites that describe it. Example [Derivative of SoftMax Antoni Parellada]. The more rigorous derivative via the Jacobian matrix is here The Softmax function and its derivative-Eli Bendersky\n\\[\n\\color{red}\n  {\n  \\begin{aligned}\n   \\frac {\\partial { p_i}}{\\partial z_i} = p_i(\\delta_{ij} -p_j)\n   \\\\ \\\\\n   \\delta_{ij} = 1 \\text{ when i =j}\n   \\\\\n   \\delta_{ij} = 0 \\text{ when i} \\ne \\text{j}\n  \\end{aligned}\n  }\n\\]\nUsing this above and from Derivative of Softmax Activation -Alijah Ahmed\n$$ {\n\\[\\begin{aligned}\n\n\\frac {\\partial L}{\\partial z_i} = -\\sum_k y_k \\cdot \\frac {1}{p_k} \\cdot \\frac {\\partial { p_k}}{\\partial z_i}\n\\\\ \\\\\n  =-\\sum_k y_k \\cdot \\frac {1}{p_k} \\cdot p_i(\\delta_{ij} -p_j)\n\\\\ \\\\ \\text{these i and j are dummy indices and we can rewrite  this as}\n\\\\ \\\\\n=-\\sum_k y_k \\cdot \\frac {1}{p_k} \\cdot p_k(\\delta_{ik} -p_i)\n\\\\ \\\\ \\text{taking the two cases and adding in above equation } \\\\ \\\\\n\\delta_{ik} = 1 \\text{ when i =k} \\text{ and }\n   \\delta_{ik} = 0 \\text{ when i} \\ne \\text{k}\n   \\\\ \\\\\n   = [- y_i \\cdot \\frac {1}{p_i} \\cdot p_i(1 -p_i)]+[-\\sum_{k \\ne i}  y_k \\cdot \\frac {1}{p_k} \\cdot p_k(0 -p_i) ]\n    \\\\ \\\\\n     = [- y_i \\cdot \\frac {1}{p_i} \\cdot p_i(1 -p_i)]+[-\\sum_{k \\ne i}  y_k \\cdot \\frac {1}{p_k} \\cdot p_k(0 -p_i) ]\n  \\\\ \\\\\n     = [- y_i(1 -p_i)]+[-\\sum_{k \\ne i}  y_k \\cdot (0 -p_i) ]\n      \\\\ \\\\\n     = -y_i + y_i.p_i + \\sum_{k \\ne i}  y_k.p_i\n     \\\\ \\\\\n     = -y_i + p_i( y_i + \\sum_{k \\ne i}  y_k)\n     \\\\ \\\\\n     = -y_i + p_i( \\sum_{k}  y_k)\n     \\\\ \\\\\n     \\text {note that } \\sum_{k}  y_k = 1  \\, \\text{as it is a One hot encoded Vector}\n     \\\\ \\\\\n     = p_i - y_i\n     \\\\ \\\\\n     \\frac {\\partial L}{\\partial z^l}  = p_i - y_i \\rightarrow \\quad \\text{EqA1.1}\n\\end{aligned}\\]\n} $$\nWe need to put this back in \\(EqA1\\). We now need to calculate the second term, to complete the equation\n\\[\n\\begin{aligned}\n\\frac {\\partial L}{\\partial W^l}\n=  \\color{red}{\\frac {\\partial L}{\\partial z^l}} \\cdot \\color{green}{\\frac {\\partial z^l}{\\partial W^l}}\n\\\\ \\\\\nz^{l} = (W^l a^{l-1}+b^l)\n\\\\\n\\color{green}{\\frac {\\partial z^l}{\\partial W^l} = (a^{l-1})^T}\n\\\\ \\\\ \\text{Putting all together} \\\\ \\\\\n\\frac {\\partial L}{\\partial W^l} = (p - y) \\cdot (a^{l-1})^T \\quad  \\rightarrow \\quad \\mathbf  {EqA1}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Backprop With Softmax and Cross Entropy"
    ]
  },
  {
    "objectID": "8_backpropogation_full.html#gradient-descent",
    "href": "8_backpropogation_full.html#gradient-descent",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Gradient descent",
    "text": "Gradient descent\nUsing Gradient descent we can keep adjusting the last layer like\n\\[\n     W{^l} = W{^l} -\\alpha \\cdot \\frac {\\partial L}{\\partial W^l}\n\\]\nNow let’s do the derivation for the inner layers",
    "crumbs": [
      "Backprop With Softmax and Cross Entropy"
    ]
  },
  {
    "objectID": "8_backpropogation_full.html#derivative-of-loss-with-respect-to-weight-in-inner-layers",
    "href": "8_backpropogation_full.html#derivative-of-loss-with-respect-to-weight-in-inner-layers",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Derivative of Loss with respect to Weight in Inner Layers",
    "text": "Derivative of Loss with respect to Weight in Inner Layers\nThe trick here is to find the derivative of the Loss with respect to the inner layer as a composition of the partial derivative we computed earlier. And also to compose each partial derivative as partial derivative with respect to either \\(z^x\\) or \\(w^x\\) but not with respect to \\(a^x\\). This is to make derivatives easier and intuitive to compute.\n\\[\n\\begin{aligned}\n\\frac {\\partial L}{\\partial W^{l-1}}\n=  \\color{blue}{\\frac {\\partial L}{\\partial z^{l-1}}} \\cdot\n     \\color{green}{\\frac {\\partial z^{l-1}}{\\partial W^{l-1}}} \\rightarrow \\text{EqA2}\n\\end{aligned}\n\\]\nWe represent the first part in terms of what we computed earlier ie \\(\\color{blue}{\\frac {\\partial L}{\\partial z^{l}}}\\)\n$$\n\\[\\begin{aligned}\n\\color{blue}{\\frac {\\partial L}{\\partial z^{l-1}}} =\n\\color{blue}{\\frac {\\partial L}{\\partial z^{l}}}.\n    \\frac {\\partial z^{l}}{\\partial a^{l-1}}.\n    \\frac {\\partial a^{l-1}}{\\partial z^{l-1}} \\rightarrow \\text{ Eq with respect to Prev Layer}\n  \\\\ \\\\\n  \\color{blue}{\\frac {\\partial L}{\\partial z^{l}}} = \\color{blue}{(p_i- y_i)}\n  \\text{ from the previous layer (from EqA1.1) }\n  \\\\ \\\\\n   z^l = w^l a^{l-1}+b^l\n    \\text{ which makes }\n    {\\frac {\\partial z^{l} }{\\partial a^{l-1}} = w^l} \\\\\n    \\text{ and }\na^{l-1} = \\sigma (z^{l-1})     \\text{ which makes }\n\\frac {\\partial a^{l-1}}{\\partial z^{l-1}} = \\sigma \\color{red}{'} (z^{l-1} )\n\n\n\\\\ \\\\ \\text{ Putting together we get the first part of Eq A2 }\n\\\\\\\\\n\\color{blue}{\\frac {\\partial L}{\\partial z^{l-1}}} = \\left( (W^l)^T \\cdot \\color{blue}{(p- y)} \\right) \\odot \\sigma \\color{red}{'} (z^{l-1} ) \\rightarrow \\text{EqA2.1 }\n\\\\ \\\\\nz^{l-1} = W^{l-1} a^{l-2}+b^{l-1}\n    \\text{ which makes }\n    \\color{green}{\\frac {\\partial z^{l-1}}{\\partial W^{l-1}}=(a^{l-2})^T}\n\\\\ \\\\\n\\frac {\\partial L}{\\partial W^{l-1}}\n=  \\color{blue}{\\frac {\\partial L}{\\partial z^{l-1}}} \\cdot\n     \\color{green}{\\frac {\\partial z^{l-1}}{\\partial W^{l-1}}} = \\left( \\left( (W^l)^T \\cdot \\color{blue}{(p- y)} \\right) \\odot \\sigma '(z^{l-1} ) \\right) \\cdot \\color{green}{(a^{l-2})^T}\n\\end{aligned}\\]\n$$\nNote All the other layers should use the previously calculated value of \\(\\color{blue}{\\frac {\\partial L}{\\partial z^{l-i}}}\\) where \\(i= current layer-1\\)\n\\[\n\\begin{aligned}\n\\frac {\\partial L}{\\partial W^{l-2}}\n=  \\color{blue}{\\frac {\\partial L}{\\partial z^{l-2}}} \\cdot\n     \\color{green}{\\frac {\\partial z^{l-2}}{\\partial W^{l-2}}}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Backprop With Softmax and Cross Entropy"
    ]
  },
  {
    "objectID": "8_backpropogation_full.html#implementation-in-python",
    "href": "8_backpropogation_full.html#implementation-in-python",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Implementation in Python",
    "text": "Implementation in Python\nHere is an implementation of a relatively simple Convolutional Neural Network to test out the forward and back-propagation algorithms given above https://github.com/alexcpn/cnn_in_python. The code is well commented and you will be able to follow the forward and backward propagation with the equations above. Note that the full learning cycle is not completed; but rather a few Convolutional layers, forward propagation and backward propogation for last few layers.",
    "crumbs": [
      "Backprop With Softmax and Cross Entropy"
    ]
  },
  {
    "objectID": "8_backpropogation_full.html#gradient-descent-1",
    "href": "8_backpropogation_full.html#gradient-descent-1",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Gradient descent",
    "text": "Gradient descent\nUsing Gradient descent we can keep adjusting the inner layers like\n\\[\n     W{^{l-1}} = W{^{l-1}} -\\alpha \\cdot \\frac {\\partial L}{\\partial W^{l-1}}\n\\]",
    "crumbs": [
      "Backprop With Softmax and Cross Entropy"
    ]
  },
  {
    "objectID": "8_backpropogation_full.html#some-implementation-details",
    "href": "8_backpropogation_full.html#some-implementation-details",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Some Implementation details",
    "text": "Some Implementation details\nFeel free to skip this section. These are some doubts that can come during implementation,and can be referred to if needed.\nFrom Index Notation to Matrix Notation\nThe above equations are correct only as far as the index notation is concerned. But practically we work with Weight matrices, and for that we need to write this Equation in Matrix Notation. For that some of the terms becomes Transposes, some matrix multiplication (dot product style) and some Hadamard product. (\\(\\odot\\)). This is illustrated and commented in the code and deviates from the equations as is,\nExample \\[\n\\frac{\\partial z^2}{\\partial W^2} = (a^{1})^T\n\\]\n\nThe Jacobian Matrix (and why Hadamard appears)\nLet \\(x \\in \\mathbb{R}^n\\) be an input vector and let an elementwise function \\(f\\) (e.g., sigmoid) produce an output vector \\(a \\in \\mathbb{R}^n\\):\n\\[\na = f(x), \\quad a_i = f(x_i)\n\\]\nIn the scalar case, the derivative is just:\n\\[\n\\frac{d}{dx} f(x) = f'(x)\n\\]\nIn the vector case, the derivative of a vector-valued function with respect to a vector input is the Jacobian matrix:\n\\[\nJ = \\frac{\\partial a}{\\partial x} \\in \\mathbb{R}^{n \\times n}, \\quad J_{ij} = \\frac{\\partial a_i}{\\partial x_j}\n\\]\nFor an elementwise function \\(a_i = f(x_i)\\), each output component depends only on the corresponding input component, so:\n\\[\n\\frac{\\partial a_i}{\\partial x_j} =\n\\begin{cases}\nf'(x_i) & \\text{if } i = j \\\\\n0 & \\text{if } i \\neq j\n\\end{cases}\n\\]\nand therefore the Jacobian is diagonal:\n\\[\n\\frac{\\partial a}{\\partial x} = \\text{diag}(f'(x))\n\\]\nWhy this becomes a Hadamard product in backprop\nSuppose we have a vector \\(v\\) coming from later in the chain rule (e.g., \\(v = \\frac{\\partial C}{\\partial a}\\)). Then:\n\\[\n\\frac{\\partial C}{\\partial x} = \\left( \\frac{\\partial a}{\\partial x} \\right)^T \\frac{\\partial C}{\\partial a} = \\text{diag}(f'(x)) \\cdot v = f'(x) \\odot v\n\\]\nSo the Hadamard product appears because the Jacobian of an elementwise function is diagonal, and multiplying by a diagonal matrix is the same as elementwise multiplication.\n\nWhat is basically done is to flatten the Matrix out\n\\[\n\\begin{aligned}\n\\text{Let's take a 2x2 matrix , X } =\n\\begin{bmatrix}\n                x_{ {1}{1} }  & x_{ {1}{2} } \\\\\n                x_{ {2}{1} }  & x_{ {2}{2} }\n\\end{bmatrix}\n\\end{aligned}\n\\] On which an element wise operation is done \\(a_{ {i}{j} } = \\sigma ({x_{ {i}{j} }})\\) Writing that out as matrix \\(A\\) \\[\n\\begin{aligned}\nA =\n\\begin{bmatrix}\n                a_{ {1}{1} }  & a_{ {1}{2} }   \\\\\n                a_{ {2}{1} }  & a_{ {2}{2} }   \\\\\n\\end{bmatrix}\n\\end{aligned}\n\\]\nThe partial derivative of the elements of A with its inputs is \\(\\frac {\\partial A }{\\partial x_{ {i}{j} }}\\)\n\\[\n\\begin{aligned}\n\\frac {\\partial \\vec A }{\\partial X} =\n\\begin{bmatrix}\n                a_{ {1}{1} }  & a_{ {1}{2} }  &  a_{ {2}{1} }  & a_{ {2}{2} }   \\\\\n\\end{bmatrix}\n\\end{aligned}\n\\] We vectorized the matrix; Now we need to take the partial derivative of the vector with each element of the matrix \\(X\\)\n\\[\n\\begin{aligned}\n\\frac {\\partial \\vec A }{\\partial X} =\n\\begin{bmatrix}\n\\frac{\\partial  a_{ {1}{1} } }{\\partial x_{ {1}{1} }} &   \\frac{\\partial  a_{ {1}{2} } }{\\partial x_{ {1}{1} }} &   \\frac{\\partial  a_{ {2}{1} } }{\\partial x_{ {1}{1} }} &   \\frac{\\partial  a_{ {2}{2}} }{\\partial x_{ {1}{1}}}  \\\\ \\\\\n\\frac{\\partial  a_{ {1}{1}} }{\\partial x_{ {1}{2}}} &   \\frac{\\partial  a_{ {1}{2}} }{\\partial x_{ {1}{2}}} &   \\frac{\\partial  a_{ {2}{1}} }{\\partial x_{ {1}{2}}} &   \\frac{\\partial  a_{ {2}{2}} }{\\partial x_{ {1}{2}}}  \\\\ \\\\\n\\frac{\\partial  a_{ {1}{1}} }{\\partial x_{ {2}{1}}} &   \\frac{\\partial  a_{ {1}{2}} }{\\partial x_{ {2}{1}}} &   \\frac{\\partial  a_{ {2}{1}} }{\\partial x_{ {2}{1}}} &   \\frac{\\partial  a_{ {2}{2}} }{\\partial x_{ {2}{1}}}  \\\\ \\\\\n\\frac{\\partial  a_{ {1}{1}} }{\\partial x_{ {2}{2}}} &   \\frac{\\partial  a_{ {1}{2}} }{\\partial x_{ {2}{2}}} &   \\frac{\\partial  a_{ {2}{1}} }{\\partial x_{ {2}{2}}} &   \\frac{\\partial  a_{ {2}{2}} }{\\partial x_{ {2}{2}}}  \\\\ \\\\\n\\end{bmatrix}\n\\end{aligned}\n\\] The non diagonal terms are of the form \\(\\frac{\\partial  a_{ {i}{j}} }{\\partial x_{ {k}{k}}}\\) and reduce to 0 and we get the resultant Jacobian Matrix as\n\\[\n\\begin{aligned}\n\\frac {\\partial \\vec A }{\\partial X} =\n\\begin{bmatrix}\n\\frac{\\partial  a_{ {i}{j}} }{\\partial x_{ {i}{i}}} & \\cdot \\cdot \\cdot & 0 \\\\\n0 & \\frac{\\partial  a_{ {i}{j}} }{\\partial x_{ {i}{i}}} & \\cdot \\cdot \\cdot  \\\\\n\\cdot \\cdot \\cdot  \\\\\n\\cdot \\cdot \\cdot  & \\cdot \\cdot \\cdot & \\frac{\\partial  a_{ {N}{N}} }{\\partial x_{ {N}{N}}}\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nHence \\(\\frac{\\partial a_{ {i}{j}}}{\\partial X}\\) can be written as \\(\\text{ diag}(f'(X))\\) ; \\((A =f(X))\\)\nNote that Multiplication of a vector by a diagonal matrix is element-wise multiplication or the Hadamard product; And matrices in Deep Learning implementation can be seen as stacked vectors for simplification.\nMore details about this here Jacobian Matrix for Element wise Operation on a Matrix (not Vector)\nNote that another way of interpreting this as treating weights as Tensor and then certain Jacobian operation can be treated as between Tensors and Vectors.",
    "crumbs": [
      "Backprop With Softmax and Cross Entropy"
    ]
  },
  {
    "objectID": "8_backpropogation_full.html#references",
    "href": "8_backpropogation_full.html#references",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "References",
    "text": "References\nEasier to follow (without explicit Matrix Calculus) though not really correct - Supervised Deep Learning Marc’Aurelio Ranzato DeepMind\nEasy to follow but lacking in some aspects - Notes on Backpropagation-Peter Sadowski Slightly hard to follow using the Jacobian - The Softmax function and its derivative-Eli Bendersky More difficult to follow with proper index notations (I could not) and probably correct - Backpropagation In Convolutional Neural Networks Jefkine",
    "crumbs": [
      "Backprop With Softmax and Cross Entropy"
    ]
  },
  {
    "objectID": "1_vectors_dot_product_and_perceptron.html",
    "href": "1_vectors_dot_product_and_perceptron.html",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "",
    "text": "Alex Punnen\n© All Rights Reserved\n\nContents\nEven the most complex Neural network is based on vectors and matrices, and it uses the concept of a cost function and algorithms like gradient descent to find a reduced cost. Then, it propagates the cost back to all constituents of the network proportionally via a method called back-propagation.\nHave you ever held an integrated circuit or chip in your hand or seen one? It looks overwhelmingly complex. But its base is the humble transistor and Boolean logic. To understand something complex, we need to understand the simpler constituents.\n\n\n\nMost people are familiar with neural networks, cost functions, gradient descent, and backpropagation. However, beyond these building blocks is the magic of representations.\nFeatures live in a multidimensional universe where the concept of a hyperplane classifies or clusters similar features together.\nThis idea applies equally to the simplest neural networks and to modern architectures such as Transformers.\nOne of the earliest neural networks, Rosenblatt’s Perceptron, introduced the idea of representing inputs as vectors and using the dot product to define a decision boundary — a hyperplane that separates input feature vectors.\nFirst a short refresher.\n\n\nA vector is an object that has both a magnitude and a direction. Example Force and Velocity. Both have magnitude as well as direction.\nHowever we need to specify also a context where this vector lives -Vector Space. For example when we are thinking about something like Force vector, the context is usually 2D or 3D Euclidean world.\n\n\n\n2Dvector\n\n\n\n\n\n3Dvector\n\n\n(Source: 3Blue1Brown)\nThe easiest way to understand the Vector is in such a geometric context, say 2D or 3D cartesian coordinates, and then extrapolate it for other Vector spaces which we encounter but cannot really imagine.\n\n\n\nVectors are represented as matrices. A Vector is a one dimensional matrix.A matrix is defined to be a rectangular array of numbers. Example here is a Euclidean Vector in three-dimensional Euclidean space (or \\(R^{3}\\)) with some magnitude and direction (from (0,0,0) origin in this case).\nA vector is represented either as column matrix (m1)or as a row matrix (1m).\n\\[\na = \\begin{bmatrix}\na_{1}\\\\a_{2}\\\\a_{3}\\\n\\end{bmatrix} = \\begin{bmatrix} a_{1} & a_{2} &a_{3}\\end{bmatrix}\n\\]\n\\(a_{1},a_{2},a_{3}\\) are the component scalars of the vector. A vector is represented as \\(\\vec a\\) in the Vector notation and as \\(a_{i}\\) in the Index Notation.\n\n\n\nThis intuition is especially helpful when we use dot products on neural network weight matrices.\n\n\n\nThis is a very important concept in linear algebra and is used in many places in machine learning.\nAlgebraically, the dot product is the sum of the products of the corresponding entries of the two sequences of numbers.\nif \\(\\vec a = \\left\\langle {a_1,a_2,a_3} \\right\\rangle\\) and \\(\\vec b = \\left\\langle {b_1,b_2,b_3} \\right\\rangle\\), then\n\\(\\vec a \\cdot \\vec b = {a_1}{b_1} + {a_2}{b_2} + {a_3}{b_3} = a_ib_i \\quad\\text {in index notation}\\)\nIn Matrix notation,\n\\[\n\\vec a \\cdot \\vec b = \\begin{bmatrix} a_{1} & a_{2} &a_{3}\\end{bmatrix} \\begin{bmatrix} b_{1}\\\\b_{2}\\\\b_{3}\\end{bmatrix} = a_ib_i\n\\]\nGeometrically, it is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them\n\\[\n\\vec a \\cdot \\vec b = \\left\\| {\\vec a} \\right\\|\\,\\,\\left\\| {\\vec b} \\right\\|\\cos \\theta\n\\]\n\n\n\ndotproduct\n\n\nNote- These definitions are equivalent when using Cartesian coordinates (Ref 8, 9)\n\n\n\nThe transpose of a matrix is an operator which flips a matrix over its diagonal; that is, it switches the row and column indices of the matrix A by producing another matrix, often denoted by \\(A^T\\).\nThis is important because the dot product of two vectors can be written as the matrix product of a row vector and a column vector: \\(\\vec a \\cdot \\vec b = a^T b\\)\n\n\n\n\nIf two vectors point in roughly the same direction, their dot product is positive. If they point in opposite directions, the dot product is negative.\nThis simple geometric fact becomes a powerful computational tool.\nImagine a problem where we want to classify whether a leaf is healthy or diseased based on certain features. Each leaf is represented as a feature vector in a two-dimensional space (for simplicity).\nIf we can find a weight vector such that:\nIts dot product with healthy leaf vectors is positive\nIts dot product with diseased leaf vectors is negative\nthen that weight vector defines a hyperplane that splits the feature space into two regions.\nThis is exactly how the Perceptron performs classification.\n\n\n\nhyperplane1\n\n\nImagine we have a problem of classifying if a leaf is healthy or not based on certain features of the leaf. For each leaf we have some feature vector set assume it is a 2D vector space with say color as the feature for simplicity.\nFor any input feature vector in that vector space, if we have a weight vector, whose dot product with one feature vector of the set of input vectors of a certain class (say leaf is healthy) is positive, and with the other set is negative, then that weight vector is splitting the feature vector hyper-plane into two.\nOr in a better way, which shows the vectors properly\n\n\n\nweightvector\n\n\nIn essence, we are using the weight vectors to split the hyper-plane into two distinctive sets.\nFor any new leaf, if we only extract the same features into a feature vector; we can dot product it with the trained weight vector and find out if it falls in healthy or deceased class.\nHere is a Colab notebook to play around with this.14\n\n\nImagine the weight vector \\(w\\) as a pointer. We want this pointer to be oriented such that: 1. It points generally in the same direction as Positive examples. 2. It points away from Negative examples.\nWe start with a random weight vector. Then, we iterate through our training data and check how the current \\(w\\) classifies each point.\n\nIf the classification is correct: We do nothing. The weight vector is already doing its job for this point.\nIf the classification is wrong: We need to “nudge” or rotate the weight vector to correct the error.\n\n\n\n\nLet’s say we have an input vector \\(x\\).\nCase 1: False Negative The input \\(x\\) is a Positive example (\\(y=1\\)), but our current \\(w\\) classified it as negative (dot product \\(w \\cdot x &lt; 0\\)). * Action: We need to rotate \\(w\\) towards \\(x\\). * Update: \\(w_{new} = w_{old} + x\\) * Result: Adding \\(x\\) to \\(w\\) makes the new vector more aligned with \\(x\\), increasing the dot product for the next time.\nCase 2: False Positive The input \\(x\\) is a Negative example (\\(y=0\\) or \\(-1\\)), but our current \\(w\\) classified it as positive (dot product \\(w \\cdot x &gt; 0\\)). * Action: We need to rotate \\(w\\) away from \\(x\\). * Update: \\(w_{new} = w_{old} - x\\) * Result: Subtracting \\(x\\) from \\(w\\) pushes it in the opposite direction, decreasing the dot product.\nWe can combine these rules into a single update equation. We often introduce a learning rate \\(\\eta\\) (a small number like 0.1) to make the updates smoother, preventing the weight vector from jumping around too wildly.\nFor each training example \\((x, y_{target})\\): 1. Compute prediction: \\(\\hat{y} = \\text{step\\_function}(w \\cdot x)\\) 2. Calculate error: \\(error = y_{target} - \\hat{y}\\) 3. Update weights: \\[ w = w + \\eta \\cdot error \\cdot x \\]\nThis is known as the Perceptron Learning Rule.\nNot all problems have their feature set which is linearly seperable. So this is a constraint of this system. For example XOR problem.\n\n\n\n\nThe fact that Perceptron could not be used for XOR or XNOR; which was demonstrated in 1969, by by Marvin Minsky and Seymour Papert led to the first AI winter, as much of the hype generated intially by Frank Rosenblatt’s discovery became a disillusionment.\n\n\n\nlinearseperable\n\n\n# Summary\nWhat we have seen so far is that we can represent real world features as vectors residing in some N dimensional space.\nWe can then use the concept of hyperplane to split the feature space into two distinctive sets.\nThis is the magic of Representation\nNext Perceptron Training",
    "crumbs": [
      "Vectors & Perceptron"
    ]
  },
  {
    "objectID": "1_vectors_dot_product_and_perceptron.html#the-magic-of-representation---vector-space-and-hyperplane",
    "href": "1_vectors_dot_product_and_perceptron.html#the-magic-of-representation---vector-space-and-hyperplane",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "",
    "text": "Most people are familiar with neural networks, cost functions, gradient descent, and backpropagation. However, beyond these building blocks is the magic of representations.\nFeatures live in a multidimensional universe where the concept of a hyperplane classifies or clusters similar features together.\nThis idea applies equally to the simplest neural networks and to modern architectures such as Transformers.\nOne of the earliest neural networks, Rosenblatt’s Perceptron, introduced the idea of representing inputs as vectors and using the dot product to define a decision boundary — a hyperplane that separates input feature vectors.\nFirst a short refresher.\n\n\nA vector is an object that has both a magnitude and a direction. Example Force and Velocity. Both have magnitude as well as direction.\nHowever we need to specify also a context where this vector lives -Vector Space. For example when we are thinking about something like Force vector, the context is usually 2D or 3D Euclidean world.\n\n\n\n2Dvector\n\n\n\n\n\n3Dvector\n\n\n(Source: 3Blue1Brown)\nThe easiest way to understand the Vector is in such a geometric context, say 2D or 3D cartesian coordinates, and then extrapolate it for other Vector spaces which we encounter but cannot really imagine.\n\n\n\nVectors are represented as matrices. A Vector is a one dimensional matrix.A matrix is defined to be a rectangular array of numbers. Example here is a Euclidean Vector in three-dimensional Euclidean space (or \\(R^{3}\\)) with some magnitude and direction (from (0,0,0) origin in this case).\nA vector is represented either as column matrix (m1)or as a row matrix (1m).\n\\[\na = \\begin{bmatrix}\na_{1}\\\\a_{2}\\\\a_{3}\\\n\\end{bmatrix} = \\begin{bmatrix} a_{1} & a_{2} &a_{3}\\end{bmatrix}\n\\]\n\\(a_{1},a_{2},a_{3}\\) are the component scalars of the vector. A vector is represented as \\(\\vec a\\) in the Vector notation and as \\(a_{i}\\) in the Index Notation.\n\n\n\nThis intuition is especially helpful when we use dot products on neural network weight matrices.\n\n\n\nThis is a very important concept in linear algebra and is used in many places in machine learning.\nAlgebraically, the dot product is the sum of the products of the corresponding entries of the two sequences of numbers.\nif \\(\\vec a = \\left\\langle {a_1,a_2,a_3} \\right\\rangle\\) and \\(\\vec b = \\left\\langle {b_1,b_2,b_3} \\right\\rangle\\), then\n\\(\\vec a \\cdot \\vec b = {a_1}{b_1} + {a_2}{b_2} + {a_3}{b_3} = a_ib_i \\quad\\text {in index notation}\\)\nIn Matrix notation,\n\\[\n\\vec a \\cdot \\vec b = \\begin{bmatrix} a_{1} & a_{2} &a_{3}\\end{bmatrix} \\begin{bmatrix} b_{1}\\\\b_{2}\\\\b_{3}\\end{bmatrix} = a_ib_i\n\\]\nGeometrically, it is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them\n\\[\n\\vec a \\cdot \\vec b = \\left\\| {\\vec a} \\right\\|\\,\\,\\left\\| {\\vec b} \\right\\|\\cos \\theta\n\\]\n\n\n\ndotproduct\n\n\nNote- These definitions are equivalent when using Cartesian coordinates (Ref 8, 9)\n\n\n\nThe transpose of a matrix is an operator which flips a matrix over its diagonal; that is, it switches the row and column indices of the matrix A by producing another matrix, often denoted by \\(A^T\\).\nThis is important because the dot product of two vectors can be written as the matrix product of a row vector and a column vector: \\(\\vec a \\cdot \\vec b = a^T b\\)",
    "crumbs": [
      "Vectors & Perceptron"
    ]
  },
  {
    "objectID": "1_vectors_dot_product_and_perceptron.html#dot-product-for-checking-vector-alignment",
    "href": "1_vectors_dot_product_and_perceptron.html#dot-product-for-checking-vector-alignment",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "",
    "text": "If two vectors point in roughly the same direction, their dot product is positive. If they point in opposite directions, the dot product is negative.\nThis simple geometric fact becomes a powerful computational tool.\nImagine a problem where we want to classify whether a leaf is healthy or diseased based on certain features. Each leaf is represented as a feature vector in a two-dimensional space (for simplicity).\nIf we can find a weight vector such that:\nIts dot product with healthy leaf vectors is positive\nIts dot product with diseased leaf vectors is negative\nthen that weight vector defines a hyperplane that splits the feature space into two regions.\nThis is exactly how the Perceptron performs classification.\n\n\n\nhyperplane1\n\n\nImagine we have a problem of classifying if a leaf is healthy or not based on certain features of the leaf. For each leaf we have some feature vector set assume it is a 2D vector space with say color as the feature for simplicity.\nFor any input feature vector in that vector space, if we have a weight vector, whose dot product with one feature vector of the set of input vectors of a certain class (say leaf is healthy) is positive, and with the other set is negative, then that weight vector is splitting the feature vector hyper-plane into two.\nOr in a better way, which shows the vectors properly\n\n\n\nweightvector\n\n\nIn essence, we are using the weight vectors to split the hyper-plane into two distinctive sets.\nFor any new leaf, if we only extract the same features into a feature vector; we can dot product it with the trained weight vector and find out if it falls in healthy or deceased class.\nHere is a Colab notebook to play around with this.14\n\n\nImagine the weight vector \\(w\\) as a pointer. We want this pointer to be oriented such that: 1. It points generally in the same direction as Positive examples. 2. It points away from Negative examples.\nWe start with a random weight vector. Then, we iterate through our training data and check how the current \\(w\\) classifies each point.\n\nIf the classification is correct: We do nothing. The weight vector is already doing its job for this point.\nIf the classification is wrong: We need to “nudge” or rotate the weight vector to correct the error.\n\n\n\n\nLet’s say we have an input vector \\(x\\).\nCase 1: False Negative The input \\(x\\) is a Positive example (\\(y=1\\)), but our current \\(w\\) classified it as negative (dot product \\(w \\cdot x &lt; 0\\)). * Action: We need to rotate \\(w\\) towards \\(x\\). * Update: \\(w_{new} = w_{old} + x\\) * Result: Adding \\(x\\) to \\(w\\) makes the new vector more aligned with \\(x\\), increasing the dot product for the next time.\nCase 2: False Positive The input \\(x\\) is a Negative example (\\(y=0\\) or \\(-1\\)), but our current \\(w\\) classified it as positive (dot product \\(w \\cdot x &gt; 0\\)). * Action: We need to rotate \\(w\\) away from \\(x\\). * Update: \\(w_{new} = w_{old} - x\\) * Result: Subtracting \\(x\\) from \\(w\\) pushes it in the opposite direction, decreasing the dot product.\nWe can combine these rules into a single update equation. We often introduce a learning rate \\(\\eta\\) (a small number like 0.1) to make the updates smoother, preventing the weight vector from jumping around too wildly.\nFor each training example \\((x, y_{target})\\): 1. Compute prediction: \\(\\hat{y} = \\text{step\\_function}(w \\cdot x)\\) 2. Calculate error: \\(error = y_{target} - \\hat{y}\\) 3. Update weights: \\[ w = w + \\eta \\cdot error \\cdot x \\]\nThis is known as the Perceptron Learning Rule.\nNot all problems have their feature set which is linearly seperable. So this is a constraint of this system. For example XOR problem.",
    "crumbs": [
      "Vectors & Perceptron"
    ]
  },
  {
    "objectID": "1_vectors_dot_product_and_perceptron.html#simple-hyperplane-split-is-not-possible-for-non-linearly-seperable-feature-set.",
    "href": "1_vectors_dot_product_and_perceptron.html#simple-hyperplane-split-is-not-possible-for-non-linearly-seperable-feature-set.",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "",
    "text": "The fact that Perceptron could not be used for XOR or XNOR; which was demonstrated in 1969, by by Marvin Minsky and Seymour Papert led to the first AI winter, as much of the hype generated intially by Frank Rosenblatt’s discovery became a disillusionment.\n\n\n\nlinearseperable\n\n\n# Summary\nWhat we have seen so far is that we can represent real world features as vectors residing in some N dimensional space.\nWe can then use the concept of hyperplane to split the feature space into two distinctive sets.\nThis is the magic of Representation\nNext Perceptron Training",
    "crumbs": [
      "Vectors & Perceptron"
    ]
  },
  {
    "objectID": "2_perceptron_training.html",
    "href": "2_perceptron_training.html",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "",
    "text": "Alex Punnen\n© All Rights Reserved\n\nContents",
    "crumbs": [
      "Perceptron Training"
    ]
  },
  {
    "objectID": "2_perceptron_training.html#perceptron-training-via-feature-vectors-hyperplane-split",
    "href": "2_perceptron_training.html#perceptron-training-via-feature-vectors-hyperplane-split",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Perceptron Training via Feature Vectors & HyperPlane split",
    "text": "Perceptron Training via Feature Vectors & HyperPlane split\nLet’s follow from the previous chapter of the Perceptron neural network.\nWe have seen how the concept of splitting the hyper-plane of feature set separates one type of feature vectors from other.\n![cornellperceptron][6]",
    "crumbs": [
      "Perceptron Training"
    ]
  },
  {
    "objectID": "2_perceptron_training.html#how-are-the-weights-learned",
    "href": "2_perceptron_training.html#how-are-the-weights-learned",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "How are the weights learned?",
    "text": "How are the weights learned?\nYou may have heard about Gradient Descent, which is the backbone of training modern neural networks. However, for the classic Perceptron, the learning algorithm is much simpler and relies on a geometric intuition.\nThe goal is to find a weight vector \\(w\\) that defines a hyperplane separating the two classes of data (e.g., Positive and Negative).\nNote this term hyperplane is used in the context of feature vector space and is used throughout neural network learning.\n\nThe Intuition: Nudging the Vector\nImagine the weight vector \\(w\\) as a pointer. We want this pointer to be oriented such that: 1. It points generally in the same direction as Positive examples. 2. It points away from Negative examples.\nWe start with a random weight vector. Then, we iterate through our training data and check how the current \\(w\\) classifies each point.\n\nIf the classification is correct: We do nothing. The weight vector is already doing its job for this point.\nIf the classification is wrong: We need to “nudge” or rotate the weight vector to correct the error.\n\n\n\nThe Update Rules\nLet’s say we have an input vector \\(x\\).\nCase 1: False Negative The input \\(x\\) is a Positive example (\\(y=1\\)), but our current \\(w\\) classified it as negative (dot product \\(w \\cdot x &lt; 0\\)). * Action: We need to rotate \\(w\\) towards \\(x\\). * Update: \\(w_{new} = w_{old} + x\\) * Result: Adding \\(x\\) to \\(w\\) makes the new vector more aligned with \\(x\\), increasing the dot product for the next time.\nCase 2: False Positive The input \\(x\\) is a Negative example (\\(y=0\\) or \\(-1\\)), but our current \\(w\\) classified it as positive (dot product \\(w \\cdot x &gt; 0\\)). * Action: We need to rotate \\(w\\) away from \\(x\\). * Update: \\(w_{new} = w_{old} - x\\) * Result: Subtracting \\(x\\) from \\(w\\) pushes it in the opposite direction, decreasing the dot product.\n\n\nThe Formal Algorithm\nWe can combine these rules into a single update equation. We often introduce a learning rate \\(\\eta\\) (a small number like 0.1) to make the updates smoother, preventing the weight vector from jumping around too wildly.\nFor each training example \\((x, y_{target})\\): 1. Compute prediction: \\(\\hat{y} = \\text{step\\_function}(w \\cdot x)\\) 2. Calculate error: \\(error = y_{target} - \\hat{y}\\) 3. Update weights: \\[ w = w + \\eta \\cdot error \\cdot x \\]\nThis is known as the Perceptron Learning Rule.\n\\[\n\\Delta w_j = \\eta (y_{target} - \\text{prediction}) x_j\n\\]\n\nNote: This is distinct from Gradient Descent. Gradient Descent requires a differentiable activation function to compute gradients (slope). The Perceptron uses a “step function” (hard 0 or 1) which is not differentiable. However, this simple rule is guaranteed to converge if the data is linearly separable.\n\nA more rigorous explanation of the proof can be found in the book Neural Networks by R.Rojas or this article.\nNext: Gradient Descent and Optimization",
    "crumbs": [
      "Perceptron Training"
    ]
  },
  {
    "objectID": "3_gradient_descent.html",
    "href": "3_gradient_descent.html",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "",
    "text": "Alex Punnen\n© All Rights Reserved\n\nContents",
    "crumbs": [
      "Gradient Descent"
    ]
  },
  {
    "objectID": "3_gradient_descent.html#neural-network-as-a-chain-of-functions",
    "href": "3_gradient_descent.html#neural-network-as-a-chain-of-functions",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Neural Network as a Chain of Functions",
    "text": "Neural Network as a Chain of Functions\nTo understand deep learning, we need to understand the concept of a neural network as a chain of functions.\nA Neural Network is essentially a chain of functions. It consists of a set of inputs connected through ‘weights’ to a set of activation functions, whose output becomes the input for the next layer, and so on.\n\n\n\nneuralnetwork\n\n\n\nThe Forward Pass\nLet’s consider a simple two-layer neural network.\n\n\\(x\\): Input vector\n\\(y\\): Output vector (prediction)\n\\(L\\): Number of layers\n\\(w^l, b^l\\): Weights and biases for layer \\(l\\)\n\\(a^l\\): Activation of layer \\(l\\) (we use sigmoid \\(\\sigma\\) here)\n\nThe flow of data (Forward Pass) can be represented as:\n\\[\nx \\rightarrow a^{1} \\rightarrow \\dots \\rightarrow a^{L} \\rightarrow y\n\\]\nFor any layer \\(l\\), the activation \\(a^l\\) is calculated as:\n\\[\n  a^{l} = \\sigma(w^l a^{l-1} + b^l)\n\\]\nwhere \\(a^0 = x\\) (the input).\nThe linear transformation:\n\\(z = w^T x + b\\)\ndefines a hyperplane (decision boundary) in the feature space.\nThe activation function then introduces non-linearity, allowing the network to combine multiple such hyperplanes into complex decision boundaries.\n\nWhy Non-Linearity Is Non-Negotiable\nWithout activation:\n\\[\nf(x) = W_L W_{L-1} \\dots W_1 x\n\\]\nThis collapses to:\n\\[\nf(x) = Wx\n\\]\nStill one big linear transformation and hence one hyperplane; the problems of not able to separate features will come. Only because of non-linearity, we can get multiple hyperplanes and hence a composable complex decision boundaries that can separate features.\nSo the concept of Vectors, Matrices and Hyperplanes remain the same as before. Let us explore the chain of functions part here\nA neural network with \\(L\\) layers can be represented as a nested function:\\[f(x) = f_L(...f_2(f_1(x))...)\\]\nEach “link” in the chain is a layer performing a linear transformation followed by a non-linear activation and cascading to the final output.\n\n\n\nThe Cost Function (Loss Function)\nTo train this network, we need to measure how “wrong” its predictions are compared to the true values. We do this using a Cost Function (or Loss Function).\nA simplest Loss function is just the difference between the predicted output and the true output ($ y(x) - a^L(x) $.)\nBut usually we use the square of the difference to make it a non-negative function.\nA common choice is the Mean Squared Error (MSE):\n\\[\nC = \\frac{1}{2n} \\sum_{x} \\|y(x) - a^L(x)\\|^2\n\\]\n\n\\(n\\): Number of training examples\n\\(y(x)\\): The true expected output (label) for input \\(x\\)\n\\(a^L(x)\\): The network’s predicted output for input \\(x\\)\n\n\n\nThe Goal of Training\nThe goal of training is to find the set of weights \\(w\\) and biases \\(b\\) that minimize this cost \\(C\\).\nThis means that we need to optimise each component of the function \\(f(x)\\) to reduce the cost proportional to its contribution to the final output. The method to do this is called Backpropagation. It helps us calculate the gradient of the cost function with respect to each weight and bias.\nOnce the gradient is calculated, we can use Gradient Descent to update the weights in the opposite direction of the gradient.\nGradient descent is a simple optimization algorithm that works by iteratively updating the weights in the opposite direction of the gradient.\nHowever neural network is a composition of vector spaces and linear transformations. Hence gradient descent acts on a very complex space.\nThere are two or three facts to understand about gradient descent:\n\nIt does not attempt to find the global minimum, but rather follows the local slope of the cost function and converges to a local minimum or a flat region. Saddle point is a good optimisation point.\nGradients can vanish or explode, leading to slow or unstable convergence. The practical solution to control this is to use learning rate and using adaptive learning rate methods like Adam or RMSprop.\nBatch Size matters: Calculating the gradient over the entire dataset (Batch Gradient Descent) is computationally expensive and memory-intensive. In practice, we use Stochastic Gradient Descent (SGD) (one example at a time) or, more commonly, Mini-batch Gradient Descent (a small batch of examples). This introduces noise into the gradient estimate, which paradoxically helps the optimization process escape shallow local minima and saddle points.",
    "crumbs": [
      "Gradient Descent"
    ]
  },
  {
    "objectID": "3_gradient_descent.html#optimization-gradient-descent-take-1",
    "href": "3_gradient_descent.html#optimization-gradient-descent-take-1",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Optimization: Gradient Descent — Take 1",
    "text": "Optimization: Gradient Descent — Take 1\nGradient Descent is a simple yet powerful optimization algorithm used to minimize functions by iteratively updating parameters in the direction that reduces the function’s output.\nFor basic scalar functions (e.g., \\(( f(x) = x^2 )\\)), the update rule is straightforward: \\[\nx \\leftarrow x - \\eta \\frac{df}{dx}\n\\] where \\(( \\eta )\\) is the learning rate.\nHowever, neural networks are not simple scalar functions. They are composite vector-valued functions — layers of transformations that take in high-dimensional input vectors and eventually output either vectors (like logits) or scalars (like loss values).\nUnderstanding how to optimize these complex, high-dimensional functions requires us to extend basic calculus: - The gradient vector helps when the function outputs a scalar but takes a vector input (e.g., a loss function w.r.t. weights). - The Jacobian matrix becomes important when both the input and the output are vectors (e.g., when computing gradients layer by layer in backpropagation).\nWe’ll build up to this step by step — starting with scalar gradients, then moving to vector calculus, Jacobians, and how backpropagation stitches it all together.\nLet’s take it one layer at a time.",
    "crumbs": [
      "Gradient Descent"
    ]
  },
  {
    "objectID": "3_gradient_descent.html#gradient-descent-for-scalar-functions",
    "href": "3_gradient_descent.html#gradient-descent-for-scalar-functions",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Gradient Descent for Scalar Functions",
    "text": "Gradient Descent for Scalar Functions\nConsider this simple system that composes two functions:\n\\[L = g(f(x, w_1), w_2)\\]\nWhere:\n\n\\(x\\) is your input (fixed, given by your data)\n\\(w_1\\) and \\(w_2\\) are parameters you can adjust (like weights in a neural network)\n\\(f\\) is the first function (think: first layer)\n\\(g\\) is the second function (think: second layer)\n\\(L\\) is the final output\n\nLet’s make this concrete with simple linear functions:\n\\[f(x, w_1) = x \\cdot w_1 + b_1\\]\n\\[g(z, w_2) = z \\cdot w_2 + b_2\\]\nSo the full composition is:\n\\[L = g(f(x, w_1), w_2) = (x \\cdot w_1 + b_1) \\cdot w_2 + b_2\\]\n\nRunning the Numbers: A Real Example\nLet’s pick actual values and see what happens:\nFixed values:\n\nInput: \\(x = 2.0\\)\nBias terms: \\(b_1 = 1.0\\), \\(b_2 = 0.5\\)\n\nCurrent parameter values:\n\n\\(w_1 = 0.5\\)\n\\(w_2 = 1.5\\)\n\nStep 1: Compute intermediate result from first function:\n\\[z = f(x, w_1) = 2.0  \\times  0.5 + 1.0 = 2.0\\]\nStep 2: Compute final output from second function:\n\\[L = g(z, w_2) = 2.0  \\times  1.5 + 0.5 = 3.5\\]\nThe problem: Suppose we want \\(L_{\\text{target}} = 5.0\\) instead!\nOur current error is:\n\\[E = \\frac{1}{2}(L - L_{\\text{target}})^2 = \\frac{1}{2}(3.5 - 5.0)^2 = \\frac{1}{2}(-1.5)^2 = 1.125\\]\nThe million-dollar question: How should we change \\(w_1\\) and \\(w_2\\) to reduce this error?\n\n\nThe Adjustment Problem: Which Direction? How Much?\nHere’s what we need to know:\n\nShould we increase or decrease \\(w_1\\)? (Which direction?)\nHow sensitive is \\(L\\) to changes in \\(w_1\\)? (How much?)\nSame questions for \\(w_2\\).\n\nThis is where derivatives come in! Specifically, we need:\n\\[\\frac{\\partial L}{\\partial w_1} \\quad  \\text{and} \\quad  \\frac{\\partial L}{\\partial w_2}\\]\nThese tell us:\n\nSign: Positive means “increase \\(w\\) increases \\(L\\)”, negative means the opposite\nMagnitude: Larger absolute value means \\(L\\) is more sensitive to changes in \\(w\\)\n\nBut there’s a complication: \\(w_1\\) doesn’t directly affect \\(L\\). It affects \\(f\\), which then affects \\(g\\), which then affects \\(L\\). This is a composition, and we need to trace the effect through multiple steps.\nThis is where the “Chain Rule” of Calculus comes into play.\n\n\nThe Chain of Effects\nLet’s visualize how changes propagate:\n\nChange w₁ → Affects f → Changes z → Affects g → Changes L\n\n↓ ↓ ↓ ↓ ↓\n\nΔw₁ ∂f/∂w₁ Δz ∂g/∂z ΔL\n\nSimilarly for \\(w_2\\) (but \\(w_2\\) directly affects \\(g\\)):\n\nChange w₂ → Affects g → Changes L\n\n↓ ↓ ↓\n\nΔw₂ ∂g/∂w₂ ΔL\n\nThe key insight: To find how \\(w_1\\) affects \\(L\\), we need to multiply the effects at each step.\nThis is the chain rule in action!\n\n\nThe Solution: Applying the Chain Rule\nFor our composition \\(L = g(f(x, w_1), w_2)\\), let’s introduce a shorthand: call \\(z = f(x, w_1)\\) the intermediate value.\nThen:\n\\[L = g(z, w_2)\\]\nComputing \\(\\frac{\\partial L}{\\partial w_1}\\):\nBy the chain rule of calculus:\n\\[\\frac{\\partial L}{\\partial w_1} = \\frac{\\partial L}{\\partial z} \\cdot  \\frac{\\partial z}{\\partial w_1}\\]\nLet’s compute each piece:\nPart 1: How does \\(L\\) change with \\(z\\)?\n\\[\\frac{\\partial L}{\\partial z} = \\frac{\\partial}{\\partial z}(z \\cdot w_2 + b_2) = w_2 = 1.5\\]\nPart 2: How does \\(z\\) change with \\(w_1\\)?\n\\[\\frac{\\partial z}{\\partial w_1} = \\frac{\\partial}{\\partial w_1}(x \\cdot w_1 + b_1) = x = 2.0\\]\nPutting it together:\n\\[\\frac{\\partial L}{\\partial w_1} = 1.5  \\times  2.0 = 3.0\\]\nInterpretation: If we increase \\(w_1\\) by 0.1, then \\(L\\) increases by approximately \\(3.0  \\times  0.1 = 0.3\\).\nComputing \\(\\frac{\\partial L}{\\partial w_2}\\):\nThis is simpler because \\(w_2\\) directly affects \\(g\\):\n\\[\\frac{\\partial L}{\\partial w_2} = \\frac{\\partial}{\\partial w_2}(z \\cdot w_2 + b_2) = z = 2.0\\]\nInterpretation: If we increase \\(w_2\\) by 0.1, then \\(L\\) increases by approximately \\(2.0  \\times  0.1 = 0.2\\).\n\n\nMaking the Update: Gradient Descent\nNow we can adjust our parameters! Since we want to increase \\(L\\) from 3.5 to 5.0, and both gradients are positive, we should increase both \\(w_1\\) and \\(w_2\\).\nUsing gradient descent with learning rate \\(\\alpha = 0.2\\):\n\\[w_1^{\\text{new}} = w_1 + \\alpha  \\cdot  \\frac{\\partial L}{\\partial w_1} = 0.5 + 0.2  \\times  3.0 = 0.5 + 0.6 = 1.1\\]\n\\[w_2^{\\text{new}} = w_2 + \\alpha  \\cdot  \\frac{\\partial L}{\\partial w_2} = 1.5 + 0.2  \\times  2.0 = 1.5 + 0.4 = 1.9\\]\nNote: We’re adding (not subtracting) because we want to increase \\(L\\). Normally in machine learning, we minimize error, so we’d use \\(w - \\alpha  \\cdot  \\frac{\\partial E}{\\partial w}\\).\n\n\nVerification: Did It Work?\nLet’s recompute with the new weights:\nStep 1: New intermediate value:\n\\[z^{\\text{new}} = x \\cdot w_1^{\\text{new}} + b_1 = 2.0  \\times  1.1 + 1.0 = 3.2\\]\nStep 2: New output:\n\\[L^{\\text{new}} = z^{\\text{new}} \\cdot w_2^{\\text{new}} + b_2 = 3.2  \\times  1.9 + 0.5 = 6.58\\]\nProgress check:\n\nBefore: \\(L = 3.5\\) (error from target = 1.5)\nAfter: \\(L = 6.58\\) (error from target = -1.58)\nWe overshot! But that’s okay - we moved in the right direction\n\nWith a smaller learning rate (say \\(\\alpha = 0.1\\)), we’d get:\n\n\\(w_1^{\\text{new}} = 0.8\\), \\(w_2^{\\text{new}} = 1.7\\)\n\\(z^{\\text{new}} = 2.6\\), \\(L^{\\text{new}} = 4.92\\)\nMuch closer to our target of 5.0!\n\nThis is how Gradient Descent works in a nutshell. The same concepts carry over in deep learning with some added complexity.",
    "crumbs": [
      "Gradient Descent"
    ]
  },
  {
    "objectID": "3_gradient_descent.html#gradient-descent-for-a-two-layer-neural-network-scalar-form",
    "href": "3_gradient_descent.html#gradient-descent-for-a-two-layer-neural-network-scalar-form",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Gradient Descent for a Two-Layer Neural Network (Scalar Form)",
    "text": "Gradient Descent for a Two-Layer Neural Network (Scalar Form)\nLet’s apply this to a simple neural network with one hidden layer. We have: * Input: \\(x\\) * Hidden Layer: 1 neuron with weight \\(w_1\\), bias \\(b_1\\), activation \\(\\sigma\\) * Output Layer: 1 neuron with weight \\(w_2\\), bias \\(b_2\\), activation \\(\\sigma\\) * Target: \\(y\\)\nForward Pass: 1. \\(z_1 = w_1 x + b_1\\) 2. \\(a_1 = \\sigma(z_1)\\) 3. \\(z_2 = w_2 a_1 + b_2\\) 4. \\(a_2 = \\sigma(z_2)\\) (This is our prediction \\(\\hat{y}\\))\nLoss Function: We use the Mean Squared Error (MSE) for this single example: \\[ C = \\frac{1}{2}(y - a_2)^2 \\]\nGoal: Find \\(\\frac{\\partial C}{\\partial w_1}, \\frac{\\partial C}{\\partial b_1}, \\frac{\\partial C}{\\partial w_2}, \\frac{\\partial C}{\\partial b_2}\\) to update the weights.\nBackward Pass (Deriving Gradients):\nLayer 2 (Output Layer): We want how \\(C\\) changes with \\(w_2\\). \\[ \\frac{\\partial C}{\\partial w_2} = \\frac{\\partial C}{\\partial a_2} \\cdot \\frac{\\partial a_2}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_2} \\]\n\n\\(\\frac{\\partial C}{\\partial a_2} = -(y - a_2)\\) (Derivative of \\(\\frac{1}{2}(y-a)^2\\))\n\\(\\frac{\\partial a_2}{\\partial z_2} = \\sigma'(z_2)\\) (Derivative of activation)\n\\(\\frac{\\partial z_2}{\\partial w_2} = a_1\\)\n\nSo, \\[ \\frac{\\partial C}{\\partial w_2} = -(y - a_2) \\sigma'(z_2) a_1 \\]\nLet’s define the “error term” for layer 2 as \\(\\delta_2 = -(y - a_2) \\sigma'(z_2)\\). Then: \\[ \\frac{\\partial C}{\\partial w_2} = \\delta_2 a_1 \\] \\[ \\frac{\\partial C}{\\partial b_2} = \\delta_2 \\cdot 1 = \\delta_2 \\]\nLayer 1 (Hidden Layer): We want how \\(C\\) changes with \\(w_1\\). The path is longer: \\(w_1 \\to z_1 \\to a_1 \\to z_2 \\to a_2 \\to C\\). \\[ \\frac{\\partial C}{\\partial w_1} = \\underbrace{\\frac{\\partial C}{\\partial a_2} \\cdot \\frac{\\partial a_2}{\\partial z_2}}_{\\delta_2} \\cdot \\frac{\\partial z_2}{\\partial a_1} \\cdot \\frac{\\partial a_1}{\\partial z_1} \\cdot \\frac{\\partial z_1}{\\partial w_1} \\]\n\nWe know the first part is \\(\\delta_2\\).\n\\(\\frac{\\partial z_2}{\\partial a_1} = w_2\\)\n\\(\\frac{\\partial a_1}{\\partial z_1} = \\sigma'(z_1)\\)\n\\(\\frac{\\partial z_1}{\\partial w_1} = x\\)\n\nSo, \\[ \\frac{\\partial C}{\\partial w_1} = \\delta_2 \\cdot w_2 \\cdot \\sigma'(z_1) \\cdot x \\]\nLet’s define the error term for layer 1 as \\(\\delta_1 = \\delta_2 w_2 \\sigma'(z_1)\\). Then: \\[ \\frac{\\partial C}{\\partial w_1} = \\delta_1 x \\] \\[ \\frac{\\partial C}{\\partial b_1} = \\delta_1 \\]\nThe Update: \\[ w_1 \\leftarrow w_1 - \\eta \\delta_1 x \\] \\[ w_2 \\leftarrow w_2 - \\eta \\delta_2 a_1 \\]\nThis pattern—calculating an error term \\(\\delta\\) at the output and propagating it back using the weights—is why it’s called Backpropagation.\nNote that we are using here scalar form of gradient descent and not directly applicable to real neural networks. But this gives us the intuition of how backpropagation works.",
    "crumbs": [
      "Gradient Descent"
    ]
  },
  {
    "objectID": "3_gradient_descent.html#some-other-notes-related-to-gradient-descent",
    "href": "3_gradient_descent.html#some-other-notes-related-to-gradient-descent",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Some other notes related to Gradient Descent",
    "text": "Some other notes related to Gradient Descent\nThe Loss/Cost function is a scalar function of the weights and biases.\nThe loss/error is a scalar function of all weights and biases.\nIn simpler Machine Learning problems like linear regression with MSE, the loss is a convex quadratic in the parameters, so optimization is well-behaved (a bowl-shaped surface)(e.g. see left in picture).\nIn deep learning, the loss becomes non-convex because it is the result of composing many nonlinear transformations. This creates a complex landscape with saddle points, flat regions, and multiple minima (e.g. see right in picture).\n\n\n\ncostfunction\n\n\nHow will Gradient Descent work in this case - non convex function?\nGradient descent does not attempt to find the global minimum, but rather follows the local slope of the cost function and converges to a local minimum or a flat region.\nThe Loss function is differentiable almost everywhere*. At any point in parameter space, the gradient indicates the direction of steepest local increase, and moving in the opposite direction reduces the cost. During optimization, the algorithm may encounter local minima or saddle points.\n(*The function is not differentiable at the point where the function is zero ex ReLU. This is not a problem in practice, as optimization algorithms handle such points using subgradients)\nIn practice, deep learning works well despite non-convexity, partly because modern networks have millions of parameters and their loss landscapes contain many saddle points and wide, flat minima rather than poor isolated local minima.\nAlso we rarely use full-batch gradient descent. Instead, we use variants such as Stochastic Gradient Descent (SGD) or mini-batch gradient descent that acts as form of sampling.\nIn these methods, gradients are computed using a single training example or a small batch of examples rather than the entire dataset.\nThe resulting gradient is an average over the batch and serves as a noisy approximation of the true gradient. This stochasticity helps the optimizer escape saddle points and sharp minima, enabling effective training in practice.\nNext: Backpropagation",
    "crumbs": [
      "Gradient Descent"
    ]
  },
  {
    "objectID": "4_backpropogation_chainrule.html",
    "href": "4_backpropogation_chainrule.html",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "",
    "text": "Alex Punnen\n© All Rights Reserved\n\nContents",
    "crumbs": [
      "Backprop (Chain Rule)"
    ]
  },
  {
    "objectID": "4_backpropogation_chainrule.html#how-backpropagation-works",
    "href": "4_backpropogation_chainrule.html#how-backpropagation-works",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "How Backpropagation works",
    "text": "How Backpropagation works\nConsider a neural network with multiple layers. The weight of layer \\(l\\) is \\(W^l\\). And for the previous layer it is \\(W^{(l-1)}\\).\nThe best way to understand backpropagation is visually and by the way it is done by the tree representation of 3Blue1Brown video linked here.\nThe below GIF is a representation of a single path in the last layer(\\(l\\) of a neural network; and it shows how the connection from previous layer - that is the activation of the previous layer and the weight of the current layer is affecting the output; and thereby the final Cost.\nThe central idea is how a small change in weight in the previous layer affects the final output of the network.\n Source : Author",
    "crumbs": [
      "Backprop (Chain Rule)"
    ]
  },
  {
    "objectID": "4_backpropogation_chainrule.html#writing-this-out-as-chain-rule",
    "href": "4_backpropogation_chainrule.html#writing-this-out-as-chain-rule",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Writing this out as Chain Rule",
    "text": "Writing this out as Chain Rule\nHere is a more detailed depiction of how the small change in weight adds through the chain to affect the final cost, and how much the small change of weight in an inner layer affect the final cost.\nThis is the Chain Rule of Calculus and the diagram is trying to illustrate that visually via a chain of activations, via a Computational Graph\n\\[\n\\frac{\\partial C_0}{\\partial W^l} = \\frac{\\partial C_0}{\\partial a^l} \\cdot \\frac{\\partial a^l}{\\partial z^l} \\cdot \\frac{\\partial z^l}{\\partial W^l}\n\\]\n Source : Author\nNext part of the recipe is adjusting the weights of each layers, depending on how they contribute to the Cost. We have already seen this in the previous chapter.\nThe weights in each layer are adjusted in proportion to how each layers weights affected the Cost function.\nThis is by calculating the new weight by following the negative of the gradient of the Cost function - basically by gradient descent.\n\\[\n  W^l_{new} = W^l_{old} - \\eta \\cdot \\frac{\\partial C_0}{\\partial W^l}\n\\]\nFor adjusting the weight in the \\((l-1)\\) layer, we do similar\nFirst calculate how the weight in this layer contributes to the final Cost or Loss\n\\[\n\\frac{\\partial C_0}{\\partial W^{l-1}} = \\frac{\\partial C_0}{\\partial a^{l-1}} \\cdot \\frac{\\partial a^{l-1}}{\\partial z^{l-1}} \\cdot \\frac{\\partial z^{l-1}}{\\partial W^{l-1}}\n\\]\nand using this. Basically we are using Chain rule to find the partial differential using the partial differentials calculated in earlier steps.\n\\[\n  W^{l-1}_{new} = W^{l-1}_{old} - \\eta \\cdot \\frac{\\partial C_0}{\\partial W^{l-1}}\n\\]",
    "crumbs": [
      "Backprop (Chain Rule)"
    ]
  },
  {
    "objectID": "4_backpropogation_chainrule.html#nerual-net-as-a-composition-of-vector-functions",
    "href": "4_backpropogation_chainrule.html#nerual-net-as-a-composition-of-vector-functions",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Nerual Net as a Composition of Vector Functions",
    "text": "Nerual Net as a Composition of Vector Functions\nLets first look at a neural network as a composition of vector functions.\nImagine a simple neural network with 3 layers. It is essentially a composition of three functions:\nA neural network is a composition of vector-valued functions, followed by a scalar-valued cost function:\n\\[\nC = \\text{Cost}(a_3) \\\\\na_3 = L_3(L_2(L_1(x)))\n\\]\nWhere \\(L_1\\), \\(L_2\\) and \\(L_3\\) are the three layers of the network and\nEach layer is defined as:\n\\[\nz_i = W_i a_{i-1} + b_i, \\quad a_i = \\sigma(z_i)\n\\]\nAnd gradient descent is defined as:\n\\[W_{i_{new}} = W_{i_{old}} - \\eta \\cdot \\partial C / \\partial W_i\\]\nProblem is to find the partial derivative of the loss function with respect to the weights at each layer.\nTo calculate how a change in the first layer’s weights (\\(W_1\\)) affects the final Cost (\\(C\\)), we have to trace the “path of influence” all the way through the network.\nA nudge in \\(W_1\\) changes the output of Layer 1. The change in Layer 1 changes the input to Layer 2. The change in Layer 2 changes the input to Layer 3. The change in Layer 3 changes the final Cost.\nMathematically, we multiply the derivatives (Linear Maps) of these links together:\nWe need to update weights of three layers\n\\[W_{1_{new}} = W_{1_{old}} - \\eta \\cdot \\partial C / \\partial W_1\\]\n\\[W_{2_{new}} = W_{2_{old}} - \\eta \\cdot \\partial C / \\partial W_2\\]\n\\[W_{3_{new}} = W_{3_{old}} - \\eta \\cdot \\partial C / \\partial W_3\\]\nAnd for that we need to find $ C / W_1 $, $ C / W_2 $, $ C / W_3 $.\nLets write down the chain rule for each layer:\n\\[\\frac{\\partial C}{\\partial W_1} = \\frac{\\partial C}{\\partial L_3} \\cdot \\frac{\\partial L_3}{\\partial L_2} \\cdot \\frac{\\partial L_2}{\\partial L_1} \\cdot \\frac{\\partial L_1}{\\partial W_1}\\]\n\\[\\frac{\\partial C}{\\partial W_2} = \\frac{\\partial C}{\\partial L_3} \\cdot \\frac{\\partial L_3}{\\partial L_2} \\cdot \\frac{\\partial L_2}{\\partial W_2}\\]\n\\[\\frac{\\partial C}{\\partial W_3} = \\frac{\\partial C}{\\partial L_3} \\cdot \\frac{\\partial L_3}{\\partial W_3}\\]\nWhy is this written this way? By the chain rule, the derivative of a composition of functions is the product of the derivatives of the functions. It is thus easy to calculate the gradient of the loss with respect to the weights of each layer.\nLets calculate the gradient of the loss with respect to the weights of the first layer.\nNotice something interesting?\n\nTo calculate \\(\\frac{\\partial C}{\\partial W_3}\\), we need \\(\\frac{\\partial C}{\\partial L_3}\\).\nTo calculate \\(\\frac{\\partial C}{\\partial W_2}\\), we need \\(\\frac{\\partial C}{\\partial L_3} \\cdot \\frac{\\partial L_3}{\\partial L_2}\\).\nTo calculate \\(\\frac{\\partial C}{\\partial W_1}\\), we need \\(\\frac{\\partial C}{\\partial L_3} \\cdot \\frac{\\partial L_3}{\\partial L_2} \\cdot \\frac{\\partial L_2}{\\partial L_1}\\).\n\nWe are re-calculating the same terms over and over again!\nIf we start from the Output (Layer 3) and move Backwards: 1. We calculate \\(\\frac{\\partial C}{\\partial L_3}\\) once. We use it to find the update for \\(W_3\\).\n\nWe pass this value back to find \\(\\frac{\\partial C}{\\partial L_2}\\) (which is \\(\\frac{\\partial C}{\\partial L_3} \\cdot \\frac{\\partial L_3}{\\partial L_2}\\)). We use it to find the update for \\(W_2\\).\nWe pass that value back to find \\(\\frac{\\partial C}{\\partial L_1}\\). We use it to find the update for \\(W_1\\).\n\nThis avoids redundant calculations and is why it’s called Backpropagation.\nIt is essentially Dynamic Programming applied to the Chain Rule.\n\nThe Backpropagation Algorithm Step-by-Step\nStep 1: The Output Layer (\\(L_3\\))\nWe want to find the gradient \\(\\frac{\\partial C}{\\partial W_3}\\). Using the Chain Rule: \\[ \\frac{\\partial C}{\\partial W_3} = \\frac{\\partial C}{\\partial a_3} \\cdot \\frac{\\partial a_3}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial W_3} \\]\nLet’s break it down term by term:\n\nDerivative of Cost w.r.t Activation (\\(\\frac{\\partial C}{\\partial a_3}\\)): For MSE \\(C = \\frac{1}{2}(a_3 - y)^2\\): \\[ \\frac{\\partial C}{\\partial a_3} = (a_3 - y) \\]\nDerivative of Activation w.r.t Input (\\(\\frac{\\partial a_3}{\\partial z_3}\\)): Since \\(a_3 = \\sigma(z_3)\\): \\[ \\frac{\\partial a_3}{\\partial z_3} = \\sigma'(z_3) \\]\nDerivative of Input w.r.t Weights (\\(\\frac{\\partial z_3}{\\partial W_3}\\)): Since \\(z_3 = W_3 a_2 + b_3\\): \\[ \\frac{\\partial z_3}{\\partial W_3} = a_2 \\]\n\nCombining them: We define the “error” term \\(\\delta_3\\) at the output layer as: \\[ \\delta_3 = \\frac{\\partial C}{\\partial z_3} = (a_3 - y) \\odot \\sigma'(z_3) \\]\n\nNote on \\(\\odot\\) (Hadamard Product): We use element-wise multiplication here because both \\((a_3 - y)\\) and \\(\\sigma'(z_3)\\) are vectors of the same size.\nThe Jacobian of an element-wise activation \\(\\sigma\\) is a diagonal matrix: \\[ \\frac{\\partial a}{\\partial z} = \\text{diag}(\\sigma'(z)) \\]\nSo multiplying by it is the same as a Hadamard product: \\[ \\text{diag}(\\sigma'(z)) \\, v = v \\odot \\sigma'(z) \\]\n\nWe will see the Jacobian and Gradient Vector later.\nSo the gradient for the weights is: \\[ \\frac{\\partial C}{\\partial W_3} = \\delta_3 \\cdot a_2^T \\]\n\nNote on Transpose (\\(a_2^T\\)): In backprop, we push gradients through a linear map \\(z = Wa + b\\). The Jacobian w.r.t. \\(a\\) is \\(W\\), so the chain rule gives:\n\\[ \\frac{\\partial C}{\\partial a} = W^T \\frac{\\partial C}{\\partial z} \\]\nThe transpose appears because we’re applying the transpose (adjoint) of the Jacobian to move gradients backward.\n\nResult: We have the update for \\(W_3\\).\n\\[W_{3_{new}} = W_{3_{old}} - \\eta \\cdot \\partial C / \\partial W_3\\]\nStep 2: Propagate Back to \\(L_2\\)\nNow we need to find the gradient for the second layer weights: \\(\\frac{\\partial C}{\\partial W_2}\\). Using the Chain Rule, we can reuse the error from the layer above: \\[ \\frac{\\partial C}{\\partial W_2} = \\frac{\\partial C}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial W_2} = \\delta_2 \\cdot a_1^T \\]\nBut what is \\(\\delta_2\\) (the error at layer 2)? \\[ \\delta_2 = \\frac{\\partial C}{\\partial z_2} = \\frac{\\partial C}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial z_2} \\]\nWe know \\(\\frac{\\partial C}{\\partial z_3} = \\delta_3\\). And since \\(z_3 = W_3 \\sigma(z_2) + b_3\\): \\[ \\frac{\\partial z_3}{\\partial z_2} = W_3 \\cdot \\sigma'(z_2) \\]\nSo, we can calculate \\(\\delta_2\\) by “backpropagating” \\(\\delta_3\\): \\[ \\delta_2 = (W_3^T \\cdot \\delta_3) \\odot \\sigma'(z_2) \\]\nThe Update Rule for Layer 2: \\[ \\frac{\\partial C}{\\partial W_2} = \\delta_2 \\cdot a_1^T \\]\nResult: We have the update for \\(W_2\\). \\[W_{2_{new}} = W_{2_{old}} - \\eta \\cdot \\frac{\\partial C}{\\partial W_2}\\]\nStep 3: Propagate Back to \\(L_1\\)\nWe repeat the exact same process to find the error at the first layer \\(\\delta_1\\). \\[ \\delta_1 = (W_2^T \\cdot \\delta_2) \\odot \\sigma'(z_1) \\]\nThe Update Rule for Layer 1: \\[ \\frac{\\partial C}{\\partial W_1} = \\delta_1 \\cdot x^T \\] (Recall that \\(a_0 = x\\), the input).\nResult: We have the update for \\(W_1\\). \\[W_{1_{new}} = W_{1_{old}} - \\eta \\cdot \\frac{\\partial C}{\\partial W_1}\\]\n\n\nSummary\nSo, Backpropagation is the efficient execution of the Chain Rule by utilizing the linear maps of each layer in reverse order. * It computes the local linear map (Jacobian) of a layer. * It takes the incoming gradient vector from the future layer. * It performs a Vector-Jacobian Product to pass the gradient to the past layer.\n\nNext A Simple NeuralNet with Back Propagation\n\n\n\n\nReferences\nhttp://neuralnetworksanddeeplearning.com/chap2.html\nhttps://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/",
    "crumbs": [
      "Backprop (Chain Rule)"
    ]
  },
  {
    "objectID": "6_neuralnetworkimpementation.html",
    "href": "6_neuralnetworkimpementation.html",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "",
    "text": "Alex Punnen\n© All Rights Reserved\n\nContents",
    "crumbs": [
      "Neural Network Implementation"
    ]
  },
  {
    "objectID": "6_neuralnetworkimpementation.html#a-simple-neuralnet-with-back-propagation",
    "href": "6_neuralnetworkimpementation.html#a-simple-neuralnet-with-back-propagation",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "A Simple NeuralNet with Back Propagation",
    "text": "A Simple NeuralNet with Back Propagation\nWith the derivative of the Cost function derived from the last chapter, we can code the network\nWe will use matrices to represent input and weight matrices.\nx = np.array(\n    [\n        [0,0,1],\n        [0,1,1],\n        [1,0,1],\n        [1,1,1]\n    ])\nThis is a 4*3 matrix. Note that each row is an input. lets take all this 4 as ‘training set’\ny = np.array(\n  [\n      [0],\n      [1],\n      [0],\n      [1]\n  ])\nNote you can change the output and try to train the Neural network\nThis is a 4*1 matrix that represent the expected output. That is for input [0,0,1] the output is [0] and for [0,1,1] the output is [1] etc.\nA neural network is implemented as a set of matrices representing the weights of the network.\nLet’s create a two layered network. Before that please note the formula for the neural network\nSo basically the output at layer l is the dot product of the weight matrix of layer l and input of the previous layer.\nNow let’s see how the matrix dot product works based on the shape of matrices.\n[m*n].[n*x] = [m*x]\n[m*x].[x*y] = [m*y]\nWe take the \\([m*n]\\) as the input matrix this is a \\([4*3]\\) matrix.\nSimilarly the output \\(y\\) is a \\([4*1]\\) matrix; so we have \\([m*y] =[4*1]\\)\nSo we have\nm=4\nn=3\nx=?\ny=1\nLets then create our two weight matrices of the above shapes, that represent the two layers of the neural network.\nw0 = x\nw1 = np.random.random((3,4))\nw2 = np.random.random((4,1))\nWe can have an array of the weights to loop through, but for the time being let’s hard-code these. Note that ‘np’ stands for the popular numpy array library in Python.\nWe also need to code in our non-linearity. We will use the Sigmoid function here.\ndef sigmoid(x):\n    return 1/(1+np.exp(-x))\n\n# derivative of the sigmoid\ndef derv_sigmoid(x):\n   return sigmoid(x)*(1-sigmoid(x))\nWith this we can have the output of first, second and third layer, using our equation of neural network forward propagation.\na0 = x\na1 = sigmoid(np.dot(a0,w1))\n\na2 = sigmoid(np.dot(a1,w2))\na2 is the calculated output from randomly initialized weights. So lets calculate the error by subtracting this from the expected value and taking the MSE.\n\\[\nC = \\frac{1}{2} \\|y-a^l\\|^2\n\\]\nc0 = ((y-a2)**2)/2\nNow we need to use the back-propagation algorithm to calculate how each weight has influenced the error and reduce it proportionally.\n\nWe use this to update weights in all the layers and do forward pass again, re-calculate the error and loss, then re-calculate the error gradient \\(\\frac{\\partial C}{\\partial w}\\) and repeat\n$$\n\\[\\begin{aligned}\n\nw^2 = w^2 - (\\frac {\\partial C}{\\partial w^2} )*learningRate \\\\ \\\\\n\nw^1 = w^1 - (\\frac {\\partial C}{\\partial w^1} )*learningRate\n\n\\end{aligned}\\]\n$$\nLet’s update the weights as per the formula (3) and (5) from last chapter\n\\[  \\mathbf{\n\\frac {\\partial C}{\\partial w^1} = \\sigma'(z^1) * (a^{0})^T*\\delta^{2}*w^2.\\sigma'(z^2) \\quad \\rightarrow \\mathbb Eq \\; (5)\n}\\]\n\\[\n\\delta^2 = (a^2-y)\n\\]\n\\[ \\mathbf{\n\\frac {\\partial C}{\\partial w^2}= \\delta^{2}*\\sigma^{'}(z^2) * (a^{1})^T \\quad \\rightarrow \\mathbb Eq \\; (3)\n}\n\\]",
    "crumbs": [
      "Neural Network Implementation"
    ]
  },
  {
    "objectID": "6_neuralnetworkimpementation.html#a-two-layered-neural-network-in-python",
    "href": "6_neuralnetworkimpementation.html#a-two-layered-neural-network-in-python",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "A Two layered Neural Network in Python",
    "text": "A Two layered Neural Network in Python\nBelow is a two layered Network; I have used the code from http://iamtrask.github.io/2015/07/12/basic-python-network/ as the basis. With minor changes to fit into how we derived the equations.\nimport numpy as np\n# seed random numbers to make calculation deterministic \nnp.random.seed(1)\n\n# pretty print numpy array\nnp.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n\n# let us code our sigmoid function\ndef sigmoid(x):\n    return 1/(1+np.exp(-x))\n\n# let us add a method that takes the derivative of x as well\ndef derv_sigmoid(x):\n   return sigmoid(x)*(1-sigmoid(x))\n\n#---------------------------------------------------------------\n\n# Two layered NW. Using from (1) and the equations we derived as explanations\n# (1) http://iamtrask.github.io/2015/07/12/basic-python-network/\n#---------------------------------------------------------------\n\n# set learning rate as 1 for this toy example\nlearningRate = 1\n\n# input x, also used as the training set here\nx = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n\n# desired output for each of the training set above\ny = np.array([[0,1,1,0]]).T\n\n# Explanation - as long as input has two ones, but not three, output is One\n\"\"\"\nInput [0,0,1]  Output = 0\nInput [0,1,1]  Output = 1\nInput [1,0,1]  Output = 1\nInput [1,1,1]  Output = 0\n\"\"\"\n\n# Randomly initialized weights\nweight1 =  np.random.random((3,4)) \nweight2 =  np.random.random((4,1)) \n\n# Activation to layer 0 is taken as input x\na0 = x\n\niterations = 1000\nfor iter in range(0,iterations):\n\n  # Forward pass - Straight Forward\n  z1= np.dot(x,weight1)\n  a1 = sigmoid(z1) \n  z2= np.dot(a1,weight2)\n  a2 = sigmoid(z2) \n  if iter == 0:\n    print(\"Initial Output \\n\",a2)\n\n  # Backward Pass - Backpropagation \n  delta2  = (a2-y)\n  #---------------------------------------------------------------\n  # Calculating change of Cost/Loss wrto weight of 2nd/last layer\n  # Eq (A) ---&gt; dC_dw2 = delta2*derv_sigmoid(z2)*a1.T\n  #---------------------------------------------------------------\n\n  dC_dw2_1  = delta2*derv_sigmoid(z2) \n  dC_dw2  = a1.T.dot(dC_dw2_1)\n  \n  #---------------------------------------------------------------\n  # Calculating change of Cost/Loss wrto weight of 2nd/last layer\n  # Eq (B)---&gt; dC_dw1 = derv_sigmoid(z1)*delta2*derv_sigmoid(z2)*weight2*a0.T\n  # dC_dw1 = derv_sigmoid(z1)*dC_dw2*weight2_1*a0.T\n  #---------------------------------------------------------------\n\n  dC_dw1 =  np.dot(dC_dw2_1, weight2.T) * derv_sigmoid(z1)\n  dC_dw1 = a0.T.dot(dC_dw1)\n\n  #---------------------------------------------------------------\n  #Gradient descent\n  #---------------------------------------------------------------\n \n  weight2 = weight2 - learningRate*(dC_dw2)\n  weight1 = weight1 - learningRate*(dC_dw1)\n\n\nprint(\"New output\",a2)\n\n#---------------------------------------------------------------\n# Training is done, weight2 and weight2 are primed for output y\n#---------------------------------------------------------------\n\n# Lets test out, two ones in input and one zero, output should be One\nx = np.array([[1,0,1]])\nz1= np.dot(x,weight1)\na1 = sigmoid(z1) \nz2= np.dot(a1,weight2)\na2 = sigmoid(z2) \nprint(\"Output after Training is \\n\",a2)\nOutput\nInitial Output \n [[ 0.758]\n [ 0.771]\n [ 0.791]\n [ 0.801]]\nNew output [[ 0.028]\n [ 0.925]\n [ 0.925]\n [ 0.090]]\nOutput after Training is \n [[ 0.925]]\nWe have trained the NW for getting the output similar to \\(y\\); that is [0,1,0,1]\nThe code in Colab\nNext: Back Propagation Pass 3 (Matrix Calculus)",
    "crumbs": [
      "Neural Network Implementation"
    ]
  },
  {
    "objectID": "7_backpropogation_matrix_calculus.html",
    "href": "7_backpropogation_matrix_calculus.html",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "",
    "text": "Alex Punnen\n© All Rights Reserved\n\nContents",
    "crumbs": [
      "Backprop (Matrix Calculus)"
    ]
  },
  {
    "objectID": "7_backpropogation_matrix_calculus.html#back-propagation--matrix-calculus",
    "href": "7_backpropogation_matrix_calculus.html#back-propagation--matrix-calculus",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Back Propagation -Matrix Calculus",
    "text": "Back Propagation -Matrix Calculus\nThe previous chapters we used a Scalar derivation of the Back Propagation formula to implement it in a simple two layer neural network. What we have done is is to use Hadamard product and matrix transposes with scalar derivation alignment.\nBut we have not really explained why we use Hadamard product and matrix transposes with scalar derivation alignment.\nThis is due to Matrix Calculus which is the real way in which we should be deriving the Back Propagation formula.\nLets explore this in this chapter. Note that we are still not using a Softmax activation function in the output layer as is usually the case with Deep Neural Networks. Deriving the Back Propagation formula with Softmax activation function is bit more complex and we will do that in a later chapter.\nLet’s take the previous two layered simple neural network,with a Mean Square Error Loss function, and derive the Back Propagation formula with Matrix Calculus now.\nLet’s write the equation of the following neural network\nx is the Input\ny is the Output.\nl is the number of layers of the Neural Network.\na is the activation function ,(we use sigmoid here)\n\\[\nx \\rightarrow a^{l-1} \\rightarrow  a^{l} \\rightarrow  y\n\\]\nWhere the activation \\(a^l\\) is \\[\n  a^{l} = \\sigma(W^l a^{l-1}+b^l).\n\\]\nand\n\\[\na^{l} = \\sigma(z^l) \\quad where \\quad\nz^l =W^l a^{l-1} +b^l\n\\]\nOur two layer neural network can be written as\n\\[\n\\mathbf { a^0 \\rightarrow a^{1} \\rightarrow  a^{2} \\rightarrow  y }\n\\]\n(\\(a^2\\) does not denote the exponent but just that it is of layer 2)",
    "crumbs": [
      "Backprop (Matrix Calculus)"
    ]
  },
  {
    "objectID": "7_backpropogation_matrix_calculus.html#some-math-intuition",
    "href": "7_backpropogation_matrix_calculus.html#some-math-intuition",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Some Math Intuition",
    "text": "Some Math Intuition\nThe concept of a Neural Network as a composition of functions remains central.\nIn our network, most layers represent functions that map a Vector to a Vector (\\(\\mathbb{R}^n \\to \\mathbb{R}^m\\)). For example, the hidden layers take an input vector and produce an activation vector.\nHowever, the final step—calculating the Loss—is different. It maps the final output vector \\(a^L\\) (and the target \\(y\\)) to a single Scalar value, the Cost \\(C\\) (\\(\\mathbb{R}^n \\to \\mathbb{R}\\)).\n\nGradient Vector\nWhen we take the derivative of a scalar-valued function (like the Cost \\(C\\)) with respect to a vector (like the weights \\(w\\)), the result is a vector of the same size as \\(w\\). This is called the Gradient Vector.\n\\[\n\\nabla_w C = \\begin{bmatrix} \\frac{\\partial C}{\\partial w_1} \\\\ \\vdots \\\\ \\frac{\\partial C}{\\partial w_n} \\end{bmatrix}\n\\]\nWhy is it called a “gradient”?\nBecause the gradient points in the direction of steepest increase of the function.\nMoving a tiny step along \\(+\\nabla_w C\\) increases the cost the fastest.\nMoving a tiny step against it, i.e. along \\(-\\nabla_w C\\), decreases the cost the fastest.\nThat’s exactly why gradient descent updates parameters like this:\n\\[\nw \\leftarrow w - \\eta \\nabla_w C\n\\]\nwhere \\(\\eta\\) is the learning rate.\nSo the gradient vector is more than a list of derivatives—it’s the local direction that tells us how to change parameters to reduce the loss.\nSee the Colab 1 for a generated visualization of this.\nThe first image is the plotting of the Cost function\n\n\n\ngradient_vector\n\n\nThe second image where you see the cones are the gradient vector of the Cost function wrto weights plotted in 3D space.\n\n\n\ngradient_vector",
    "crumbs": [
      "Backprop (Matrix Calculus)"
    ]
  },
  {
    "objectID": "7_backpropogation_matrix_calculus.html#jacobian-matrix",
    "href": "7_backpropogation_matrix_calculus.html#jacobian-matrix",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Jacobian Matrix",
    "text": "Jacobian Matrix\nThe second key concept is the Jacobian Matrix.\nAs mentioned earlier, in our network, most layers represent functions that map a Vector to a Vector (\\(\\mathbb{R}^n \\to \\mathbb{R}^m\\)). For example, a hidden layer takes an input vector \\(x\\) and produces an activation vector \\(a\\).\nWhat is the derivative of a vector-valued function with respect to a vector input? This is where the Jacobian comes in.\nFor a function \\(f: \\mathbb{R}^n \\to \\mathbb{R}^m\\) that maps an input vector \\(x\\) of size \\(n\\) to an output vector \\(y\\) of size \\(m\\), the derivative is an \\(m \\times n\\) matrix called the Jacobian Matrix \\(J\\).\nThe entry \\(J_{ij}\\) is the partial derivative of the \\(i\\)-th output component with respect to the \\(j\\)-th input component:\n\\[\nJ_{ij} = \\frac{\\partial f_i}{\\partial x_j}\n\\]\n\nThe Chain Rule with Matrices\nThe beauty of the Jacobian is that it allows us to generalize the chain rule.\nFor scalar functions, the chain rule is just multiplication: \\((f \\circ g)'(x) = f'(g(x)) \\cdot g'(x)\\).\nFor vector functions, the chain rule becomes Matrix Multiplication of the Jacobians:\nIf we have a composition of functions \\(y = f(g(x))\\), and we let \\(A\\) be the Jacobian of \\(f\\) and \\(B\\) be the Jacobian of \\(g\\), then the Jacobian of the composition is simply the matrix product \\(A \\cdot B\\).\n\\[\n(A \\cdot B)_{ij} = \\sum_{k=1}^m A_{ik} \\cdot B_{kj}\n\\]\nSo, the Jacobian is a matrix of partial derivatives that represents the local linear approximation of a vector function. When we say the Jacobian represents a “local linear approximation,” we mean:\n\\[ \\text{Change in Output} \\approx \\text{Jacobian Matrix} \\cdot \\text{Change in Input} \\]\n\\[ \\Delta y \\approx J \\cdot \\Delta x \\]\nIt tells us: “If I nudge the input vector by a tiny vector \\(\\Delta x\\), the output vector will change by roughly the matrix-vector product \\(J \\cdot \\Delta x\\).”\n\n\nBackPropogation Trick - VJP (Vector Jacobian Product) and JVP (Jacobian Vector Product)\nThere is one more trick that we can use to make backpropogation more efficient.\nLet me explain with an example.\nSuppose we have a chain of functions: \\(y = f(g(h(x)))\\). To find the derivative \\(\\frac{\\partial y}{\\partial x}\\), the chain rule tells us to multiply the Jacobians: \\[ J_{total} = J_f \\cdot J_g \\cdot J_h \\]\nIf \\(x, h, g, f\\) are all vectors of size 1000, then each Jacobian is a \\(1000 \\times 1000\\) matrix. Multiplying them is expensive (\\(O(N^3)\\)).\nHowever, in Backpropagation, we always start with a scalar Loss function. The final derivative \\(\\frac{\\partial C}{\\partial y}\\) is a row vector (size \\(1 \\times N\\)).\nSo we are computing: \\[ \\nabla C = \\underbrace{\\frac{\\partial C}{\\partial y}}_{1 \\times N} \\cdot \\underbrace{J_f}_{N \\times N} \\cdot \\underbrace{J_g}_{N \\times N} \\cdot \\underbrace{J_h}_{N \\times N} \\]\nNotice the order of operations matters! 1. Jacobian-Matrix Product: If we multiply the matrices first (\\(J_f \\cdot J_g\\)), we do expensive matrix-matrix multiplication. 2. Vector-Jacobian Product (VJP): If we multiply from left to right: * \\(v_1 = \\frac{\\partial C}{\\partial y} \\cdot J_f\\) (Vector \\(\\times\\) Matrix \\(\\to\\) Vector) * \\(v_2 = v_1 \\cdot J_g\\) (Vector \\(\\times\\) Matrix \\(\\to\\) Vector) * \\(v_3 = v_2 \\cdot J_h\\) (Vector \\(\\times\\) Matrix \\(\\to\\) Vector)\nWe never explicitly compute or store the full Jacobian matrix. We only compute the product of a vector with the Jacobian. This is much faster (\\(O(N^2)\\)) and uses less memory.\nThis is the secret sauce of efficient Backpropagation!\n\n\nHadamard Product\nAnother important operation we use is the Hadamard Product (denoted by \\(\\odot\\) or sometimes \\(\\circ\\)). This is simply element-wise multiplication of two vectors or matrices of the same size.\n\\[\n\\begin{bmatrix} a_1 \\\\ a_2 \\end{bmatrix} \\odot \\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix} = \\begin{bmatrix} a_1 \\cdot b_1 \\\\ a_2 \\cdot b_2 \\end{bmatrix}\n\\]\nIt is different from the dot product (which sums the results to a scalar) and matrix multiplication. In backpropagation, it often appears when we apply the chain rule through an activation function that operates element-wise (like sigmoid or ReLU).",
    "crumbs": [
      "Backprop (Matrix Calculus)"
    ]
  },
  {
    "objectID": "7_backpropogation_matrix_calculus.html#backpropagation-derivation",
    "href": "7_backpropogation_matrix_calculus.html#backpropagation-derivation",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Backpropagation Derivation",
    "text": "Backpropagation Derivation\n\nThe 2-Layer Neural Network Model\nFor this derivation, we use a simple 2-layer network (one hidden layer, one output layer):\n\\[\nx \\xrightarrow{W^1, b^1} a^1 \\xrightarrow{W^2, b^2} a^2\n\\]\nForward Pass Equations: 1. Hidden Layer: \\[ z^1 = W^1 x + b^1 \\] \\[ a^1 = \\sigma(z^1) \\] 2. Output Layer: \\[ z^2 = W^2 a^1 + b^2 \\] \\[ a^2 = \\sigma(z^2) \\]\nWe use the Mean Squared Error (MSE) loss function: \\[ C = \\frac{1}{2} \\|y - a^2\\|^2 \\]\n\n\nGradient Vector/2D-Tensor of Loss function in last layer\n\\[\nC = \\frac{1}{2} \\|y - a^2\\|^2 = \\frac{1}{2} \\sum_j (y_j-a^2_j)^2\n\\]\nWhere: \\[\na^2 = \\sigma(z^2) \\quad \\text{and} \\quad z^2 = W^2 a^1 + b^2\n\\]\nWe want to find \\(\\frac{\\partial C}{\\partial W^2}\\). Using the Chain Rule:\n\\[\n\\frac{\\partial C}{\\partial W^2} = \\frac{\\partial C}{\\partial z^2} \\cdot \\frac{\\partial z^2}{\\partial W^2}\n\\]\nLet’s define the error term \\(\\delta^2\\) as the derivative of the cost with respect to the pre-activation \\(z^2\\):\n\\[\n\\delta^2 \\equiv \\frac{\\partial C}{\\partial z^2} = \\frac{\\partial C}{\\partial a^2} \\odot \\frac{\\partial a^2}{\\partial z^2}\n\\]\n\n\\(\\frac{\\partial C}{\\partial a^2} = (a^2 - y)\\)\n\\(\\frac{\\partial a^2}{\\partial z^2} = \\sigma'(z^2)\\)\n\nSo, using the Hadamard product (\\(\\odot\\)) for element-wise multiplication:\n\nNote that none of these terms are exponents but super scripts.!\nHadamard product or Element-wise multiplication\nThe confusion usually lies in this term:\\[\\frac{\\partial a}{\\partial z}\\]\nSince \\(a\\) is a vector and \\(z\\) is a vector, the derivative of one with respect to the other is technically a Jacobian Matrix, not a vector.However, because the activation function \\(\\sigma\\) is applied element-wise (i.e., \\(a_i\\) depends only on \\(z_i\\), not on \\(z_j\\) - That is - activation function in one layer is just dependent of the output of only the previous layer and no other layers), all off-diagonal elements of this Jacobian are zero.\n\\[\nJ = \\frac{\\partial a}{\\partial z} = \\begin{bmatrix}\n\\sigma'(z_1) & 0 & \\dots \\\\\n0 & \\sigma'(z_2) & \\dots \\\\\n\\vdots & \\vdots & \\ddots\n\\end{bmatrix}\\]\nWhen you apply the chain rule, you are multiplying the gradient vector \\(\\nabla_a C\\) by this diagonal matrix \\(J\\) - VJP (Vector-Jacobian Product).\nKey Identity: Multiplying a vector by a diagonal matrix is mathematically identical to taking the Hadamard product of the vector and the diagonal elements and this is why we use Hadamard product in backpropogation.\n\\[\n\\delta^2 = (a^2 - y) \\odot \\sigma'(z^2)\n\\]\n\nNow for the second part, we need to find how the Cost changes with respect to the weights \\(W^2\\).\nWe know that \\(z^2 = W^2 a^1\\). In index notation, for a single element \\(z^2_i\\): \\[ z^2_i = \\sum_k W^2_{ik} a^1_k \\]\nWe want to find \\(\\frac{\\partial C}{\\partial W^2_{ik}}\\). Using the chain rule: \\[ \\frac{\\partial C}{\\partial W^2_{ik}} = \\frac{\\partial C}{\\partial z^2_i} \\cdot \\frac{\\partial z^2_i}{\\partial W^2_{ik}} \\]\n\nWe already defined \\(\\frac{\\partial C}{\\partial z^2_i} = \\delta^2_i\\).\nFrom the linear equation \\(z^2_i = \\dots + W^2_{ik} a^1_k + \\dots\\), the derivative with respect to \\(W^2_{ik}\\) is simply \\(a^1_k\\).\n\nSo: \\[ \\frac{\\partial C}{\\partial W^2_{ik}} = \\delta^2_i \\cdot a^1_k \\]\nIf we organize these gradients into a matrix, the element at row \\(i\\) and column \\(k\\) is the product of the \\(i\\)-th element of \\(\\delta^2\\) and the \\(k\\)-th element of \\(a^1\\).\nLet’s visualize the matrix of gradients \\(\\nabla W\\):\\[\\nabla W =\n\\begin{bmatrix}\n\\frac{\\partial C}{\\partial W_{11}} & \\frac{\\partial C}{\\partial W_{12}} \\\\\n\\frac{\\partial C}{\\partial W_{21}} & \\frac{\\partial C}{\\partial W_{22}}\n\\end{bmatrix}\\]Substitute the result from step 3 (\\(\\delta_i \\cdot a_k\\)):\\[\\nabla W =\n\\begin{bmatrix}\n\\delta_1 a_1 & \\delta_1 a_2 \\\\\n\\delta_2 a_1 & \\delta_2 a_2\n\\end{bmatrix}\\]\nThis is exactly the definition of the Outer Product \\(\\otimes\\) of two vectors:\n\\[\n\\frac{\\partial C}{\\partial W^2} =  \\delta^2 \\otimes a^1 = \\delta^2  (a^1)^T \\quad \\rightarrow (Eq \\; 3)\n\\]\nThis gives us the gradient matrix for the last layer weights.",
    "crumbs": [
      "Backprop (Matrix Calculus)"
    ]
  },
  {
    "objectID": "7_backpropogation_matrix_calculus.html#jacobian-of-loss-function-in-inner-layer",
    "href": "7_backpropogation_matrix_calculus.html#jacobian-of-loss-function-in-inner-layer",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Jacobian of Loss function in Inner Layer",
    "text": "Jacobian of Loss function in Inner Layer\nNow let’s do the same for the inner layer (\\(W^1\\)).\n\\[\n\\frac{\\partial C}{\\partial W^1} = \\frac{\\partial C}{\\partial z^1} \\cdot \\frac{\\partial z^1}{\\partial W^1} = \\delta^1 (a^0)^T\n\\]\nWe need to find \\(\\delta^1 = \\frac{\\partial C}{\\partial z^1}\\). We can backpropagate the error \\(\\delta^2\\) from the next layer:\n\\[\n\\delta^1 = \\frac{\\partial C}{\\partial z^1} = \\left( (W^2)^T \\delta^2 \\right) \\odot \\sigma'(z^1)\n\\]\nExplanation: 1. We propagate \\(\\delta^2\\) backwards through the weights \\((W^2)^T\\). 2. We multiply element-wise by the derivative of the activation function \\(\\sigma'(z^1)\\).\nPutting it all together:\n\\[\n\\frac{\\partial C}{\\partial W^1} = \\left( (W^2)^T \\delta^2 \\odot \\sigma'(z^1) \\right) (a^0)^T \\quad \\rightarrow (Eq \\; 5)\n\\]\n\nSummary of Backpropagation Equations\n\nCompute Output Error: \\[ \\delta^L = (a^L - y) \\odot \\sigma'(z^L) \\]\nBackpropagate Error: \\[ \\delta^l = ((W^{l+1})^T \\delta^{l+1}) \\odot \\sigma'(z^l) \\]\nCompute Gradients: \\[ \\frac{\\partial C}{\\partial W^l} = \\delta^l (a^{l-1})^T \\]\n\n \n\n\nSummary of Backpropagation Equations in terms of say Numpy\nHere is how these equations translate to Python code using NumPy, assuming standard column vectors (shape (N, 1)).\n# Forward pass context:\n# x, a1, a2 are column vectors\n# W1, W2 are weight matrices\n# sigmoid_prime(z) is the derivative of activation\n\n# 1. Compute Output Error (Hadamard Product)\n# '*' operator in numpy is element-wise multiplication (Hadamard)\ndelta2 = (a2 - y) * sigmoid_prime(z2) \n\n# 2. Gradient for W2 (Outer Product)\n# We need shape (n_out, 1) @ (1, n_hidden) -&gt; (n_out, n_hidden)\ndC_dW2 = np.matmul(delta2, a1.T)\n\n# Alternative using einsum for outer product:\n# dC_dW2 = np.einsum('i,j-&gt;ij', delta2.flatten(), a1.flatten())\n\n# 3. Backpropagate Error to Hidden Layer\n# Matrix multiplication (W2.T @ delta2) followed by Hadamard product\ndelta1 = np.matmul(W2.T, delta2) * sigmoid_prime(z1)\n\n# 4. Gradient for W1 (Outer Product)\ndC_dW1 = np.matmul(delta1, x.T)",
    "crumbs": [
      "Backprop (Matrix Calculus)"
    ]
  },
  {
    "objectID": "7_backpropogation_matrix_calculus.html#using-gradient-descent-to-find-the-optimal-weights-to-reduce-the-loss-function",
    "href": "7_backpropogation_matrix_calculus.html#using-gradient-descent-to-find-the-optimal-weights-to-reduce-the-loss-function",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Using Gradient Descent to find the optimal weights to reduce the Loss function",
    "text": "Using Gradient Descent to find the optimal weights to reduce the Loss function\n \nWith equations (3) and (5) we can calculate the gradient of the Loss function with respect to weights in any layer - in this example\n\\[\\frac {\\partial C}{\\partial W^1},\\frac {\\partial C}{\\partial W^2}\\]\n \nWe now need to adjust the previous weight, by gradient descent.\n \nSo using the above gradients we get the new weights iteratively like below. If you notice this is exactly what is happening in gradient descent as well; only chain rule is used to calculate the gradients here. Backpropagation is the algorithm that helps calculate the gradients for each layer.\n \n\\[\\mathbf {\n  W^{l-1}_{new} = W^{l-1}_{old} - \\eta \\cdot \\frac{\\partial C}{\\partial W^{l-1}}\n}\\]\n \nWhere \\(\\eta\\) is the learning rate.\nReference\n\nhttps://cedar.buffalo.edu/~srihari/CSE574/Chap5/Chap5.3-BackProp.pdf\nhttp://neuralnetworksanddeeplearning.com/chap2.html\n\nNext: Back Propagation in Full - With Softmax & CrossEntropy Loss",
    "crumbs": [
      "Backprop (Matrix Calculus)"
    ]
  }
]