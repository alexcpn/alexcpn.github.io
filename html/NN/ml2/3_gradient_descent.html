<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.26">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>gradient_descent – The Mathematical Intuition Behind Deep Learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js" type="module"></script>
<script src="site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-617dda62c7243b6972367a47cbfd8072.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar docked nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">The Mathematical Intuition Behind Deep Learning</span>
    </a>
  </div>
        <div class="quarto-navbar-tools tools-end">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./3_gradient_descent.html">Gradient Descent</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Home</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./1_vectors_dot_product_and_perceptron.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Vectors &amp; Perceptron</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./2_perceptron_training.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Perceptron Training</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./3_gradient_descent.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Gradient Descent</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./4_backpropogation_chainrule.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Backprop (Chain Rule)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./6_neuralnetworkimpementation.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Neural Network Implementation</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./7_backpropogation_matrix_calculus.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Backprop (Matrix Calculus)</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./8_backpropogation_full.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Backprop With Softmax and Cross Entropy</span></a>
  </div>
</li>
    </ul>
    </div>
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-mathematical-intuition-behind-deep-learning" id="toc-the-mathematical-intuition-behind-deep-learning" class="nav-link active" data-scroll-target="#the-mathematical-intuition-behind-deep-learning">The Mathematical Intuition Behind Deep Learning</a></li>
  <li><a href="#chapter-3" id="toc-chapter-3" class="nav-link" data-scroll-target="#chapter-3">Chapter 3</a>
  <ul class="collapse">
  <li><a href="#neural-network-as-a-chain-of-functions" id="toc-neural-network-as-a-chain-of-functions" class="nav-link" data-scroll-target="#neural-network-as-a-chain-of-functions">Neural Network as a Chain of Functions</a>
  <ul class="collapse">
  <li><a href="#the-forward-pass" id="toc-the-forward-pass" class="nav-link" data-scroll-target="#the-forward-pass">The Forward Pass</a></li>
  <li><a href="#the-cost-function-loss-function" id="toc-the-cost-function-loss-function" class="nav-link" data-scroll-target="#the-cost-function-loss-function">The Cost Function (Loss Function)</a></li>
  <li><a href="#the-goal-of-training" id="toc-the-goal-of-training" class="nav-link" data-scroll-target="#the-goal-of-training">The Goal of Training</a></li>
  </ul></li>
  <li><a href="#optimization-gradient-descent-take-1" id="toc-optimization-gradient-descent-take-1" class="nav-link" data-scroll-target="#optimization-gradient-descent-take-1">Optimization: Gradient Descent — Take 1</a></li>
  <li><a href="#gradient-descent-for-scalar-functions" id="toc-gradient-descent-for-scalar-functions" class="nav-link" data-scroll-target="#gradient-descent-for-scalar-functions">Gradient Descent for Scalar Functions</a>
  <ul class="collapse">
  <li><a href="#running-the-numbers-a-real-example" id="toc-running-the-numbers-a-real-example" class="nav-link" data-scroll-target="#running-the-numbers-a-real-example">Running the Numbers: A Real Example</a></li>
  <li><a href="#the-adjustment-problem-which-direction-how-much" id="toc-the-adjustment-problem-which-direction-how-much" class="nav-link" data-scroll-target="#the-adjustment-problem-which-direction-how-much">The Adjustment Problem: Which Direction? How Much?</a></li>
  <li><a href="#the-chain-of-effects" id="toc-the-chain-of-effects" class="nav-link" data-scroll-target="#the-chain-of-effects">The Chain of Effects</a></li>
  <li><a href="#the-solution-applying-the-chain-rule" id="toc-the-solution-applying-the-chain-rule" class="nav-link" data-scroll-target="#the-solution-applying-the-chain-rule">The Solution: Applying the Chain Rule</a></li>
  <li><a href="#making-the-update-gradient-descent" id="toc-making-the-update-gradient-descent" class="nav-link" data-scroll-target="#making-the-update-gradient-descent">Making the Update: Gradient Descent</a></li>
  <li><a href="#verification-did-it-work" id="toc-verification-did-it-work" class="nav-link" data-scroll-target="#verification-did-it-work">Verification: Did It Work?</a></li>
  </ul></li>
  <li><a href="#gradient-descent-for-a-two-layer-neural-network-scalar-form" id="toc-gradient-descent-for-a-two-layer-neural-network-scalar-form" class="nav-link" data-scroll-target="#gradient-descent-for-a-two-layer-neural-network-scalar-form">Gradient Descent for a Two-Layer Neural Network (Scalar Form)</a></li>
  <li><a href="#some-other-notes-related-to-gradient-descent" id="toc-some-other-notes-related-to-gradient-descent" class="nav-link" data-scroll-target="#some-other-notes-related-to-gradient-descent">Some other notes related to Gradient Descent</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content" id="quarto-document-content"><header id="title-block-header" class="quarto-title-block"></header>




<section id="the-mathematical-intuition-behind-deep-learning" class="level1">
<h1>The Mathematical Intuition Behind Deep Learning</h1>
<p>Alex Punnen<br>
© All Rights Reserved</p>
<hr>
<p><a href="./index.html">Contents</a></p>
</section>
<section id="chapter-3" class="level1">
<h1>Chapter 3</h1>
<section id="neural-network-as-a-chain-of-functions" class="level2">
<h2 class="anchored" data-anchor-id="neural-network-as-a-chain-of-functions">Neural Network as a Chain of Functions</h2>
<p>To understand deep learning, we need to understand the concept of a neural network as a chain of functions.</p>
<p>A Neural Network is essentially a chain of functions. It consists of a set of inputs connected through ‘weights’ to a set of activation functions, whose output becomes the input for the next layer, and so on.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://i.imgur.com/gE3QKCf.png" class="img-fluid figure-img"></p>
<figcaption>neuralnetwork</figcaption>
</figure>
</div>
<section id="the-forward-pass" class="level3">
<h3 class="anchored" data-anchor-id="the-forward-pass">The Forward Pass</h3>
<p>Let’s consider a simple two-layer neural network.</p>
<ul>
<li><span class="math inline">\(x\)</span>: Input vector</li>
<li><span class="math inline">\(y\)</span>: Output vector (prediction)</li>
<li><span class="math inline">\(L\)</span>: Number of layers</li>
<li><span class="math inline">\(w^l, b^l\)</span>: Weights and biases for layer <span class="math inline">\(l\)</span></li>
<li><span class="math inline">\(a^l\)</span>: Activation of layer <span class="math inline">\(l\)</span> (we use sigmoid <span class="math inline">\(\sigma\)</span> here)</li>
</ul>
<p>The flow of data (Forward Pass) can be represented as:</p>
<p><span class="math display">\[
x \rightarrow a^{1} \rightarrow \dots \rightarrow a^{L} \rightarrow y
\]</span></p>
<p>For any layer <span class="math inline">\(l\)</span>, the activation <span class="math inline">\(a^l\)</span> is calculated as:</p>
<p><span class="math display">\[
  a^{l} = \sigma(w^l a^{l-1} + b^l)
\]</span></p>
<p>where <span class="math inline">\(a^0 = x\)</span> (the input).</p>
<p>The linear transformation:</p>
<p><span class="math inline">\(z = w^T x + b\)</span></p>
<p>defines a hyperplane (decision boundary) in the feature space.</p>
<p>The activation function then introduces <em>non-linearity</em>, allowing the network to combine <strong>multiple such hyperplanes into complex decision boundaries.</strong></p>
<section id="why-non-linearity-is-non-negotiable" class="level4">
<h4 class="anchored" data-anchor-id="why-non-linearity-is-non-negotiable">Why Non-Linearity Is Non-Negotiable</h4>
<p>Without activation:</p>
<p><span class="math display">\[
f(x) = W_L W_{L-1} \dots W_1 x
\]</span></p>
<p>This collapses to:</p>
<p><span class="math display">\[
f(x) = Wx
\]</span></p>
<p>Still one big linear transformation and hence one hyperplane; the problems of not able to separate features will come. Only because of non-linearity, we can get multiple hyperplanes and hence a composable complex decision boundaries that can separate features.</p>
<p>So the concept of Vectors, Matrices and Hyperplanes remain the same as before. Let us explore the chain of functions part here</p>
<p>A neural network with <span class="math inline">\(L\)</span> layers can be represented as a nested function:<span class="math display">\[f(x) = f_L(...f_2(f_1(x))...)\]</span></p>
<p>Each “link” in the chain is a layer performing a linear transformation followed by a non-linear activation and cascading to the final output.</p>
</section>
</section>
<section id="the-cost-function-loss-function" class="level3">
<h3 class="anchored" data-anchor-id="the-cost-function-loss-function">The Cost Function (Loss Function)</h3>
<p>To train this network, we need to measure how “wrong” its predictions are compared to the true values. We do this using a <strong>Cost Function</strong> (or Loss Function).</p>
<p>A simplest Loss function is just the difference between the predicted output and the true output ($ y(x) - a^L(x) $.)</p>
<p>But usually we use the square of the difference to make it a non-negative function.</p>
<p>A common choice is the <strong>Mean Squared Error (MSE)</strong>:</p>
<p><span class="math display">\[
C = \frac{1}{2n} \sum_{x} \|y(x) - a^L(x)\|^2
\]</span></p>
<ul>
<li><span class="math inline">\(n\)</span>: Number of training examples</li>
<li><span class="math inline">\(y(x)\)</span>: The true expected output (label) for input <span class="math inline">\(x\)</span></li>
<li><span class="math inline">\(a^L(x)\)</span>: The network’s predicted output for input <span class="math inline">\(x\)</span></li>
</ul>
</section>
<section id="the-goal-of-training" class="level3">
<h3 class="anchored" data-anchor-id="the-goal-of-training">The Goal of Training</h3>
<p>The goal of training is to find the set of weights <span class="math inline">\(w\)</span> and biases <span class="math inline">\(b\)</span> that minimize this cost <span class="math inline">\(C\)</span>.</p>
<p>This means that we need to optimise each component of the function <span class="math inline">\(f(x)\)</span> to reduce the cost proportional to its contribution to the final output. The method to do this is called <strong>Backpropagation</strong>. It helps us calculate the <strong>gradient</strong> of the cost function with respect to each weight and bias.</p>
<p>Once the gradient is calculated, we can use <strong>Gradient Descent</strong> to update the weights in the opposite direction of the gradient.</p>
<p>Gradient descent is a simple optimization algorithm that works by iteratively updating the weights in the opposite direction of the gradient.</p>
<p>However neural network is a composition of vector spaces and linear transformations. Hence gradient descent acts on a very complex space.</p>
<p>There are two or three facts to understand about gradient descent:</p>
<ol type="1">
<li><p>It does not attempt to find the <strong>global minimum</strong>, but rather follows the <strong>local slope</strong> of the cost function and converges to a local minimum or a flat region. <strong>Saddle point</strong> is a good optimisation point.</p></li>
<li><p>Gradients can <strong>vanish or explode</strong>, leading to slow or unstable convergence. The practical solution to control this is to use <strong>learning rate</strong> and using <strong>adaptive learning rate</strong> methods like <strong>Adam</strong> or <strong>RMSprop</strong>.</p></li>
<li><p><strong>Batch Size matters</strong>: Calculating the gradient over the entire dataset (Batch Gradient Descent) is computationally expensive and memory-intensive. In practice, we use <strong>Stochastic Gradient Descent (SGD)</strong> (one example at a time) or, more commonly, <strong>Mini-batch Gradient Descent</strong> (a small batch of examples). This introduces noise into the gradient estimate, which paradoxically helps the optimization process escape shallow local minima and saddle points.</p></li>
</ol>
</section>
</section>
<section id="optimization-gradient-descent-take-1" class="level2">
<h2 class="anchored" data-anchor-id="optimization-gradient-descent-take-1">Optimization: Gradient Descent — Take 1</h2>
<p>Gradient Descent is a simple yet powerful optimization algorithm used to minimize functions by iteratively updating parameters in the direction that reduces the function’s output.</p>
<p>For basic scalar functions (e.g., <span class="math inline">\(( f(x) = x^2 )\)</span>), the update rule is straightforward: <span class="math display">\[
x \leftarrow x - \eta \frac{df}{dx}
\]</span> where <span class="math inline">\(( \eta )\)</span> is the learning rate.</p>
<p>However, <strong>neural networks are not simple scalar functions</strong>. They are <strong>composite vector-valued functions</strong> — layers of transformations that take in high-dimensional input vectors and eventually output either vectors (like logits) or scalars (like loss values).</p>
<p>Understanding how to optimize these complex, high-dimensional functions requires us to extend basic calculus: - The <strong>gradient vector</strong> helps when the function outputs a scalar but takes a vector input (e.g., a loss function w.r.t. weights). - The <strong>Jacobian matrix</strong> becomes important when both the input and the output are vectors (e.g., when computing gradients layer by layer in backpropagation).</p>
<p>We’ll build up to this step by step — starting with scalar gradients, then moving to vector calculus, Jacobians, and how backpropagation stitches it all together.</p>
<p>Let’s take it one layer at a time.</p>
</section>
<section id="gradient-descent-for-scalar-functions" class="level2">
<h2 class="anchored" data-anchor-id="gradient-descent-for-scalar-functions">Gradient Descent for Scalar Functions</h2>
<p>Consider this simple system that composes two functions:</p>
<p><span class="math display">\[L = g(f(x, w_1), w_2)\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(x\)</span> is your input (fixed, given by your data)</p></li>
<li><p><span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span> are <strong>parameters you can adjust</strong> (like weights in a neural network)</p></li>
<li><p><span class="math inline">\(f\)</span> is the first function (think: first layer)</p></li>
<li><p><span class="math inline">\(g\)</span> is the second function (think: second layer)</p></li>
<li><p><span class="math inline">\(L\)</span> is the final output</p></li>
</ul>
<p>Let’s make this concrete with simple linear functions:</p>
<p><span class="math display">\[f(x, w_1) = x \cdot w_1 + b_1\]</span></p>
<p><span class="math display">\[g(z, w_2) = z \cdot w_2 + b_2\]</span></p>
<p>So the full composition is:</p>
<p><span class="math display">\[L = g(f(x, w_1), w_2) = (x \cdot w_1 + b_1) \cdot w_2 + b_2\]</span></p>
<section id="running-the-numbers-a-real-example" class="level3">
<h3 class="anchored" data-anchor-id="running-the-numbers-a-real-example">Running the Numbers: A Real Example</h3>
<p>Let’s pick actual values and see what happens:</p>
<p><strong>Fixed values:</strong></p>
<ul>
<li><p>Input: <span class="math inline">\(x = 2.0\)</span></p></li>
<li><p>Bias terms: <span class="math inline">\(b_1 = 1.0\)</span>, <span class="math inline">\(b_2 = 0.5\)</span></p></li>
</ul>
<p><strong>Current parameter values:</strong></p>
<ul>
<li><p><span class="math inline">\(w_1 = 0.5\)</span></p></li>
<li><p><span class="math inline">\(w_2 = 1.5\)</span></p></li>
</ul>
<p><strong>Step 1</strong>: Compute intermediate result from first function:</p>
<p><span class="math display">\[z = f(x, w_1) = 2.0  \times  0.5 + 1.0 = 2.0\]</span></p>
<p><strong>Step 2</strong>: Compute final output from second function:</p>
<p><span class="math display">\[L = g(z, w_2) = 2.0  \times  1.5 + 0.5 = 3.5\]</span></p>
<p><strong>The problem</strong>: Suppose we want <span class="math inline">\(L_{\text{target}} = 5.0\)</span> instead!</p>
<p>Our current error is:</p>
<p><span class="math display">\[E = \frac{1}{2}(L - L_{\text{target}})^2 = \frac{1}{2}(3.5 - 5.0)^2 = \frac{1}{2}(-1.5)^2 = 1.125\]</span></p>
<p><strong>The million-dollar question</strong>: How should we change <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span> to reduce this error?</p>
</section>
<section id="the-adjustment-problem-which-direction-how-much" class="level3">
<h3 class="anchored" data-anchor-id="the-adjustment-problem-which-direction-how-much">The Adjustment Problem: Which Direction? How Much?</h3>
<p>Here’s what we need to know:</p>
<ol type="1">
<li><p><strong>Should we increase or decrease <span class="math inline">\(w_1\)</span>?</strong> (Which direction?)</p></li>
<li><p><strong>How sensitive is <span class="math inline">\(L\)</span> to changes in <span class="math inline">\(w_1\)</span>?</strong> (How much?)</p></li>
<li><p><strong>Same questions for <span class="math inline">\(w_2\)</span>.</strong></p></li>
</ol>
<p>This is where derivatives come in! Specifically, we need:</p>
<p><span class="math display">\[\frac{\partial L}{\partial w_1} \quad  \text{and} \quad  \frac{\partial L}{\partial w_2}\]</span></p>
<p>These tell us:</p>
<ul>
<li><p><strong>Sign</strong>: Positive means “increase <span class="math inline">\(w\)</span> increases <span class="math inline">\(L\)</span>”, negative means the opposite</p></li>
<li><p><strong>Magnitude</strong>: Larger absolute value means <span class="math inline">\(L\)</span> is more sensitive to changes in <span class="math inline">\(w\)</span></p></li>
</ul>
<p>But there’s a complication: <span class="math inline">\(w_1\)</span> doesn’t directly affect <span class="math inline">\(L\)</span>. It affects <span class="math inline">\(f\)</span>, which then affects <span class="math inline">\(g\)</span>, which then affects <span class="math inline">\(L\)</span>. This is a <strong>composition</strong>, and we need to trace the effect through multiple steps.</p>
<p>This is where the “Chain Rule” of Calculus comes into play.</p>
</section>
<section id="the-chain-of-effects" class="level3">
<h3 class="anchored" data-anchor-id="the-chain-of-effects">The Chain of Effects</h3>
<p>Let’s visualize how changes propagate:</p>
<pre><code>
Change w₁ → Affects f → Changes z → Affects g → Changes L

↓ ↓ ↓ ↓ ↓

Δw₁ ∂f/∂w₁ Δz ∂g/∂z ΔL
</code></pre>
<p>Similarly for <span class="math inline">\(w_2\)</span> (but <span class="math inline">\(w_2\)</span> directly affects <span class="math inline">\(g\)</span>):</p>
<pre><code>
Change w₂ → Affects g → Changes L

↓ ↓ ↓

Δw₂ ∂g/∂w₂ ΔL
</code></pre>
<p>The key insight: <strong>To find how <span class="math inline">\(w_1\)</span> affects <span class="math inline">\(L\)</span>, we need to multiply the effects at each step.</strong></p>
<p>This is the <strong>chain rule</strong> in action!</p>
</section>
<section id="the-solution-applying-the-chain-rule" class="level3">
<h3 class="anchored" data-anchor-id="the-solution-applying-the-chain-rule">The Solution: Applying the Chain Rule</h3>
<p>For our composition <span class="math inline">\(L = g(f(x, w_1), w_2)\)</span>, let’s introduce a shorthand: call <span class="math inline">\(z = f(x, w_1)\)</span> the intermediate value.</p>
<p>Then:</p>
<p><span class="math display">\[L = g(z, w_2)\]</span></p>
<p><strong>Computing <span class="math inline">\(\frac{\partial L}{\partial w_1}\)</span>:</strong></p>
<p>By the chain rule of calculus:</p>
<p><span class="math display">\[\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial z} \cdot  \frac{\partial z}{\partial w_1}\]</span></p>
<p>Let’s compute each piece:</p>
<p><strong>Part 1</strong>: How does <span class="math inline">\(L\)</span> change with <span class="math inline">\(z\)</span>?</p>
<p><span class="math display">\[\frac{\partial L}{\partial z} = \frac{\partial}{\partial z}(z \cdot w_2 + b_2) = w_2 = 1.5\]</span></p>
<p><strong>Part 2</strong>: How does <span class="math inline">\(z\)</span> change with <span class="math inline">\(w_1\)</span>?</p>
<p><span class="math display">\[\frac{\partial z}{\partial w_1} = \frac{\partial}{\partial w_1}(x \cdot w_1 + b_1) = x = 2.0\]</span></p>
<p><strong>Putting it together</strong>:</p>
<p><span class="math display">\[\frac{\partial L}{\partial w_1} = 1.5  \times  2.0 = 3.0\]</span></p>
<p><strong>Interpretation</strong>: If we increase <span class="math inline">\(w_1\)</span> by 0.1, then <span class="math inline">\(L\)</span> increases by approximately <span class="math inline">\(3.0  \times  0.1 = 0.3\)</span>.</p>
<p><strong>Computing <span class="math inline">\(\frac{\partial L}{\partial w_2}\)</span>:</strong></p>
<p>This is simpler because <span class="math inline">\(w_2\)</span> directly affects <span class="math inline">\(g\)</span>:</p>
<p><span class="math display">\[\frac{\partial L}{\partial w_2} = \frac{\partial}{\partial w_2}(z \cdot w_2 + b_2) = z = 2.0\]</span></p>
<p><strong>Interpretation</strong>: If we increase <span class="math inline">\(w_2\)</span> by 0.1, then <span class="math inline">\(L\)</span> increases by approximately <span class="math inline">\(2.0  \times  0.1 = 0.2\)</span>.</p>
</section>
<section id="making-the-update-gradient-descent" class="level3">
<h3 class="anchored" data-anchor-id="making-the-update-gradient-descent">Making the Update: Gradient Descent</h3>
<p>Now we can adjust our parameters! Since we want to <strong>increase</strong> <span class="math inline">\(L\)</span> from 3.5 to 5.0, and both gradients are positive, we should increase both <span class="math inline">\(w_1\)</span> and <span class="math inline">\(w_2\)</span>.</p>
<p>Using gradient descent with learning rate <span class="math inline">\(\alpha = 0.2\)</span>:</p>
<p><span class="math display">\[w_1^{\text{new}} = w_1 + \alpha  \cdot  \frac{\partial L}{\partial w_1} = 0.5 + 0.2  \times  3.0 = 0.5 + 0.6 = 1.1\]</span></p>
<p><span class="math display">\[w_2^{\text{new}} = w_2 + \alpha  \cdot  \frac{\partial L}{\partial w_2} = 1.5 + 0.2  \times  2.0 = 1.5 + 0.4 = 1.9\]</span></p>
<p><strong>Note</strong>: We’re adding (not subtracting) because we want to increase <span class="math inline">\(L\)</span>. Normally in machine learning, we minimize error, so we’d use <span class="math inline">\(w - \alpha  \cdot  \frac{\partial E}{\partial w}\)</span>.</p>
</section>
<section id="verification-did-it-work" class="level3">
<h3 class="anchored" data-anchor-id="verification-did-it-work">Verification: Did It Work?</h3>
<p>Let’s recompute with the new weights:</p>
<p><strong>Step 1</strong>: New intermediate value:</p>
<p><span class="math display">\[z^{\text{new}} = x \cdot w_1^{\text{new}} + b_1 = 2.0  \times  1.1 + 1.0 = 3.2\]</span></p>
<p><strong>Step 2</strong>: New output:</p>
<p><span class="math display">\[L^{\text{new}} = z^{\text{new}} \cdot w_2^{\text{new}} + b_2 = 3.2  \times  1.9 + 0.5 = 6.58\]</span></p>
<p><strong>Progress check</strong>:</p>
<ul>
<li><p>Before: <span class="math inline">\(L = 3.5\)</span> (error from target = 1.5)</p></li>
<li><p>After: <span class="math inline">\(L = 6.58\)</span> (error from target = -1.58)</p></li>
<li><p>We overshot! But that’s okay - we moved in the right direction</p></li>
</ul>
<p>With a smaller learning rate (say <span class="math inline">\(\alpha = 0.1\)</span>), we’d get:</p>
<ul>
<li><p><span class="math inline">\(w_1^{\text{new}} = 0.8\)</span>, <span class="math inline">\(w_2^{\text{new}} = 1.7\)</span></p></li>
<li><p><span class="math inline">\(z^{\text{new}} = 2.6\)</span>, <span class="math inline">\(L^{\text{new}} = 4.92\)</span></p></li>
<li><p>Much closer to our target of 5.0!</p></li>
</ul>
<p>This is how Gradient Descent works in a nutshell. The same concepts carry over in deep learning with some added complexity.</p>
</section>
</section>
<section id="gradient-descent-for-a-two-layer-neural-network-scalar-form" class="level2">
<h2 class="anchored" data-anchor-id="gradient-descent-for-a-two-layer-neural-network-scalar-form">Gradient Descent for a Two-Layer Neural Network (Scalar Form)</h2>
<p>Let’s apply this to a simple neural network with one hidden layer. We have: * <strong>Input</strong>: <span class="math inline">\(x\)</span> * <strong>Hidden Layer</strong>: 1 neuron with weight <span class="math inline">\(w_1\)</span>, bias <span class="math inline">\(b_1\)</span>, activation <span class="math inline">\(\sigma\)</span> * <strong>Output Layer</strong>: 1 neuron with weight <span class="math inline">\(w_2\)</span>, bias <span class="math inline">\(b_2\)</span>, activation <span class="math inline">\(\sigma\)</span> * <strong>Target</strong>: <span class="math inline">\(y\)</span></p>
<p><strong>Forward Pass:</strong> 1. <span class="math inline">\(z_1 = w_1 x + b_1\)</span> 2. <span class="math inline">\(a_1 = \sigma(z_1)\)</span> 3. <span class="math inline">\(z_2 = w_2 a_1 + b_2\)</span> 4. <span class="math inline">\(a_2 = \sigma(z_2)\)</span> (This is our prediction <span class="math inline">\(\hat{y}\)</span>)</p>
<p><strong>Loss Function:</strong> We use the Mean Squared Error (MSE) for this single example: <span class="math display">\[ C = \frac{1}{2}(y - a_2)^2 \]</span></p>
<p><strong>Goal:</strong> Find <span class="math inline">\(\frac{\partial C}{\partial w_1}, \frac{\partial C}{\partial b_1}, \frac{\partial C}{\partial w_2}, \frac{\partial C}{\partial b_2}\)</span> to update the weights.</p>
<p><strong>Backward Pass (Deriving Gradients):</strong></p>
<p><strong>Layer 2 (Output Layer):</strong> We want how <span class="math inline">\(C\)</span> changes with <span class="math inline">\(w_2\)</span>. <span class="math display">\[ \frac{\partial C}{\partial w_2} = \frac{\partial C}{\partial a_2} \cdot \frac{\partial a_2}{\partial z_2} \cdot \frac{\partial z_2}{\partial w_2} \]</span></p>
<ul>
<li><span class="math inline">\(\frac{\partial C}{\partial a_2} = -(y - a_2)\)</span> (Derivative of <span class="math inline">\(\frac{1}{2}(y-a)^2\)</span>)</li>
<li><span class="math inline">\(\frac{\partial a_2}{\partial z_2} = \sigma'(z_2)\)</span> (Derivative of activation)</li>
<li><span class="math inline">\(\frac{\partial z_2}{\partial w_2} = a_1\)</span></li>
</ul>
<p>So, <span class="math display">\[ \frac{\partial C}{\partial w_2} = -(y - a_2) \sigma'(z_2) a_1 \]</span></p>
<p>Let’s define the “error term” for layer 2 as <span class="math inline">\(\delta_2 = -(y - a_2) \sigma'(z_2)\)</span>. Then: <span class="math display">\[ \frac{\partial C}{\partial w_2} = \delta_2 a_1 \]</span> <span class="math display">\[ \frac{\partial C}{\partial b_2} = \delta_2 \cdot 1 = \delta_2 \]</span></p>
<p><strong>Layer 1 (Hidden Layer):</strong> We want how <span class="math inline">\(C\)</span> changes with <span class="math inline">\(w_1\)</span>. The path is longer: <span class="math inline">\(w_1 \to z_1 \to a_1 \to z_2 \to a_2 \to C\)</span>. <span class="math display">\[ \frac{\partial C}{\partial w_1} = \underbrace{\frac{\partial C}{\partial a_2} \cdot \frac{\partial a_2}{\partial z_2}}_{\delta_2} \cdot \frac{\partial z_2}{\partial a_1} \cdot \frac{\partial a_1}{\partial z_1} \cdot \frac{\partial z_1}{\partial w_1} \]</span></p>
<ul>
<li>We know the first part is <span class="math inline">\(\delta_2\)</span>.</li>
<li><span class="math inline">\(\frac{\partial z_2}{\partial a_1} = w_2\)</span></li>
<li><span class="math inline">\(\frac{\partial a_1}{\partial z_1} = \sigma'(z_1)\)</span></li>
<li><span class="math inline">\(\frac{\partial z_1}{\partial w_1} = x\)</span></li>
</ul>
<p>So, <span class="math display">\[ \frac{\partial C}{\partial w_1} = \delta_2 \cdot w_2 \cdot \sigma'(z_1) \cdot x \]</span></p>
<p>Let’s define the error term for layer 1 as <span class="math inline">\(\delta_1 = \delta_2 w_2 \sigma'(z_1)\)</span>. Then: <span class="math display">\[ \frac{\partial C}{\partial w_1} = \delta_1 x \]</span> <span class="math display">\[ \frac{\partial C}{\partial b_1} = \delta_1 \]</span></p>
<p><strong>The Update:</strong> <span class="math display">\[ w_1 \leftarrow w_1 - \eta \delta_1 x \]</span> <span class="math display">\[ w_2 \leftarrow w_2 - \eta \delta_2 a_1 \]</span></p>
<p>This pattern—calculating an error term <span class="math inline">\(\delta\)</span> at the output and propagating it back using the weights—is why it’s called <strong>Backpropagation</strong>.</p>
<p>Note that we are using here scalar form of gradient descent and not directly applicable to real neural networks. But this gives us the intuition of how backpropagation works.</p>
</section>
<section id="some-other-notes-related-to-gradient-descent" class="level2">
<h2 class="anchored" data-anchor-id="some-other-notes-related-to-gradient-descent">Some other notes related to Gradient Descent</h2>
<p>The Loss/Cost function is a scalar function of the weights and biases.</p>
<p>The loss/error is a scalar function of all weights and biases.</p>
<p>In simpler Machine Learning problems like linear regression with MSE, the loss is a convex quadratic in the parameters, so optimization is well-behaved (a bowl-shaped surface)(e.g.&nbsp;see left in picture).</p>
<p>In deep learning, the loss becomes non-convex because it is the result of composing many nonlinear transformations. This creates a complex landscape with saddle points, flat regions, and multiple minima (e.g.&nbsp;see right in picture).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/costfunction.png" class="img-fluid figure-img"></p>
<figcaption>costfunction</figcaption>
</figure>
</div>
<p><strong>How will Gradient Descent work in this case - non convex function?</strong></p>
<p>Gradient descent does not attempt to find the <strong>global minimum</strong>, but rather follows the local slope of the cost function and converges to a local minimum or a flat region.</p>
<p>The Loss function is differentiable almost everywhere*. At any point in parameter space, the gradient indicates the direction of steepest local increase, and moving in the opposite direction reduces the cost. During optimization, the algorithm may encounter local minima or saddle points.</p>
<p>(*The function is not differentiable at the point where the function is zero ex ReLU. This is not a problem in practice, as optimization algorithms handle such points using <a href="images/subgradient.png">subgradients</a>)</p>
<p>In practice, deep learning works well despite non-convexity, partly because modern networks have millions of parameters and their loss landscapes contain many saddle points and wide, flat minima rather than <a href="images/poorlocalminima.png">poor isolated local minima</a>.</p>
<p>Also we rarely use full-batch gradient descent. Instead, we use variants such as Stochastic Gradient Descent (SGD) or mini-batch gradient descent that acts as form of sampling.</p>
<p>In these methods, gradients are computed using a single training example or a small batch of examples rather than the entire dataset.</p>
<p>The resulting gradient is an average over the batch and serves as a noisy approximation of the true gradient. This stochasticity helps the optimizer escape saddle points and sharp minima, enabling effective training in practice.</p>
<p>Next: <a href="./4_backpropogation_chainrule.html">Backpropagation</a></p>



</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>