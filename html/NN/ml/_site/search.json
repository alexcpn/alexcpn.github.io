[
  {
    "objectID": "8_backpropogation_full.html",
    "href": "8_backpropogation_full.html",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "",
    "text": "Alex Punnen\n© All Rights Reserved\n\nContents",
    "crumbs": [
      "Backprop With Softmax and Cross Entropy"
    ]
  },
  {
    "objectID": "8_backpropogation_full.html#back-propagation-in-full---with-softmax-crossentropy-loss",
    "href": "8_backpropogation_full.html#back-propagation-in-full---with-softmax-crossentropy-loss",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Back Propagation in Full - With Softmax & CrossEntropy Loss",
    "text": "Back Propagation in Full - With Softmax & CrossEntropy Loss\nLet’s think of a \\(l\\) layered neural network whose input is \\(x=a^0\\) and output is \\(a^l\\).In this network we will be using the sigmoid (\\(\\sigma\\) ) function as the activation function for all layers except the last layer \\(l\\). For the last layer we use the Softmax activation function. We will use the Cross Entropy Loss as the loss function.\nThis is how a proper Neural Network should be.",
    "crumbs": [
      "Backprop With Softmax and Cross Entropy"
    ]
  },
  {
    "objectID": "8_backpropogation_full.html#the-neural-network-model",
    "href": "8_backpropogation_full.html#the-neural-network-model",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "The Neural Network Model",
    "text": "The Neural Network Model\nI am writing this out, without index notation, and with the super script representing just the layers of the network.\n\\[\n\\mathbf {\n\\begin{aligned}\na^0 \\rightarrow\n      \\underbrace{\\text{hidden layers}}_{a^{l-2}}\n      \\,\\rightarrow\n      \\underbrace{w^{l-1} a^{l-2}+b^{l-1}}_{z^{l-1} }\n      \\,\\rightarrow\n      \\underbrace{\\sigma(z^{l-1})}_{a^{l-1}}\n    \\,\\rightarrow\n     \\underbrace{w^l a^{l-1}+b^l}_{z^{l}/logits }\n    \\,\\rightarrow\n    \\underbrace{P(z^l)}_{\\vec P/ \\text{softmax} /a^{l}}\n    \\,\\rightarrow\n    \\underbrace{L ( \\vec P, \\vec Y)}_{\\text{CrossEntropyLoss}}\n\\end{aligned}\n}\n\\]\n\\(Y\\) is the target vector or the Truth vector. This is a one hot encoded vector, example \\(Y=[0,1,0]\\), here the second element is the desired class.The training is done so that the CrossEntropyLoss is minimized using Gradient Loss algorithm.\n\\(P\\) is the Softmax output and is the activation of the last layer \\(a^l\\). This is a vector. All elements of the Softmax output add to 1; hence this is a probability distribution unlike a Sigmoid output.The Cross Entropy Loss \\(L\\) is a Scalar.\nNote the Index notation is the representation an element of a Vector or a Tensor, and is easier to deal with while deriving out the equations.\nSoftmax (in Index notation)\nBelow I am skipping the superscript part, which I used to represent the layers of the network.\n\\[\n\\begin{aligned}\np_j = \\frac{e^{z_j}}{\\sum_k e^{z_k}}\n\\end{aligned}\n\\]\nThis represent one element of the softmax vector, example \\(\\vec P= [p_1,p_2,p_3]\\)\nCross Entropy Loss (in Index notation)\nHere \\(y_i\\) is the indexed notation of an element in the target vector \\(Y\\).\n\\[\n\\begin{aligned}\nL = -\\sum_j y_j \\log p_j\n\\end{aligned}\n\\]\n\nThere are too many articles related to Back propagation, many of which are very good.However many explain in terms of index notation and though it is illuminating, to really use this with code, you need to understand how it translates to Matrix notation via Matrix Calculus and with help form StackOverflow related sites.\n\nCrossEntropy Loss with respect to Weight in last layer\n\\[\n\\mathbf {\n\\frac {\\partial L}{\\partial w^l}\n=  \\color{red}{\\frac {\\partial L}{\\partial z^l}}.\\color{green}{\\frac {\\partial z^l}{\\partial w^l}} \\rightarrow \\quad EqA1\n}\n\\]\nWhere \\[\nL = -\\sum_k y_k \\log {\\color{red}{p_k}} \\quad \\text{and} \\quad p_j = \\frac {e^{\\color{red}{z_j}}} {\\sum_k e^{z_k}}\n\\]\nIf you are confused with the indexes, just take a short example and substitute. Basically i,j,k etc are dummy indices used to illustrate in index notation the vectors.\nI am going to drop the superscript \\(l\\) denoting the layer number henceforth and focus on the index notation for the softmax vector \\(P\\) and target vector \\(Y\\)\nFrom Derivative of Softmax Activation -Alijah Ahmed\n$$ {\n\\[\\begin{aligned}\n\n    \\frac {\\partial L}{\\partial z_i} = \\frac {\\partial ({-\\sum_j y_k \\log {p_k})}}{\\partial z_i}\n   \\\\ \\\\ \\text {taking the summation outside} \\\\ \\\\\n   = -\\sum_j y_k\\frac {\\partial ({ \\log {p_k})}}{\\partial z_i}\n  \\\\ \\\\ \\color{grey}{\\text {since }\n  \\frac{d}{dx} (f(g(x))) = f'(g(x))g'(x) }\n  \\\\ \\\\\n  = -\\sum_k y_k * \\frac {1}{p_k} *\\frac {\\partial { p_k}}{\\partial z_i}\n  \n\\end{aligned}\\]\n} $$\nThe last term \\(\\frac {\\partial { p_k}}{\\partial z_i}\\) is the derivative of Softmax with respect to it’s inputs also called logits. This is easy to derive and there are many sites that describe it. Example [Derivative of SoftMax Antoni Parellada]. The more rigorous derivative via the Jacobian matrix is here The Softmax function and its derivative-Eli Bendersky\n\\[\n\\color{red}\n  {\n  \\begin{aligned}\n   \\frac {\\partial { p_i}}{\\partial z_i} = p_i(\\delta_{ij} -p_j)\n   \\\\ \\\\\n   \\delta_{ij} = 1 \\text{ when i =j}\n   \\\\\n   \\delta_{ij} = 0 \\text{ when i} \\ne \\text{j}\n  \\end{aligned}\n  }\n\\]\nUsing this above and from Derivative of Softmax Activation -Alijah Ahmed\n$$ {\n\\[\\begin{aligned}\n\n\\frac {\\partial L}{\\partial z_i} = -\\sum_k y_k * \\frac {1}{p_k} *\\frac {\\partial { p_k}}{\\partial z_i}\n\\\\ \\\\\n  =-\\sum_k y_k * \\frac {1}{p_k} * p_i(\\delta_{ij} -p_j)\n\\\\ \\\\ \\text{these i and j are dummy indices and we can rewrite  this as}\n\\\\ \\\\\n=-\\sum_k y_k * \\frac {1}{p_k} * p_k(\\delta_{ik} -p_i)\n\\\\ \\\\ \\text{taking the two cases and adding in above equation } \\\\ \\\\\n\\delta_{ij} = 1 \\text{ when i =k} \\text{ and }\n   \\delta_{ij} = 0 \\text{ when i} \\ne \\text{k}\n   \\\\ \\\\\n   = [- \\sum_i y_i * \\frac {1}{p_i} * p_i(1 -p_i)]+[-\\sum_{k \\ne i}  y_k * \\frac {1}{p_k} * p_k(0 -p_i) ]\n    \\\\ \\\\\n     = [- y_i * \\frac {1}{p_i} * p_i(1 -p_i)]+[-\\sum_{k \\ne i}  y_k * \\frac {1}{p_k} * p_k(0 -p_i) ]\n  \\\\ \\\\\n     = [- y_i(1 -p_i)]+[-\\sum_{k \\ne i}  y_k *(0 -p_i) ]\n      \\\\ \\\\\n     = -y_i + y_i.p_i + \\sum_{k \\ne i}  y_k.p_i\n     \\\\ \\\\\n     = -y_i + p_i( y_i + \\sum_{k \\ne i}  y_k)\n     \\\\ \\\\\n     = -y_i + p_i( \\sum_{k}  y_k)\n     \\\\ \\\\\n     \\text {note that } \\sum_{k}  y_k = 1  \\, \\text{as it is a One hot encoded Vector}\n     \\\\ \\\\\n     = p_i - y_i\n     \\\\ \\\\\n     \\frac {\\partial L}{\\partial z^l}  = p_i - y_i \\rightarrow \\quad \\text{EqA1.1}\n\\end{aligned}\\]\n} $$\nWe need to put this back in \\(EqA1\\). We now need to calculate the second term, to complete the equation\n\\[\n\\begin{aligned}\n\\frac {\\partial L}{\\partial w^l}\n=  \\color{red}{\\frac {\\partial L}{\\partial z^l}}.\\color{green}{\\frac {\\partial z^l}{\\partial w^l}}\n\\\\ \\\\\nz^{l} = (w^l a^{l-1}+b^l)\n\\\\\n\\color{green}{\\frac {\\partial z^l}{\\partial w^l} = a^{l-1}}\n\\\\ \\\\ \\text{Putting all together} \\\\ \\\\\n\\frac {\\partial L}{\\partial w^l} = (p_i - y_i) *a^{l-1} \\quad  \\rightarrow \\quad \\mathbf  {EqA1}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Backprop With Softmax and Cross Entropy"
    ]
  },
  {
    "objectID": "8_backpropogation_full.html#gradient-descent",
    "href": "8_backpropogation_full.html#gradient-descent",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Gradient descent",
    "text": "Gradient descent\nUsing Gradient descent we can keep adjusting the last layer like\n\\[\n     w{^l}{_i} = w{^l}{_i} -\\alpha *  \\frac {\\partial L}{\\partial w^l}\n\\]\nNow let’s do the derivation for the inner layers",
    "crumbs": [
      "Backprop With Softmax and Cross Entropy"
    ]
  },
  {
    "objectID": "8_backpropogation_full.html#derivative-of-loss-with-respect-to-weight-in-inner-layers",
    "href": "8_backpropogation_full.html#derivative-of-loss-with-respect-to-weight-in-inner-layers",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Derivative of Loss with respect to Weight in Inner Layers",
    "text": "Derivative of Loss with respect to Weight in Inner Layers\nThe trick here is to find the derivative of the Loss with respect to the inner layer as a composition of the partial derivative we computed earlier. And also to compose each partial derivative as partial derivative with respect to either \\(z^x\\) or \\(w^x\\) but not with respect to \\(a^x\\). This is to make derivatives easier and intuitive to compute.\n\\[\n\\begin{aligned}\n\\frac {\\partial L}{\\partial w^{l-1}}\n=  \\color{blue}{\\frac {\\partial L}{\\partial z^{l-1}}}.\n     \\color{green}{\\frac {\\partial z^{l-1}}{\\partial w^{l-1}}} \\rightarrow \\text{EqA2}\n\\end{aligned}\n\\]\nWe represent the first part in terms of what we computed earlier ie \\(\\color{blue}{\\frac {\\partial L}{\\partial z^{l}}}\\)\n$$\n\\[\\begin{aligned}\n\\color{blue}{\\frac {\\partial L}{\\partial z^{l-1}}} =\n\\color{blue}{\\frac {\\partial L}{\\partial z^{l}}}.\n    \\frac {\\partial z^{l}}{\\partial a^{l-1}}.\n    \\frac {\\partial a^{l-1}}{\\partial z^{l-1}} \\rightarrow \\text{ Eq with respect to Prev Layer}\n  \\\\ \\\\\n  \\color{blue}{\\frac {\\partial L}{\\partial z^{l}}} = \\color{blue}{(p_i- y_i)}\n  \\text{ from the previous layer (from EqA1.1) }\n  \\\\ \\\\\n   z^l = w^l a^{l-1}+b^l\n    \\text{ which makes }\n    {\\frac {\\partial z^{l} }{\\partial a^{l-1}} = w^l} \\\\\n    \\text{ and }\na^{l-1} = \\sigma (z^{l-1})     \\text{ which makes }\n\\frac {\\partial a^{l-1}}{\\partial z^{l-1}} = \\sigma \\color{red}{'} (z^{l-1} )\n\n\n\\\\ \\\\ \\text{ Putting together we get the first part of Eq A2 }\n\\\\\\\\\n\\color{blue}{\\frac {\\partial L}{\\partial z^{l-1}}} =\\color{blue}{(p_i- y_i).w^l.\\sigma }\\color{red}{'} (z^{l-1} ) \\rightarrow \\text{EqA2.1 }\n\\\\ \\\\\nz^{l-1} = w^{l-1} a^{l-2}+b^{l-1}\n    \\text{ which makes }\n    \\color{green}{\\frac {\\partial z^{l-1}}{\\partial w^{l-1}}=a^{l-2}}\n\\\\ \\\\\n\\frac {\\partial L}{\\partial w^{l-1}}\n=  \\color{blue}{\\frac {\\partial L}{\\partial z^{l-1}}}.\n     \\color{green}{\\frac {\\partial z^{l-1}}{\\partial w^{l-1}}} = \\color{blue}{(p_i- y_i).w^l.\\sigma '(z^{l-1} )}.\\color{green}{a^{l-2}}\n\\end{aligned}\\]\n$$\nNote All the other layers should use the previously calculated value of \\(\\color{blue}{\\frac {\\partial L}{\\partial z^{l-i}}}\\) where \\(i= current layer-1\\)\n\\[\n\\begin{aligned}\n\\frac {\\partial L}{\\partial w^{l-2}}\n=  \\color{blue}{\\frac {\\partial L}{\\partial z^{l-2}}}.\n     \\color{green}{\\frac {\\partial z^{l-2}}{\\partial w^{l-2}}}\n      \\  \\color{red}{ \\ne (p_i- y_i)}.\\color{blue}{w^{l-1}.\\sigma '(z^{l-2} )}.\\color{green}{a^{l-3}}\n\\end{aligned}\n\\]",
    "crumbs": [
      "Backprop With Softmax and Cross Entropy"
    ]
  },
  {
    "objectID": "8_backpropogation_full.html#implementation-in-python",
    "href": "8_backpropogation_full.html#implementation-in-python",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Implementation in Python",
    "text": "Implementation in Python\nHere is an implementation of a relatively simple Convolutional Neural Network to test out the forward and back-propagation algorithms given above https://github.com/alexcpn/cnn_in_python. The code is well commented and you will be able to follow the forward and backward propagation with the equations above. Note that the full learning cycle is not completed; but rather a few Convolutional layers, forward propagation and backward propogation for last few layers.",
    "crumbs": [
      "Backprop With Softmax and Cross Entropy"
    ]
  },
  {
    "objectID": "8_backpropogation_full.html#gradient-descent-1",
    "href": "8_backpropogation_full.html#gradient-descent-1",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Gradient descent",
    "text": "Gradient descent\nUsing Gradient descent we can keep adjusting the inner layers like\n\\[\n     w{^{l-1}}{_i} = w{^{l-1}}{_i} -\\alpha *  \\frac {\\partial L}{\\partial w^{l-1}}\n\\]",
    "crumbs": [
      "Backprop With Softmax and Cross Entropy"
    ]
  },
  {
    "objectID": "8_backpropogation_full.html#some-implementation-details",
    "href": "8_backpropogation_full.html#some-implementation-details",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Some Implementation details",
    "text": "Some Implementation details\nFeel free to skip this section. These are some doubts that can come during implementation,and can be refereed to if needed.\nFrom Index Notation to Matrix Notation\nThe above equations are correct only as far as the index notation is concerned. But practically we work with Weight matrices, and for that we need to write this Equation in Matrix Notation. For that some of the terms becomes Transposes, some matrix multiplication (dot product style) and some Hadamard product. (\\(\\odot\\)). This is illustrated and commented in the code and deviates from the equations as is,\nExample \\[\n\\frac{\\partial z^2}{\\partial w^2} = (a^{1})^T\n\\]\nThe Jacobian Matrix\nFor an input vector \\(\\textbf{x} = \\{x_1, x_2, \\dots, x_n\\}\\) on which an element wise function is applied; say the activation function sigmoid \\(\\sigma\\); and it give the output vector \\(\\textbf{a} = \\{a_1, a_2, \\dots, a_n\\}\\)\n$a_i= f(x_i); $\nIn scalar case this becomes \\(\\frac { \\partial f(x)}{ \\partial x} = f'(x)\\)\nIn Vector case, that is when we take the derivative of a vector with respect to another vector to get the following (square) Jacobian matrix\nExample from ref 2\n\\[\n\\begin{aligned}\n\\\\ \\\\\n\\text{The Jacobain, J } = \\frac {\\partial a}{\\partial x} =\n\\begin{bmatrix}\n                \\frac{\\partial a_{1}}{\\partial x_{1}}  & \\frac{\\partial a_{2}}{\\partial x_{1}}     & \\dots     & \\frac{\\partial a_{n}}{\\partial x_{1}}    \\\\\n                \\frac{\\partial a_{1}}{\\partial x_{2}}  & \\frac{\\partial a_{2}}{\\partial x_{2}}     & \\dots     & \\frac{\\partial a_{n}}{\\partial x_{2}}    \\\\\n                \\vdots  & \\vdots    & \\ddots    & \\vdots    \\\\\n                \\frac{\\partial a_{1}}{\\partial x_{n}}  & \\frac{\\partial a_{2}}{\\partial x_{n}}    & \\dots     & \\frac{\\partial a_{n}}{\\partial x_{n}}    \\\\\n\\end{bmatrix}\n\\end{aligned}\n\\]\nThe diagonal of J are the only terms that can be nonzero:\n\\[\n\\begin{aligned}\nJ = \\begin{bmatrix}\n                \\frac{\\partial a_{1}}{\\partial x_{1}}  & 0     & \\dots     & 0    \\\\\n                0  & \\frac{\\partial a_{2}}{\\partial x_{2}}     & \\dots     & 0    \\\\\n                \\vdots  & \\vdots    & \\ddots    & \\vdots    \\\\\n                0  & 0    & \\dots     & \\frac{\\partial a_{n}}{\\partial x_{n}}    \\\\\n        \\end{bmatrix}\n\\end{aligned}\n\\]\n\\[\n\\text{ As }\n(\\frac{\\partial a}{\\partial x})_{ij} = \\frac{\\partial a_i}{\\partial x_j} = \\frac { \\partial f(x_i)}{ \\partial x_j} =\n\\begin{cases}\nf'(x_i)  & \\text{if $i=j$} \\\\\n0 & \\text{otherwise}\n\\end{cases}\n\\] And the authors go on to explain that \\(\\frac{\\partial a}{\\partial x}\\) can be written as \\(\\text{diag}(f'(x))\\) and the Hadamard or element-wise multiplication (\\(\\odot\\) or \\(\\circ\\)) can be applied instead of matrix multiplication to this Jacobian matrix like \\(\\odot f'(x)\\) when applying the Chain Rule and converting from index notation to matrix notation.\nHowever,while implementing the neural network practically the input is not a Vector but an \\(M*N\\) dimensional Matrix ; \\(M, N &gt; 1\\).\nTaking a simple \\(2*2\\) input matrix on which the sigmoid activation function is done; the Jacobian of the same is a \\(4*4\\) matrix.\nDoes it make sense to say the derivative of Matrix \\(a_{i,j}\\) - where an element-wise function is applied; over the input matrix \\(x_{i,j}\\) as a Jacobian ?\n\\[\n\\frac{\\partial A}{\\partial X} = J\n\\]\nThere is no certainty that this will be a square matrix and we can generalize to the diagonal ?\nHowever, all articles treat this matrix case as a generalization of the Vector case and write \\(\\frac{\\partial a}{\\partial x}\\) as the \\(\\text{diag}(f'(x))\\), and then use the element-wise/Hadamard product for the Chain Rule. This way also in implementation. But there is no meaning of diagonal in a non-square matrix.\n\nWhat is basically done is to flatten the Matrix out\n\\[\n\\begin{aligned}\n\\text{Let's take a 2x2 matrix , X } =\n\\begin{bmatrix}\n                x_{ {1}{1} }  & x_{ {1}{2} } \\\\\n                x_{ {2}{1} }  & x_{ {2}{2} }\n\\end{bmatrix}\n\\end{aligned}\n\\] On which an element wise operation is done \\(a_{ {i}{j} } = \\sigma ({x_{ {i}{j} }})\\) Writing that out as matrix \\(A\\) \\[\n\\begin{aligned}\nA =\n\\begin{bmatrix}\n                a_{ {1}{1} }  & a_{ {1}{2} }   \\\\\n                a_{ {2}{1} }  & a_{ {2}{2} }   \\\\\n\\end{bmatrix}\n\\end{aligned}\n\\]\nThe partial derivative of the elements of A with its inputs is \\(\\frac {\\partial A }{\\partial x_{ {i}{j} }}\\)\n\\[\n\\begin{aligned}\n\\frac {\\partial \\vec A }{\\partial X} =\n\\begin{bmatrix}\n                a_{ {1}{1} }  & a_{ {1}{2} }  &  a_{ {2}{1} }  & a_{ {2}{2} }   \\\\\n\\end{bmatrix}\n\\end{aligned}\n\\] We vectorized the matrix; Now we need to take the partial derivative of the vector with each element of the matrix \\(X\\)\n\\[\n\\begin{aligned}\n\\frac {\\partial \\vec A }{\\partial X} =\n\\begin{bmatrix}\n\\frac{\\partial  a_{ {1}{1} } }{\\partial x_{ {1}{1} }} &   \\frac{\\partial  a_{ {1}{2} } }{\\partial x_{ {1}{1} }} &   \\frac{\\partial  a_{ {2}{1} } }{\\partial x_{ {1}{1} }} &   \\frac{\\partial  a_{ {2}{2}} }{\\partial x_{ {1}{1}}}  \\\\ \\\\\n\\frac{\\partial  a_{ {1}{1}} }{\\partial x_{ {1}{2}}} &   \\frac{\\partial  a_{ {1}{2}} }{\\partial x_{ {1}{2}}} &   \\frac{\\partial  a_{ {2}{1}} }{\\partial x_{ {1}{2}}} &   \\frac{\\partial  a_{ {2}{2}} }{\\partial x_{ {1}{2}}}  \\\\ \\\\\n\\frac{\\partial  a_{ {1}{1}} }{\\partial x_{ {2}{1}}} &   \\frac{\\partial  a_{ {1}{2}} }{\\partial x_{ {2}{1}}} &   \\frac{\\partial  a_{ {2}{1}} }{\\partial x_{ {2}{1}}} &   \\frac{\\partial  a_{ {2}{2}} }{\\partial x_{ {2}{1}}}  \\\\ \\\\\n\\frac{\\partial  a_{ {1}{1}} }{\\partial x_{ {2}{2}}} &   \\frac{\\partial  a_{ {1}{2}} }{\\partial x_{ {2}{2}}} &   \\frac{\\partial  a_{ {2}{1}} }{\\partial x_{ {2}{2}}} &   \\frac{\\partial  a_{ {2}{2}} }{\\partial x_{ {2}{2}}}  \\\\ \\\\\n\\end{bmatrix}\n\\end{aligned}\n\\] The non diagonal terms are of the form \\(\\frac{\\partial  a_{ {i}{j}} }{\\partial x_{ {k}{k}}}\\) and reduce to 0 and we get the resultant Jacobian Matrix as\n\\[\n\\begin{aligned}\n\\frac {\\partial \\vec A }{\\partial X} =\n\\begin{bmatrix}\n\\frac{\\partial  a_{ {i}{j}} }{\\partial x_{ {i}{i}}} & \\cdot \\cdot \\cdot & 0 \\\\\n0 & \\frac{\\partial  a_{ {i}{j}} }{\\partial x_{ {i}{i}}} & \\cdot \\cdot \\cdot  \\\\\n\\cdot \\cdot \\cdot  \\\\\n\\cdot \\cdot \\cdot  & \\cdot \\cdot \\cdot & \\frac{\\partial  a_{ {N}{N}} }{\\partial x_{ {N}{N}}}\n\\end{bmatrix}\n\\end{aligned}\n\\]\n\nHence \\(\\frac{\\partial a_{ {i}{j}}}{\\partial X}\\) can be written as \\(\\text{ diag}(f'(X))\\) ; \\((A =f(X))\\)\nNote that Multiplication of a vector by a diagonal matrix is element-wise multiplication or the Hadamard product; And matrices in Deep Learning implementation can be seen as stacked vectors for simplification.\nMore details about this here Jacobian Matrix for Element wise Opeation on a Matrix (not Vector)",
    "crumbs": [
      "Backprop With Softmax and Cross Entropy"
    ]
  },
  {
    "objectID": "8_backpropogation_full.html#references",
    "href": "8_backpropogation_full.html#references",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "References",
    "text": "References\nEasier to follow (without explicit Matrix Calculus) though not really correct - Supervised Deep Learning Marc’Aurelio Ranzato DeepMind\nEasy to follow but lacking in some aspects - Notes on Backpropagation-Peter Sadowski Slightly hard to follow using the Jacobian - The Softmax function and its derivative-Eli Bendersky More difficult to follow with proper index notations (I could not) and probably correct - Backpropagation In Convolutional Neural Networks Jefkine",
    "crumbs": [
      "Backprop With Softmax and Cross Entropy"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "The Maths behind Neural Networks",
    "section": "",
    "text": "Alex Punnen\n© All Rights Reserved\n\n\n\n\nChapter 1: The simplest Neural Network - Perceptron using Vectors and Dot Products\nChapter 2: Perceptron Training via Feature Vectors & HyperPlane split\nChapter 3: Gradient Descent and Optimization\nChapter 4: Back Propagation - Pass 1 (Chain Rule)\nChapter 5: A Simple NeuralNet with Back Propagation\nChapter 6: Back Propagation Pass 3 (Matrix Calculus)\nChapter 7: Back Propagation in Full - With Softmax & CrossEntropy Loss",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#contents",
    "href": "index.html#contents",
    "title": "The Maths behind Neural Networks",
    "section": "",
    "text": "Chapter 1: The simplest Neural Network - Perceptron using Vectors and Dot Products\nChapter 2: Perceptron Training via Feature Vectors & HyperPlane split\nChapter 3: Gradient Descent and Optimization\nChapter 4: Back Propagation - Pass 1 (Chain Rule)\nChapter 5: A Simple NeuralNet with Back Propagation\nChapter 6: Back Propagation Pass 3 (Matrix Calculus)\nChapter 7: Back Propagation in Full - With Softmax & CrossEntropy Loss",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "4_backpropogation_chainrule.html",
    "href": "4_backpropogation_chainrule.html",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "",
    "text": "Alex Punnen\n© All Rights Reserved\n\nContents",
    "crumbs": [
      "Backprop (Chain Rule)"
    ]
  },
  {
    "objectID": "4_backpropogation_chainrule.html#back-propagation",
    "href": "4_backpropogation_chainrule.html#back-propagation",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Back Propagation",
    "text": "Back Propagation\nNeural network is basically a set of inputs connected through ‘weights’ to a set of activation functions whose output will be the input for the next layers and so on.\n\n\n\nneuralnetwork\n\n\nFor training a neural network we need a dataset which has the input and expected output. The weights are randomly initialized, and the inputs passed into the activation function and gives an output.\nThis output or computed values can be compared to the expected value and the difference gives the error of the network. We can create a ‘Cost Function’ like what we saw in the last chapter say the Mean Squared Error",
    "crumbs": [
      "Backprop (Chain Rule)"
    ]
  },
  {
    "objectID": "4_backpropogation_chainrule.html#how-backpropagation-works",
    "href": "4_backpropogation_chainrule.html#how-backpropagation-works",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "How Backpropagation works",
    "text": "How Backpropagation works\nConsider a neural network with multiple layers. The weight of layer \\(l\\) is \\(w^l\\). And for the previous layer it is \\(w^{(l-1)}\\).\nThe best way to understand backpropagation is visually and by the way it is done by the tree representation of 3Blue1Brown video linked here.\nThe below GIF is a representation of a single path in the last layer(\\(l\\) of a neural network; and it shows how the connection from previous layer - that is the activation of the previous layer and the weight of the current layer is affecting the output; and thereby the final Cost.\nThe central idea is how a small change in weight in the previous layer affects the final output of the network.\n Source : Author",
    "crumbs": [
      "Backprop (Chain Rule)"
    ]
  },
  {
    "objectID": "4_backpropogation_chainrule.html#writing-this-out-as-chain-rule",
    "href": "4_backpropogation_chainrule.html#writing-this-out-as-chain-rule",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Writing this out as Chain Rule",
    "text": "Writing this out as Chain Rule\nHere is a more detailed depiction of how the small change in weight adds through the chain to affect the final cost, and how much the small change of weight in an inner layer affect the final cost.\nThis is the Chain Rule of Calculus and the diagram is trying to illustrate that visually via a chain of activations, via a Computational Graph\n\\[\n\\delta C_0/\\delta w^l = \\delta z^l/\\delta w^l . \\delta a^l/\\delta z^l . \\delta C_0/\\delta a^l\n\\]\n Source : Author\nNext part of the recipe is adjusting the weights of each layers, depending on how they contribute to the Cost. We have already seen this in the previous chapter.\nThe weights in each layer are adjusted in proportion to how each layers weights affected the Cost function.\nThis is by calculating the new weight by following the negative of the gradient of the Cost function - basically by gradient descent.\n$$\nW^l_{new} = W^l_{old} - learningRate* C_0/ w^l\n$$\nFor adjusting the weight in the \\((l-1)\\) layer, we do similar\nFirst calculate how the weight in this layer contributes to the final Cost or Loss\n\\[\n\\delta C_0/\\delta w^{l-1} = \\delta z^{l-1}/\\delta w^{l-1} . \\delta a^{l-1}/\\delta z^{l-1} . \\delta C_0/\\delta a^{l-1}\n\\]\nand using this. Basically we are using Chain rule to find the partial differential using the partial differentials calculated in earlier steps.\n\\[\n  W^{l-1}_{new} = W^{l-1}_{old} - learningRate* \\delta C_0/ \\delta w^{l-1}\n\\]",
    "crumbs": [
      "Backprop (Chain Rule)"
    ]
  },
  {
    "objectID": "4_backpropogation_chainrule.html#nerual-net-as-a-composition-of-vector-functions",
    "href": "4_backpropogation_chainrule.html#nerual-net-as-a-composition-of-vector-functions",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Nerual Net as a Composition of Vector Functions",
    "text": "Nerual Net as a Composition of Vector Functions\nLets first look at a neural network as a composition of vector functions.\nImagine a simple neural network with 3 layers. It is essentially a composition of three functions:\nA neural network is a composition of vector-valued functions, followed by a scalar-valued cost function:\n\\[\nC = \\text{Cost}(a_3) \\\\\na_3 = L_3(L_2(L_1(x)))\n\\]\nWhere \\(L_1\\), \\(L_2\\) and \\(L_3\\) are the three layers of the network and\nEach layer is defined as:\n\\[\nz_i = W_i a_{i-1} + b_i, \\quad a_i = \\sigma(z_i)\n\\]\nAnd gradient descent is defined as:\n\\[w_{i_{new}} = w_{i_{old}} - \\eta \\cdot \\partial C / \\partial w_i\\]\nProblem is to find the partial derivative of the loss function with respect to the weights at each layer.\nTo calculate how a change in the first layer’s weights (\\(w_1\\)) affects the final Cost (\\(C\\)), we have to trace the “path of influence” all the way through the network.\nA nudge in \\(w_1\\) changes the output of Layer 1. The change in Layer 1 changes the input to Layer 2. The change in Layer 2 changes the input to Layer 3. The change in Layer 3 changes the final Cost.\nMathematically, we multiply the derivatives (Linear Maps) of these links together:\nWe need to update weights of three layers\n\\[w_{1_{new}} = w_{1_{old}} - \\eta \\cdot \\partial C / \\partial w_1\\]\n\\[w_{2_{new}} = w_{2_{old}} - \\eta \\cdot \\partial C / \\partial w_2\\]\n\\[w_{3_{new}} = w_{3_{old}} - \\eta \\cdot \\partial C / \\partial w_3\\]\nAnd for that we need to find $ C / w_1 $, $ C / w_2 $, $ C / w_3 $.\nLets write down the chain rule for each layer:\n\\[\\frac{\\partial C}{\\partial w_1} = \\frac{\\partial C}{\\partial L_3} \\cdot \\frac{\\partial L_3}{\\partial L_2} \\cdot \\frac{\\partial L_2}{\\partial L_1} \\cdot \\frac{\\partial L_1}{\\partial w_1}\\]\n\\[\\frac{\\partial C}{\\partial w_2} = \\frac{\\partial C}{\\partial L_3} \\cdot \\frac{\\partial L_3}{\\partial L_2} \\cdot \\frac{\\partial L_2}{\\partial w_2}\\]\n\\[\\frac{\\partial C}{\\partial w_3} = \\frac{\\partial C}{\\partial L_3} \\cdot \\frac{\\partial L_3}{\\partial w_3}\\]\nWhy is this written this way? By the chain rule, the derivative of a composition of functions is the product of the derivatives of the functions. It is thus easy to calculate the gradient of the loss with respect to the weights of each layer.\nLets calculate the gradient of the loss with respect to the weights of the first layer.\nNotice something interesting?\n\nTo calculate \\(\\frac{\\partial C}{\\partial w_3}\\), we need \\(\\frac{\\partial C}{\\partial L_3}\\).\nTo calculate \\(\\frac{\\partial C}{\\partial w_2}\\), we need \\(\\frac{\\partial C}{\\partial L_3} \\cdot \\frac{\\partial L_3}{\\partial L_2}\\).\nTo calculate \\(\\frac{\\partial C}{\\partial w_1}\\), we need \\(\\frac{\\partial C}{\\partial L_3} \\cdot \\frac{\\partial L_3}{\\partial L_2} \\cdot \\frac{\\partial L_2}{\\partial L_1}\\).\n\nWe are re-calculating the same terms over and over again!\nIf we start from the Output (Layer 3) and move Backwards: 1. We calculate \\(\\frac{\\partial C}{\\partial L_3}\\) once. We use it to find the update for \\(w_3\\).\n\nWe pass this value back to find \\(\\frac{\\partial C}{\\partial L_2}\\) (which is \\(\\frac{\\partial C}{\\partial L_3} \\cdot \\frac{\\partial L_3}{\\partial L_2}\\)). We use it to find the update for \\(w_2\\).\nWe pass that value back to find \\(\\frac{\\partial C}{\\partial L_1}\\). We use it to find the update for \\(w_1\\).\n\nThis avoids redundant calculations and is why it’s called Backpropagation.\nIt is essentially Dynamic Programming applied to the Chain Rule.\n\nThe Backpropagation Algorithm Step-by-Step\nStep 1: The Output Layer (\\(L_3\\))\nWe want to find the gradient \\(\\frac{\\partial C}{\\partial w_3}\\). Using the Chain Rule: \\[ \\frac{\\partial C}{\\partial w_3} = \\frac{\\partial C}{\\partial a_3} \\cdot \\frac{\\partial a_3}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial w_3} \\]\nLet’s break it down term by term:\n\nDerivative of Cost w.r.t Activation (\\(\\frac{\\partial C}{\\partial a_3}\\)): For MSE \\(C = \\frac{1}{2}(a_3 - y)^2\\): \\[ \\frac{\\partial C}{\\partial a_3} = (a_3 - y) \\]\nDerivative of Activation w.r.t Input (\\(\\frac{\\partial a_3}{\\partial z_3}\\)): Since \\(a_3 = \\sigma(z_3)\\): \\[ \\frac{\\partial a_3}{\\partial z_3} = \\sigma'(z_3) \\]\nDerivative of Input w.r.t Weights (\\(\\frac{\\partial z_3}{\\partial w_3}\\)): Since \\(z_3 = w_3 a_2 + b_3\\): \\[ \\frac{\\partial z_3}{\\partial w_3} = a_2 \\]\n\nCombining them: We define the “error” term \\(\\delta_3\\) at the output layer as: \\[ \\delta_3 = \\frac{\\partial C}{\\partial z_3} = (a_3 - y) \\odot \\sigma'(z_3) \\]\n\nNote on \\(\\odot\\) (Hadamard Product): We use element-wise multiplication here because both \\((a_3 - y)\\) and \\(\\sigma'(z_3)\\) are vectors of the same size.\nThe Jacobian of an element-wise activation \\(\\sigma\\) is a diagonal matrix: \\[ \\frac{\\partial a}{\\partial z} = \\text{diag}(\\sigma'(z)) \\]\nSo multiplying by it is the same as a Hadamard product: \\[ \\text{diag}(\\sigma'(z)) \\, v = v \\odot \\sigma'(z) \\]\n\nSo the gradient for the weights is: \\[ \\frac{\\partial C}{\\partial w_3} = \\delta_3 \\cdot a_2^T \\]\n\nNote on Transpose (\\(a_2^T\\)): In backprop, we push gradients through a linear map \\(z = Wa + b\\). The Jacobian w.r.t. \\(a\\) is \\(W\\), so the chain rule gives:\n\\[ \\frac{\\partial C}{\\partial a} = W^T \\frac{\\partial C}{\\partial z} \\]\nThe transpose appears because we’re applying the transpose (adjoint) of the Jacobian to move gradients backward.\n\nResult: We have the update for \\(w_3\\).\n\\[w_{3_{new}} = w_{3_{old}} - \\eta \\cdot \\partial C / \\partial w_3\\]\nStep 2: Propagate Back to \\(L_2\\)\nNow we need to find the gradient for the second layer weights: \\(\\frac{\\partial C}{\\partial w_2}\\). Using the Chain Rule, we can reuse the error from the layer above: \\[ \\frac{\\partial C}{\\partial w_2} = \\frac{\\partial C}{\\partial z_2} \\cdot \\frac{\\partial z_2}{\\partial w_2} = \\delta_2 \\cdot a_1^T \\]\nBut what is \\(\\delta_2\\) (the error at layer 2)? \\[ \\delta_2 = \\frac{\\partial C}{\\partial z_2} = \\frac{\\partial C}{\\partial z_3} \\cdot \\frac{\\partial z_3}{\\partial z_2} \\]\nWe know \\(\\frac{\\partial C}{\\partial z_3} = \\delta_3\\). And since \\(z_3 = w_3 \\sigma(z_2) + b_3\\): \\[ \\frac{\\partial z_3}{\\partial z_2} = w_3 \\cdot \\sigma'(z_2) \\]\nSo, we can calculate \\(\\delta_2\\) by “backpropagating” \\(\\delta_3\\): \\[ \\delta_2 = (w_3^T \\cdot \\delta_3) \\odot \\sigma'(z_2) \\]\nThe Update Rule for Layer 2: \\[ \\frac{\\partial C}{\\partial w_2} = \\delta_2 \\cdot a_1^T \\]\nResult: We have the update for \\(w_2\\). \\[w_{2_{new}} = w_{2_{old}} - \\eta \\cdot \\frac{\\partial C}{\\partial w_2}\\]\nStep 3: Propagate Back to \\(L_1\\)\nWe repeat the exact same process to find the error at the first layer \\(\\delta_1\\). \\[ \\delta_1 = (w_2^T \\cdot \\delta_2) \\odot \\sigma'(z_1) \\]\nThe Update Rule for Layer 1: \\[ \\frac{\\partial C}{\\partial w_1} = \\delta_1 \\cdot x^T \\] (Recall that \\(a_0 = x\\), the input).\nResult: We have the update for \\(w_1\\). \\[w_{1_{new}} = w_{1_{old}} - \\eta \\cdot \\frac{\\partial C}{\\partial w_1}\\]\n\n\nSummary\nSo, Backpropagation is the efficient execution of the Chain Rule by utilizing the linear maps of each layer in reverse order. * It computes the local linear map (Jacobian) of a layer. * It takes the incoming gradient vector from the future layer. * It performs a Vector-Jacobian Product to pass the gradient to the past layer.\n\nNext A Simple NeuralNet with Back Propagation\n\n\n\n\nReferences\nhttp://neuralnetworksanddeeplearning.com/chap2.html\nhttps://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/",
    "crumbs": [
      "Backprop (Chain Rule)"
    ]
  },
  {
    "objectID": "3_gradient_descent.html",
    "href": "3_gradient_descent.html",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "",
    "text": "Alex Punnen\n© All Rights Reserved\n\nContents",
    "crumbs": [
      "Gradient Descent"
    ]
  },
  {
    "objectID": "3_gradient_descent.html#neural-network-as-a-chain-of-functions",
    "href": "3_gradient_descent.html#neural-network-as-a-chain-of-functions",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Neural Network as a Chain of Functions",
    "text": "Neural Network as a Chain of Functions\nTo understand deep learning, we need to understand the concept of a neural network as a chain of functions.\nA Neural Network is essentially a chain of functions. It consists of a set of inputs connected through ‘weights’ to a set of activation functions, whose output becomes the input for the next layer, and so on.\n\n\n\nneuralnetwork\n\n\n\nThe Forward Pass\nLet’s consider a simple two-layer neural network.\n\n\\(x\\): Input vector\n\\(y\\): Output vector (prediction)\n\\(L\\): Number of layers\n\\(w^l, b^l\\): Weights and biases for layer \\(l\\)\n\\(a^l\\): Activation of layer \\(l\\) (we use sigmoid \\(\\sigma\\) here)\n\nThe flow of data (Forward Pass) can be represented as:\n\\[\nx \\rightarrow a^{1} \\rightarrow \\dots \\rightarrow a^{L} \\rightarrow y\n\\]\nFor any layer \\(l\\), the activation \\(a^l\\) is calculated as:\n\\[\n  a^{l} = \\sigma(w^l a^{l-1} + b^l)\n\\]\nwhere \\(a^0 = x\\) (the input).\nThe linear transformation:\n\\(z = w^T x + b\\)\ndefines a hyperplane (decision boundary) in the feature space.\nThe activation function then introduces non-linearity, allowing the network to combine multiple such hyperplanes into complex decision boundaries.\n\nWhy Non-Linearity Is Non-Negotiable\nWithout activation:\n\\[\nf(x) = W_L W_{L-1} \\dots W_1 x\n\\]\nThis collapses to:\n\\[\nf(x) = Wx\n\\]\nStill one big linear transformation and hence one hyperplane; the problems of not able to separate features will come. Only because of non-linearity, we can get multiple hyperplanes and hence a composable complex decision boundaries that can separate features.\nSo the concept of Vectors, Matrices and Hyperplanes remain the same as before. Let us explore the chain of functions part here\nA neural network with \\(L\\) layers can be represented as a nested function:\\[f(x) = f_L(...f_2(f_1(x))...)\\]\nEach “link” in the chain is a layer performing a linear transformation followed by a non-linear activation and cascading to the final output.\n\n\n\nThe Cost Function (Loss Function)\nTo train this network, we need to measure how “wrong” its predictions are compared to the true values. We do this using a Cost Function (or Loss Function).\nA simplest Loss function is just the difference between the predicted output and the true output ($ y(x) - a^L(x) $.)\nBut usually we use the square of the difference to make it a non-negative function.\nA common choice is the Mean Squared Error (MSE):\n\\[\nC = \\frac{1}{2n} \\sum_{x} \\|y(x) - a^L(x)\\|^2\n\\]\n\n\\(n\\): Number of training examples\n\\(y(x)\\): The true expected output (label) for input \\(x\\)\n\\(a^L(x)\\): The network’s predicted output for input \\(x\\)\n\n\n\nThe Goal of Training\nThe goal of training is to find the set of weights \\(w\\) and biases \\(b\\) that minimize this cost \\(C\\).\nThis means that we need to optimise each component of the function \\(f(x)\\) to reduce the cost proportional to its contribution to the final output. The method to do this is called Backpropagation. It helps us calculate the gradient of the cost function with respect to each weight and bias.\nOnce the gradient is calculated, we can use Gradient Descent to update the weights in the opposite direction of the gradient.\nGradient descent is a simple optimization algorithm that works by iteratively updating the weights in the opposite direction of the gradient.\nHowever neural network is a composition of vector spaces and linear transformations. Hence gradient descent acts on a very complex space.\nThere are two or three facts to understand about gradient descent:\n\nIt does not attempt to find the global minimum, but rather follows the local slope of the cost function and converges to a local minimum or a flat region. Saddle point is a good optimisation point.\nGradients can vanish or explode, leading to slow or unstable convergence. The practical solution to control this is to use learning rate and using adaptive learning rate methods like Adam or RMSprop.\nBatch Size matters: Calculating the gradient over the entire dataset (Batch Gradient Descent) is computationally expensive and memory-intensive. In practice, we use Stochastic Gradient Descent (SGD) (one example at a time) or, more commonly, Mini-batch Gradient Descent (a small batch of examples). This introduces noise into the gradient estimate, which paradoxically helps the optimization process escape shallow local minima and saddle points.",
    "crumbs": [
      "Gradient Descent"
    ]
  },
  {
    "objectID": "3_gradient_descent.html#optimization-gradient-descent---take-1",
    "href": "3_gradient_descent.html#optimization-gradient-descent---take-1",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Optimization: Gradient Descent - Take 1",
    "text": "Optimization: Gradient Descent - Take 1\nFor the Cost Function \\(C(w, b)\\), we need to find the minimum of this function with respect to the weights \\(w\\) and biases \\(b\\).\nWe calculate the gradient of the cost function with respect to each weight and bias using backpropagation \\[\n\\nabla C = \\begin{bmatrix}\n\\frac{\\partial C}{\\partial w} \\\\\n\\frac{\\partial C}{\\partial b}\n\\end{bmatrix}\n\\]\nThen update the weights in the opposite direction of the gradient:\n\\[\nw_{new} = w_{old} - \\eta \\frac{\\partial C}{\\partial w}\n\\] \\[\nb_{new} = b_{old} - \\eta \\frac{\\partial C}{\\partial b}\n\\]\nwhere \\(\\eta\\) is the learning rate. This update rule is called Gradient Descent.\n\nWhy not Newton’s Method?\nYou might ask, why not use faster optimization methods like Newton’s Method (Newton-Raphson)?\nNewton’s method uses the second derivative (curvature) to find the minimum faster. \\[ w_{new} = w_{old} - \\frac{C'(w)}{C''(w)} \\]\nHowever, for a neural network with millions of weights, calculating the second derivative (the Hessian matrix) is computationally infeasible. Gradient Descent, which uses only the first derivative, is much more scalable and efficient for Deep Learning.",
    "crumbs": [
      "Gradient Descent"
    ]
  },
  {
    "objectID": "3_gradient_descent.html#take-2---deeper-dive",
    "href": "3_gradient_descent.html#take-2---deeper-dive",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Take 2 - Deeper Dive",
    "text": "Take 2 - Deeper Dive\nThe Error function is a scalar function of the weights and biases.\nThe loss (error) is a scalar function of all weights and biases.\nIn linear regression with MSE, the loss is a convex quadratic in the parameters, so optimization is well-behaved (a bowl-shaped surface)(e.g. see left in picture).\nIn deep learning, the loss becomes non-convex because it is the result of composing many nonlinear transformations. This creates a complex landscape with saddle points, flat regions, and multiple minima (e.g. see right in picture).\n\n\n\ncostfunction\n\n\nHow will Gradient Descent work in this case - non convex function?\nGradient descent does not attempt to find the global minimum, but rather follows the local slope of the cost function and converges to a local minimum or a flat region.\nThe cost function is differentiable almost everywhere*. At any point in parameter space, the gradient indicates the direction of steepest local increase, and moving in the opposite direction reduces the cost. During optimization, the algorithm may encounter local minima or saddle points.\n(*The function is not differentiable at the point where the function is zero ex ReLU. This is not a problem in practice, as optimization algorithms handle such points using subgradients)\nIn practice, deep learning works well despite non-convexity, partly because modern networks have millions of parameters and their loss landscapes contain many saddle points and wide, flat minima rather than poor isolated local minima.\nFurthermore, we rarely use full-batch gradient descent. Instead, we use variants such as Stochastic Gradient Descent (SGD) or mini-batch gradient descent.\nIn these methods, gradients are computed using a single training example or a small batch of examples rather than the entire dataset. The resulting gradient is an average over the batch and serves as a noisy approximation of the true gradient. This stochasticity helps the optimizer escape saddle points and sharp minima, enabling effective training in practice.",
    "crumbs": [
      "Gradient Descent"
    ]
  },
  {
    "objectID": "3_gradient_descent.html#backpropagation",
    "href": "3_gradient_descent.html#backpropagation",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Backpropagation",
    "text": "Backpropagation\nWhat does it mean to take the derivative of a scalar function with respect to vector-valued parameters?\nFinding the gradient is the job of Backpropagation.\nBackpropagation is based on Automatic Differentiation. Auto Diff is a fancy term to describe creating a computational graph and then differentiating it- via chain rule.\nChain rule is a basic rule of differentiation of composite functions.\nIn Neural networks each function is composed of vector functions.\nWhat is a derivative of a vector function? This is something hard to explain in a simple way. It is a matrix of gradients.\nThis paper - The Simple Essence of Automatic Differentiation Extended version Conal Elliott explains this in detail. Quoting from it below.\n\n\nJacobian Matrix\nThe derivative \\(f'(x)\\) of a function \\(f: \\mathbb{R} \\to \\mathbb{R}\\) at a point \\(x\\) (in the domain of \\(f\\)) is a number, defined as follows:\n\\[\nf'(x) = \\lim_{\\epsilon \\to 0} \\frac{f(x + \\epsilon) - f(x)}{\\epsilon}\n\\]\nThat is, \\(f'(x)\\) tells us how fast \\(f\\) is scaling input changes at \\(x\\).\nNote here that \\(\\mathbb{R}\\) is the set of real numbers and \\(\\epsilon\\) is also a real number.\nHow well does this definition hold up beyond functions of type \\(\\mathbb{R} \\to \\mathbb{R}\\)?\nWhen we extend to \\(\\mathbb{R}^m \\to \\mathbb{R}^n\\), this definition no longer makes sense, as it would rely on dividing by a vector \\(\\epsilon \\in \\mathbb{R}^m\\).\nThis difficulty of differentiation with non-scalar domains is usually addressed with the notion of “partial derivatives” with respect to the \\(m\\) scalar components of the domain \\(\\mathbb{R}^m\\).\nWhen the codomain \\(\\mathbb{R}^n\\) is also non-scalar (i.e., \\(n &gt; 1\\)), we have a matrix \\(J\\) (the Jacobian), with \\(J_{ij} = \\partial f_i / \\partial x_j\\) for \\(i \\in \\{1, \\dots, n\\}\\), where each \\(f_i\\) projects out the \\(i\\)-th scalar value from the result of \\(f\\).\nMoreover, each of these situations has an accompanying chain rule, which says how to differentiate the composition of two functions. Where the scalar chain rule involves multiplying two scalar derivatives, the vector chain rule involves “multiplying” two matrices \\(A\\) and \\(B\\) (the Jacobians), defined as follows:\n\\[\n(A \\cdot B)_{ij} = \\sum_{k=1}^m A_{ik} \\cdot B_{kj}\n\\]\nSince one can think of scalars as a special case of vectors, and scalar multiplication as a special case of matrix multiplication, perhaps we’ve reached the needed generality.\nThe derivative of a function \\(f: a \\to b\\) at some value in \\(a\\) is thus not a number, vector, matrix, or higher-dimensional variant, but rather a linear map (also called “linear transformation”) from \\(a\\) to \\(b\\), which we will write as \\(a \\multimap b\\).\nThe numbers, vectors, matrices, etc. mentioned above are all different representations of linear maps; and the various forms of “multiplication” appearing in their associated chain rules are all implementations of linear map composition for those representations.\n\nThis above passage is taken from the paper on auto differentiation by Conal Elliott. Why I put it here is that it applies to our case of gradient descent as well.\nSo the Loss function is a function of weight vectors \\(w\\) and bias vectors \\(b\\).\n\\[\nC(w, b)\n\\]\nTo minimize this, we need the derivative. But since our input is a vector (the weights) and our output is a scalar (the loss), what does the derivative look like?\nFollowing Conal Elliott’s definition, the derivative is not just a number, but a Linear Map. It is a function that tells us: ‘If we nudge the weights by a tiny vector \\(\\vec{v}\\), how much will the Loss change?’\nWe represent this abstract linear map using a concrete list of numbers called the Gradient Vector \\(\\nabla C\\).\n\\[\n\\nabla C = \\begin{bmatrix}\n\\frac{\\partial C}{\\partial w} \\\\\n\\frac{\\partial C}{\\partial b}\n\\end{bmatrix}\n\\]\nThis linear map tells us how any small change in the parameters will change the loss. Gradient descent exploits this first-order approximation by choosing the parameter update that most rapidly decreases the loss.\nMathematically this is more rigorous than the usual explanation of rolling down the gradient/slope which does not make sense in the context of a vector space.\nSo what is Gradient Descent?\nGradient Descent takes the Gradient Vector found by Backprop and performs a vector subtraction in the weight space:\n\\[w_{new} = w_{old} - \\eta \\cdot \\nabla C\\]\nwhere \\(\\eta\\) is the learning rate.\nAnd how do we find the gradient vector \\(\\nabla C\\)? via Backpropagation\nNext: Backpropagation",
    "crumbs": [
      "Gradient Descent"
    ]
  },
  {
    "objectID": "7_backpropogation_matrix_calculus.html",
    "href": "7_backpropogation_matrix_calculus.html",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "",
    "text": "Alex Punnen\n© All Rights Reserved\n\nContents",
    "crumbs": [
      "Backprop (Matrix Calculus)"
    ]
  },
  {
    "objectID": "7_backpropogation_matrix_calculus.html#back-propagation--matrix-calculus",
    "href": "7_backpropogation_matrix_calculus.html#back-propagation--matrix-calculus",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Back Propagation -Matrix Calculus",
    "text": "Back Propagation -Matrix Calculus\nThe previous chapters we used a Scalar derivation of the Back Propagation formula to implement it in a simple two layer neural network. What we have done is is to use Hadmard product and matrix transposes with scalar derivation alignment.\nLet’s take the previous two layered simple neural network,with a Mean Square Error Loss function, and derive the Back Propagation formula with Matrix Calculus now.\nLet’s write the equation of the following neural network\nx is the Input\ny is the Output.\nl is the number of layers of the Neural Network.\na is the activation function ,(we use sigmoid here)\n\\[\nx \\rightarrow a^{l-1} \\rightarrow  a^{l} \\rightarrow  y\n\\]\nWhere the activation \\(a^l\\) is \\[\n  a^{l} = \\sigma(w^l a^{l-1}+b^l).\n\\]\nand\n\\[\na^{l} = \\sigma(z^l) \\quad where \\quad\nz^l =w^l a^{l-1} +b^l\n\\]\nOur two layer neural network can be written as\n\\[\n\\mathbf { a^0 \\rightarrow a^{1} \\rightarrow  a^{2} \\rightarrow  y }\n\\]\n(\\(a^2\\) does not denote the exponent but just that it is of layer 2)\nLets write down the derivative of Loss function wrto weight using chain rule\n\\[\n\\mathbf {\n\\frac {\\partial C}{\\partial w^l}\n= \\frac {\\partial a^l}{\\partial w^l} . \\frac {\\partial C}{\\partial a^l}\n}\n\\]\nWe will use the above equation as the basis for the rest of the chapter.",
    "crumbs": [
      "Backprop (Matrix Calculus)"
    ]
  },
  {
    "objectID": "7_backpropogation_matrix_calculus.html#gradient-vector2d-tensor-of-loss-function-in-last-layer",
    "href": "7_backpropogation_matrix_calculus.html#gradient-vector2d-tensor-of-loss-function-in-last-layer",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Gradient Vector/2D-Tensor of Loss function in last layer",
    "text": "Gradient Vector/2D-Tensor of Loss function in last layer\n\\[\nC = \\frac{1}{2} \\sum_j (y_j-a^L_j)^2\n\\]\nAssuming a neural net with 2 layers, we have the final Loss as\n\\[\nC = \\frac{1}{2} \\sum_j (y_j-a^2_j)^2\n\\]\nWhere\n\\[\na^2 = \\sigma(w^2.a^1)\n\\]\nWe can then write\n\\[\nC = \\frac{1}{2} \\sum_j v^2 \\quad \\rightarrow (Eq \\;A)\n\\]\nWhere\n\\[\nv= y-a^2\n\\]",
    "crumbs": [
      "Backprop (Matrix Calculus)"
    ]
  },
  {
    "objectID": "7_backpropogation_matrix_calculus.html#partial-derivative-of-loss-function-with-respect-to-weight",
    "href": "7_backpropogation_matrix_calculus.html#partial-derivative-of-loss-function-with-respect-to-weight",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Partial Derivative of Loss function with respect to Weight",
    "text": "Partial Derivative of Loss function with respect to Weight\nFor the last layer, lets use Chain Rule to split like below\n\\[\n\\frac {\\partial C}{\\partial w^2} = \\frac{\\partial v^2}{\\partial v} * \\frac{\\partial v}{\\partial w^2} \\quad \\rightarrow (Eq \\;B)\n\\]\n\\[\n\\frac{\\partial v^2}{\\partial v} =2v \\quad \\rightarrow (Eq \\;B.1)\n\\]\n\\[\n\\frac{\\partial v}{\\partial w^2}=  \\frac{\\partial (y-a^2)}{\\partial w^2} =\n0-\\frac{\\partial a^2}{\\partial w^2} \\quad \\rightarrow (Eq \\;B.2)\n\\]\n\\[\n\\frac {\\partial C}{\\partial w^2} = \\frac{1}{2} *2v(0-\\frac{\\partial a^2}{\\partial w^2}) \\quad \\rightarrow (Eq \\;B)\n\\]  \n\nNow we need to find \\[\\frac{\\partial a^2}{\\partial w^2}\\]\nLet\n\\[\n\\begin{aligned}\na^2= \\sigma(\\sum(w^2 \\otimes a^1 )) = \\sigma(z^2)\n\\\\\\\\\nz^2 =  \\sum(w^2 \\otimes a^1)\n\\\\\\\\\nz^2 = \\sum(k^2) \\; \\text {, where} \\; k^2=w^2 \\otimes a^1\n\\end{aligned}\n\\]\nWe now need to derive an intermediate term which we will use later\n\\[\n\\begin{aligned}\n\\frac{\\partial z^2}{\\partial w^2} =\\frac{\\partial z^2}{\\partial k^2}*\\frac{\\partial k^2}{\\partial w^2}\n\\\\\\\\\n=\\frac {\\partial \\sum(k^2)}{\\partial k^2}* \\frac {\\partial (w^2 \\otimes a^1 )} {\\partial w^2}\n\\\\ \\\\\n\\frac{\\partial z^2}{\\partial w^2} = (1^{\\rightarrow})^T* diag(a^1) =(a^{1})^T \\quad \\rightarrow (Eq \\;B.3)\n\\end{aligned}\n\\]\nThough these are written like scalar here; actually all these are partial differentiation of Vector by Vector, or Vector by Scalar. A set of vectors can be represented as the matrix here.More details here https://explained.ai/matrix-calculus/#sec6.2\nThe Vector dot product \\(w.a\\) when applied on matrices becomes the sum of elementwise multiplication (also called Hadamard product) \\(\\sum w^2 \\otimes a^1\\)\nGoing back to \\(Eq \\;(B.2)\\)\n\\[\n\\frac {\\partial a^2}{\\partial w^2} = \\frac{\\partial a^2}{\\partial z^2} * \\frac{\\partial z^2}{\\partial w^2}\n\\]\nUsing \\(Eq \\;(B.3)\\) for the term in left\n\\[\n=  \\frac{\\partial a^2}{\\partial z^2} * (a^{1})^T\n\\]\n\\[\n=  \\frac{\\partial \\sigma(z^2)}{\\partial z^2} * (a^{1})^T\n\\]\n\\[\n\\frac {\\partial a^2}{\\partial w^2} =   \\sigma^{'}(z^2) * (a^{1})^T \\quad \\rightarrow (Eq \\;B.4)\n\\]\nNow lets got back to partial derivative of Loss function wrto to weight\n\\[\n\\frac {\\partial C}{\\partial w^2} = \\frac {1}{2}*2v(0-\\frac{\\partial a^2}{\\partial w^2}) \\quad \\rightarrow (Eq \\;B)\n\\]\nUsing \\(Eq \\;(B.4)\\) to substitute in the last term\n\\[\n\\begin{aligned}\n= v(0- \\sigma^{'}(z^2) * (a^{1})^T)\n\\\\\\\\\n= v*-1*\\sigma^{'}(z^2) * (a^{1})^T\n\\\\\\\\\n= (y-a^2)*-1*\\sigma^{'}(z^2) * (a^{1})^T\n\\\\\\\\\n\\frac {\\partial C}{\\partial w^2}= (a^2-y)*\\sigma^{'}(z^2) * (a^{1})^T \\quad \\rightarrow \\mathbb Eq \\; (3)\n\\end{aligned}\n\\]",
    "crumbs": [
      "Backprop (Matrix Calculus)"
    ]
  },
  {
    "objectID": "7_backpropogation_matrix_calculus.html#gradient-vector-of-loss-function-in-inner-layer",
    "href": "7_backpropogation_matrix_calculus.html#gradient-vector-of-loss-function-in-inner-layer",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Gradient Vector of Loss function in Inner Layer",
    "text": "Gradient Vector of Loss function in Inner Layer\n\n \nNow let’s do the same for the inner layer. This is bit more tricky and we use the Chain rule to derive this\n \n\\[\n\\frac {\\partial C}{\\partial w^1} = \\frac {\\partial a^1}{\\partial w^1} . \\frac {\\partial C}{\\partial a^1}  \\quad \\rightarrow (4.0)\n\\]\n \nWe can calculate the first part of this from \\(Eq\\; (B.4)\\) that we derived above\n\\[\n\\begin{aligned}\n\\frac {\\partial a^2}{\\partial w^2} =   \\sigma^{'}(z^2) * (a^{1})^T \\quad \\rightarrow (Eq \\;B.4)\n\\\\\\\\\n\\frac {\\partial a^1}{\\partial w^1}  = \\sigma'(z^1) * (a^{0})^T \\quad \\rightarrow (4.1)\n\\end{aligned}\n\\]\nFor the second part, we use Chain Rule to split like below, the first part of which we calculated in the earlier step.\n$$\n\\[\\begin{aligned}\n\\frac{\\partial C}{\\partial(a^1)} =  \\frac{\\partial C}{\\partial(a^2)}.\\frac{\\partial(a^2)}{\\partial(a^1)}\n\\\\\\\\\n{\n\\frac{\\partial C}{\\partial(a^2)} = \\frac {\\partial({\\frac{1}{2} \\|y-a^2\\|^2)}}{\\partial(a^2)} = \\frac{1}{2}*2*(a^2-y) =(a^2-y) = \\delta^{2}  }\n\\\\\\\\\n\\frac {\\partial C}{\\partial(a^2)}  =\\delta^{2}  \\rightarrow (4.2)\\\\ \\\\\n\n\\text {Now to calculate} \\quad\n\n\\frac{\\partial(a^2)}{\\partial(a^1)} \\quad where \\quad\n\na^{2} = \\sigma(w^2 a^{1}+b^2) \\\\ \\\\\n\n\\frac{\\partial(a^2)}{\\partial(a^1)} = \\frac{\\partial(\\sigma(w^2 a^{1}+b^2))}{\\partial(a^1)} =  w^2.\\sigma'(w^2 a^{1}+b^2) = w^2.\\sigma'(z^2)\\rightarrow (4.3)*\n\n\\end{aligned}\\]\n$$\n*https://math.stackexchange.com/a/4065766/284422\nPutting (4.1) (4.2) and (4.3) together",
    "crumbs": [
      "Backprop (Matrix Calculus)"
    ]
  },
  {
    "objectID": "7_backpropogation_matrix_calculus.html#final-equations",
    "href": "7_backpropogation_matrix_calculus.html#final-equations",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Final Equations",
    "text": "Final Equations\n\\[  \\mathbf{\n\\frac {\\partial C}{\\partial w^1} = \\sigma'(z^1) * (a^{0})^T*\\delta^{2}*w^2.\\sigma'(z^2) \\quad \\rightarrow \\mathbb Eq \\; (5)\n}\\]\n\\[\n\\delta^2 = (a^2-y)\n\\]\nAdding also the partial derivate of loss funciton with respect to weight in the final layer\n\\[ \\mathbf{\n\\frac {\\partial C}{\\partial w^2}= \\delta^{2}*\\sigma^{'}(z^2) * (a^{1})^T \\quad \\rightarrow \\mathbb Eq \\; (3)\n}\n\\]\nYou can see that the inner layer derivative have terms from the outer layer. So if we store and use the result; this is like dynamic program; maybe the back=propagation algorithm is the most elegant dynamic programming till date.\n\\[  \\mathbf{\n\\frac {\\partial C}{\\partial w^1} = \\delta^{2}*\\sigma'(z^2)*(a^{0})^T*w^2.\\sigma'(z^1) \\quad \\rightarrow \\mathbb Eq \\; (5)\n}\\]",
    "crumbs": [
      "Backprop (Matrix Calculus)"
    ]
  },
  {
    "objectID": "7_backpropogation_matrix_calculus.html#using-gradient-descent-to-find-the-optimal-weights-to-reduce-the-loss-function",
    "href": "7_backpropogation_matrix_calculus.html#using-gradient-descent-to-find-the-optimal-weights-to-reduce-the-loss-function",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Using Gradient Descent to find the optimal weights to reduce the Loss function",
    "text": "Using Gradient Descent to find the optimal weights to reduce the Loss function\n \nWith equations (3) and (5) we can calculate the gradient of the Loss function with respect to weights in any layel - in this example\n\\[\\frac {\\partial C}{\\partial w^1},\\frac {\\partial C}{\\partial w^2}\\]\n \nWe now need to adjust the previous weight, by gradient descent.\n \nSo using the above gradients we get the new weights iteratively like below. If you notice this is exactly what is happening in gradient descent as well; only chain rule is used to calculate the gradients here. Backpropagation is the algorithm that helps calculate the gradients for each layer.\n \n\\[\\mathbf {\n  W^{l-1}_{new} = W^{l-1}_{old} - learningRate* \\delta C_0/ \\delta w^{l-1}\n}\\]\n \nReference\n\nhttps://cedar.buffalo.edu/~srihari/CSE574/Chap5/Chap5.3-BackProp.pdf\nhttp://neuralnetworksanddeeplearning.com/chap2.html\n\nNext: Back Propagation in Full - With Softmax & CrossEntropy Loss",
    "crumbs": [
      "Backprop (Matrix Calculus)"
    ]
  },
  {
    "objectID": "1_vectors_dot_product_and_perceptron.html",
    "href": "1_vectors_dot_product_and_perceptron.html",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "",
    "text": "Alex Punnen\n© All Rights Reserved\n\nContents\nEven the most complex Neural network is based on vectors and matrices, and it uses the concept of a cost function and algorithms like gradient descent to find a reduced cost. Then, it propagates the cost back to all constituents of the network proportionally via a method called back-propagation.\nHave you ever held an integrated circuit or chip in your hand or seen one? It looks overwhelmingly complex. But its base is the humble transistor and Boolean logic. To understand something complex, we need to understand the simpler constituents.\n\n\n\nMost people are familiar with neural networks, cost functions, gradient descent, and backpropagation. However, beyond these building blocks is the magic of representations.\nFeatures live in a multidimensional universe where the concept of a hyperplane classifies or clusters similar features together.\nThis idea applies equally to the simplest neural networks and to modern architectures such as Transformers.\nOne of the earliest neural networks, Rosenblatt’s Perceptron, introduced the idea of representing inputs as vectors and using the dot product to define a decision boundary — a hyperplane that separates input feature vectors.\nFirst a short refresher.\n\n\nA vector is an object that has both a magnitude and a direction. Example Force and Velocity. Both have magnitude as well as direction.\nHowever we need to specify also a context where this vector lives -Vector Space. For example when we are thinking about something like Force vector, the context is usually 2D or 3D Euclidean world.\n\n\n\n2Dvector\n\n\n\n\n\n3Dvector\n\n\n(Source: 3Blue1Brown)\nThe easiest way to understand the Vector is in such a geometric context, say 2D or 3D cartesian coordinates, and then extrapolate it for other Vector spaces which we encounter but cannot really imagine.\n\n\n\nVectors are represented as matrices. A Vector is a one dimensional matrix.A matrix is defined to be a rectangular array of numbers. Example here is a Euclidean Vector in three-dimensional Euclidean space (or \\(R^{3}\\)) with some magnitude and direction (from (0,0,0) origin in this case).\nA vector is represented either as column matrix (m1)or as a row matrix (1m).\n\\[\na = \\begin{bmatrix}\na_{1}\\\\a_{2}\\\\a_{3}\\\n\\end{bmatrix} = \\begin{bmatrix} a_{1} & a_{2} &a_{3}\\end{bmatrix}\n\\]\n\\(a_{1},a_{2},a_{3}\\) are the component scalars of the vector. A vector is represented as \\(\\vec a\\) in the Vector notation and as \\(a_{i}\\) in the Index Notation.\n\n\n\nThis intuition is especially helpful when we use dot products on neural network weight matrices.\n\n\n\nThis is a very important concept in linear algebra and is used in many places in machine learning.\nAlgebraically, the dot product is the sum of the products of the corresponding entries of the two sequences of numbers.\nif \\(\\vec a = \\left\\langle {a_1,a_2,a_3} \\right\\rangle\\) and \\(\\vec b = \\left\\langle {b_1,b_2,b_3} \\right\\rangle\\), then\n\\(\\vec a \\cdot \\vec b = {a_1}{b_1} + {a_2}{b_2} + {a_3}{b_3} = a_ib_i \\quad\\text {in index notation}\\)\nGeometrically, it is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them\n\\[\n\\vec a \\cdot \\vec b = \\left\\| {\\vec a} \\right\\|\\,\\,\\left\\| {\\vec b} \\right\\|\\cos \\theta\n\\]\n\n\n\ndotproduct\n\n\nNote- These definitions are equivalent when using Cartesian coordinates (Ref 8, 9)\n\n\n\n\nIf two vectors point in roughly the same direction, their dot product is positive. If they point in opposite directions, the dot product is negative.\nThis simple geometric fact becomes a powerful computational tool.\nImagine a problem where we want to classify whether a leaf is healthy or diseased based on certain features. Each leaf is represented as a feature vector in a two-dimensional space (for simplicity).\nIf we can find a weight vector such that:\nIts dot product with healthy leaf vectors is positive\nIts dot product with diseased leaf vectors is negative\nthen that weight vector defines a hyperplane that splits the feature space into two regions.\nThis is exactly how the Perceptron performs classification.\n\n\n\nhyperplane1\n\n\nImagine we have a problem of classifying if a leaf is healthy or not based on certain features of the leaf. For each leaf we have some feature vector set assume it is a 2D vector space with say color as the feature for simplicity.\nFor any input feature vector in that vector space, if we have a weight vector, whose dot product with one feature vector of the set of input vectors of a certain class (say leaf is healthy) is positive, and with the other set is negative, then that weight vector is splitting the feature vector hyper-plane into two.\nOr in a better way, which shows the vectors properly\n\n\n\nweightvector\n\n\nIn essence, we are using the weight vectors to split the hyper-plane into two distinctive sets.\nFor any new leaf, if we only extract the same features into a feature vector; we can dot product it with the trained weight vector and find out if it falls in healthy or deceased class.\nHere is a Colab notebook to play around with this.14\n\n\nImagine the weight vector \\(w\\) as a pointer. We want this pointer to be oriented such that: 1. It points generally in the same direction as Positive examples. 2. It points away from Negative examples.\nWe start with a random weight vector. Then, we iterate through our training data and check how the current \\(w\\) classifies each point.\n\nIf the classification is correct: We do nothing. The weight vector is already doing its job for this point.\nIf the classification is wrong: We need to “nudge” or rotate the weight vector to correct the error.\n\n\n\n\nLet’s say we have an input vector \\(x\\).\nCase 1: False Negative The input \\(x\\) is a Positive example (\\(y=1\\)), but our current \\(w\\) classified it as negative (dot product \\(w \\cdot x &lt; 0\\)). * Action: We need to rotate \\(w\\) towards \\(x\\). * Update: \\(w_{new} = w_{old} + x\\) * Result: Adding \\(x\\) to \\(w\\) makes the new vector more aligned with \\(x\\), increasing the dot product for the next time.\nCase 2: False Positive The input \\(x\\) is a Negative example (\\(y=0\\) or \\(-1\\)), but our current \\(w\\) classified it as positive (dot product \\(w \\cdot x &gt; 0\\)). * Action: We need to rotate \\(w\\) away from \\(x\\). * Update: \\(w_{new} = w_{old} - x\\) * Result: Subtracting \\(x\\) from \\(w\\) pushes it in the opposite direction, decreasing the dot product.\nWe can combine these rules into a single update equation. We often introduce a learning rate \\(\\eta\\) (a small number like 0.1) to make the updates smoother, preventing the weight vector from jumping around too wildly.\nFor each training example \\((x, y_{target})\\): 1. Compute prediction: \\(\\hat{y} = \\text{step\\_function}(w \\cdot x)\\) 2. Calculate error: \\(error = y_{target} - \\hat{y}\\) 3. Update weights: \\[ w = w + \\eta \\cdot error \\cdot x \\]\nThis is known as the Perceptron Learning Rule.\nNot all problems have their feature set which is linearly seperable. So this is a constraint of this system. For example XOR problem.\n\n\n\n\nThe fact that Perceptron could not be used for XOR or XNOR; which was demonstrated in 1969, by by Marvin Minsky and Seymour Papert led to the first AI winter, as much of the hype generated intially by Frank Rosenblatt’s discovery became a disillusionment.\n\n\n\nlinearseperable\n\n\n# Summary\nWhat we have seen so far is that we can represent real world features as vectors residing in some N dimensional space.\nWe can then use the concept of hyperplane to split the feature space into two distinctive sets.\nThis is the magic of Representation\nNext Perceptron Training",
    "crumbs": [
      "Vectors & Perceptron"
    ]
  },
  {
    "objectID": "1_vectors_dot_product_and_perceptron.html#the-magic-of-representation---vector-space-and-hyperplane",
    "href": "1_vectors_dot_product_and_perceptron.html#the-magic-of-representation---vector-space-and-hyperplane",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "",
    "text": "Most people are familiar with neural networks, cost functions, gradient descent, and backpropagation. However, beyond these building blocks is the magic of representations.\nFeatures live in a multidimensional universe where the concept of a hyperplane classifies or clusters similar features together.\nThis idea applies equally to the simplest neural networks and to modern architectures such as Transformers.\nOne of the earliest neural networks, Rosenblatt’s Perceptron, introduced the idea of representing inputs as vectors and using the dot product to define a decision boundary — a hyperplane that separates input feature vectors.\nFirst a short refresher.\n\n\nA vector is an object that has both a magnitude and a direction. Example Force and Velocity. Both have magnitude as well as direction.\nHowever we need to specify also a context where this vector lives -Vector Space. For example when we are thinking about something like Force vector, the context is usually 2D or 3D Euclidean world.\n\n\n\n2Dvector\n\n\n\n\n\n3Dvector\n\n\n(Source: 3Blue1Brown)\nThe easiest way to understand the Vector is in such a geometric context, say 2D or 3D cartesian coordinates, and then extrapolate it for other Vector spaces which we encounter but cannot really imagine.\n\n\n\nVectors are represented as matrices. A Vector is a one dimensional matrix.A matrix is defined to be a rectangular array of numbers. Example here is a Euclidean Vector in three-dimensional Euclidean space (or \\(R^{3}\\)) with some magnitude and direction (from (0,0,0) origin in this case).\nA vector is represented either as column matrix (m1)or as a row matrix (1m).\n\\[\na = \\begin{bmatrix}\na_{1}\\\\a_{2}\\\\a_{3}\\\n\\end{bmatrix} = \\begin{bmatrix} a_{1} & a_{2} &a_{3}\\end{bmatrix}\n\\]\n\\(a_{1},a_{2},a_{3}\\) are the component scalars of the vector. A vector is represented as \\(\\vec a\\) in the Vector notation and as \\(a_{i}\\) in the Index Notation.\n\n\n\nThis intuition is especially helpful when we use dot products on neural network weight matrices.\n\n\n\nThis is a very important concept in linear algebra and is used in many places in machine learning.\nAlgebraically, the dot product is the sum of the products of the corresponding entries of the two sequences of numbers.\nif \\(\\vec a = \\left\\langle {a_1,a_2,a_3} \\right\\rangle\\) and \\(\\vec b = \\left\\langle {b_1,b_2,b_3} \\right\\rangle\\), then\n\\(\\vec a \\cdot \\vec b = {a_1}{b_1} + {a_2}{b_2} + {a_3}{b_3} = a_ib_i \\quad\\text {in index notation}\\)\nGeometrically, it is the product of the Euclidean magnitudes of the two vectors and the cosine of the angle between them\n\\[\n\\vec a \\cdot \\vec b = \\left\\| {\\vec a} \\right\\|\\,\\,\\left\\| {\\vec b} \\right\\|\\cos \\theta\n\\]\n\n\n\ndotproduct\n\n\nNote- These definitions are equivalent when using Cartesian coordinates (Ref 8, 9)",
    "crumbs": [
      "Vectors & Perceptron"
    ]
  },
  {
    "objectID": "1_vectors_dot_product_and_perceptron.html#dot-product-for-checking-vector-alignment",
    "href": "1_vectors_dot_product_and_perceptron.html#dot-product-for-checking-vector-alignment",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "",
    "text": "If two vectors point in roughly the same direction, their dot product is positive. If they point in opposite directions, the dot product is negative.\nThis simple geometric fact becomes a powerful computational tool.\nImagine a problem where we want to classify whether a leaf is healthy or diseased based on certain features. Each leaf is represented as a feature vector in a two-dimensional space (for simplicity).\nIf we can find a weight vector such that:\nIts dot product with healthy leaf vectors is positive\nIts dot product with diseased leaf vectors is negative\nthen that weight vector defines a hyperplane that splits the feature space into two regions.\nThis is exactly how the Perceptron performs classification.\n\n\n\nhyperplane1\n\n\nImagine we have a problem of classifying if a leaf is healthy or not based on certain features of the leaf. For each leaf we have some feature vector set assume it is a 2D vector space with say color as the feature for simplicity.\nFor any input feature vector in that vector space, if we have a weight vector, whose dot product with one feature vector of the set of input vectors of a certain class (say leaf is healthy) is positive, and with the other set is negative, then that weight vector is splitting the feature vector hyper-plane into two.\nOr in a better way, which shows the vectors properly\n\n\n\nweightvector\n\n\nIn essence, we are using the weight vectors to split the hyper-plane into two distinctive sets.\nFor any new leaf, if we only extract the same features into a feature vector; we can dot product it with the trained weight vector and find out if it falls in healthy or deceased class.\nHere is a Colab notebook to play around with this.14\n\n\nImagine the weight vector \\(w\\) as a pointer. We want this pointer to be oriented such that: 1. It points generally in the same direction as Positive examples. 2. It points away from Negative examples.\nWe start with a random weight vector. Then, we iterate through our training data and check how the current \\(w\\) classifies each point.\n\nIf the classification is correct: We do nothing. The weight vector is already doing its job for this point.\nIf the classification is wrong: We need to “nudge” or rotate the weight vector to correct the error.\n\n\n\n\nLet’s say we have an input vector \\(x\\).\nCase 1: False Negative The input \\(x\\) is a Positive example (\\(y=1\\)), but our current \\(w\\) classified it as negative (dot product \\(w \\cdot x &lt; 0\\)). * Action: We need to rotate \\(w\\) towards \\(x\\). * Update: \\(w_{new} = w_{old} + x\\) * Result: Adding \\(x\\) to \\(w\\) makes the new vector more aligned with \\(x\\), increasing the dot product for the next time.\nCase 2: False Positive The input \\(x\\) is a Negative example (\\(y=0\\) or \\(-1\\)), but our current \\(w\\) classified it as positive (dot product \\(w \\cdot x &gt; 0\\)). * Action: We need to rotate \\(w\\) away from \\(x\\). * Update: \\(w_{new} = w_{old} - x\\) * Result: Subtracting \\(x\\) from \\(w\\) pushes it in the opposite direction, decreasing the dot product.\nWe can combine these rules into a single update equation. We often introduce a learning rate \\(\\eta\\) (a small number like 0.1) to make the updates smoother, preventing the weight vector from jumping around too wildly.\nFor each training example \\((x, y_{target})\\): 1. Compute prediction: \\(\\hat{y} = \\text{step\\_function}(w \\cdot x)\\) 2. Calculate error: \\(error = y_{target} - \\hat{y}\\) 3. Update weights: \\[ w = w + \\eta \\cdot error \\cdot x \\]\nThis is known as the Perceptron Learning Rule.\nNot all problems have their feature set which is linearly seperable. So this is a constraint of this system. For example XOR problem.",
    "crumbs": [
      "Vectors & Perceptron"
    ]
  },
  {
    "objectID": "1_vectors_dot_product_and_perceptron.html#simple-hyperplane-split-is-not-possible-for-non-linearly-seperable-feature-set.",
    "href": "1_vectors_dot_product_and_perceptron.html#simple-hyperplane-split-is-not-possible-for-non-linearly-seperable-feature-set.",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "",
    "text": "The fact that Perceptron could not be used for XOR or XNOR; which was demonstrated in 1969, by by Marvin Minsky and Seymour Papert led to the first AI winter, as much of the hype generated intially by Frank Rosenblatt’s discovery became a disillusionment.\n\n\n\nlinearseperable\n\n\n# Summary\nWhat we have seen so far is that we can represent real world features as vectors residing in some N dimensional space.\nWe can then use the concept of hyperplane to split the feature space into two distinctive sets.\nThis is the magic of Representation\nNext Perceptron Training",
    "crumbs": [
      "Vectors & Perceptron"
    ]
  },
  {
    "objectID": "6_neuralnetworkimpementation.html",
    "href": "6_neuralnetworkimpementation.html",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "",
    "text": "Alex Punnen\n© All Rights Reserved\n\nContents",
    "crumbs": [
      "Neural Network Implementation"
    ]
  },
  {
    "objectID": "6_neuralnetworkimpementation.html#a-simple-neuralnet-with-back-propagation",
    "href": "6_neuralnetworkimpementation.html#a-simple-neuralnet-with-back-propagation",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "A Simple NeuralNet with Back Propagation",
    "text": "A Simple NeuralNet with Back Propagation\nWith the derivative of the Cost function dervied from the last chapter, we can code the network\nWe will use matrices to represent input and weight matrices.\nx = np.array(\n    [\n        [0,0,1],\n        [0,1,1],\n        [1,0,1],\n        [1,1,1]\n    ])\nThis is a 4*3 matrix. Note that each row is an input. lets take all this 4 as ‘training set’\ny = np.array(\n  [\n      [0],\n      [1],\n      [0],\n      [1]\n  ])\nNote you can change the output and try to train the Neural network\nThis is a 4*1 matrix that represent the expected output. That is for input [0,0,1] the output is [0] and for [0,1,1] the output is [1] etc.\nA neural network is implemented as a set of matrices representing the weights of the network.\nLet’s create a two layered network. Before that please not the formula for the neural network\nSo basically the output at layer l is the dot product of the weight matrix of layer l and input of the previous layer.\nNow let’s see how the matrix dot product works based on the shape of matrices.\n[m*n].[n*x] = [m*x]\n[m*x].[x*y] = [m*y]\nWe take the \\([m*n]\\) as the input matrix this is a \\([4*3]\\) matrix.\nSimilarly the output \\(y\\) is a \\([4*1]\\) matrix; so we have \\([m*y] =[4*1]\\)\nSo we have\nm=4\nn=3\nx=?\ny=1\nLets then create our two weight matrices of the above shapes, that represent the two layers of the neural network.\nw0 = x\nw1 = np.random.random((3,4))\nw2 = np.random.random((4,1))\nWe can have an array of the weights to loop through, but for the time being let’s hard-code these. Note that ‘np’ stands for the popular numpy array library in Python.\nWe also need to code in our non linearity.We will use the Sigmoid function here.\ndef sigmoid(x):\n    return 1/(1+np.exp(-x))\n\n# derivative of the sigmoid\ndef derv_sigmoid(x):\n   return sigmoid(x)*(1-sigmoid(x))\nWith this we can have the output of first, second and third layer, using our equation of neural network forward propagation.\na0 = x\na1 = sigmoid(np.dot(a0,w1))\n\na2 = sigmoid(np.dot(a1,w2))\na2 is the calculated output from randomly initialized weights. So lets calculate the error by subtracting this from the expected value and taking the MSE.\n\\[\nC = \\frac{1}{2} \\|y-a^l\\|^2\n\\]\nc0 = ((y-a2)**2)/2\nNow we need to use the back-propagation algorithm to calculate how each weight has influenced the error and reduce it proportionally.\n\nWe use this to update weights in all the layers and do forward pass again, re-calculate the error and loss, then re-calculate the error gradient \\(\\frac{\\partial C}{\\partial w}\\) and repeat\n$$\n\\[\\begin{aligned}\n\nw^2 = w^2 - (\\frac {\\partial C}{\\partial w^2} )*learningRate \\\\ \\\\\n\nw^1 = w^1 - (\\frac {\\partial C}{\\partial w^1} )*learningRate\n\n\\end{aligned}\\]\n$$\nLet’s update the weights as per the formula (3) and (5) from last chapter\n\\[  \\mathbf{\n\\frac {\\partial C}{\\partial w^1} = \\sigma'(z^1) * (a^{0})^T*\\delta^{2}*w^2.\\sigma'(z^2) \\quad \\rightarrow \\mathbb Eq \\; (5)\n}\\]\n\\[\n\\delta^2 = (a^2-y)\n\\]\n\\[ \\mathbf{\n\\frac {\\partial C}{\\partial w^2}= \\delta^{2}*\\sigma^{'}(z^2) * (a^{1})^T \\quad \\rightarrow \\mathbb Eq \\; (3)\n}\n\\]",
    "crumbs": [
      "Neural Network Implementation"
    ]
  },
  {
    "objectID": "6_neuralnetworkimpementation.html#a-two-layered-neural-network-in-python",
    "href": "6_neuralnetworkimpementation.html#a-two-layered-neural-network-in-python",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "A Two layered Neural Network in Python",
    "text": "A Two layered Neural Network in Python\nBelow is a two layered Network; I have used the code from http://iamtrask.github.io/2015/07/12/basic-python-network/ as the basis. With minor changes to fit into how we derived the equations.\nimport numpy as np\n# seed random numbers to make calculation deterministic \nnp.random.seed(1)\n\n# pretty print numpy array\nnp.set_printoptions(formatter={'float': '{: 0.3f}'.format})\n\n# let us code our sigmoid funciton\ndef sigmoid(x):\n    return 1/(1+np.exp(-x))\n\n# let us add a method that takes the derivative of x as well\ndef derv_sigmoid(x):\n   return sigmoid(x)*(1-sigmoid(x))\n\n#---------------------------------------------------------------\n\n# Two layered NW. Using from (1) and the equations we derived as explanaionns\n# (1) http://iamtrask.github.io/2015/07/12/basic-python-network/\n#---------------------------------------------------------------\n\n# set learning rate as 1 for this toy example\nlearningRate = 1\n\n# input x, also used as the training set here\nx = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n\n# desired output for each of the training set above\ny = np.array([[0,1,1,0]]).T\n\n# Explanaiton - as long as input has two ones, but not three, ouput is One\n\"\"\"\nInput [0,0,1]  Output = 0\nInput [0,1,1]  Output = 1\nInput [1,0,1]  Output = 1\nInput [1,1,1]  Output = 0\n\"\"\"\n\n# Randomly initalised weights\nweight1 =  np.random.random((3,4)) \nweight2 =  np.random.random((4,1)) \n\n# Activation to layer 0 is taken as input x\na0 = x\n\niterations = 1000\nfor iter in range(0,iterations):\n\n  # Forward pass - Straight Forward\n  z1= np.dot(x,weight1)\n  a1 = sigmoid(z1) \n  z2= np.dot(a1,weight2)\n  a2 = sigmoid(z2) \n  if iter == 0:\n    print(\"Intial Ouput \\n\",a2)\n\n  # Backward Pass - Backpropagation \n  delta2  = (a2-y)\n  #---------------------------------------------------------------\n  # Calcluating change of Cost/Loss wrto weight of 2nd/last layer\n  # Eq (A) ---&gt; dC_dw2 = delta2*derv_sigmoid(z2)*a1.T\n  #---------------------------------------------------------------\n\n  dC_dw2_1  = delta2*derv_sigmoid(z2) \n  dC_dw2  = a1.T.dot(dC_dw2_1)\n  \n  #---------------------------------------------------------------\n  # Calcluating change of Cost/Loss wrto weight of 2nd/last layer\n  # Eq (B)---&gt; dC_dw1 = derv_sigmoid(z1)*delta2*derv_sigmoid(z2)*weight2*a0.T\n  # dC_dw1 = derv_sigmoid(z1)*dC_dw2*weight2_1*a0.T\n  #---------------------------------------------------------------\n\n  dC_dw1 =  np.multiply(dC_dw2_1,weight2.T) * derv_sigmoid(z1)\n  # todo - the weight2.T is the only thing not in equation here\n  dC_dw1 = a0.T.dot(dC_dw1)\n\n  #---------------------------------------------------------------\n  #Gradinent descent\n  #---------------------------------------------------------------\n \n  weight2 = weight2 - learningRate*(dC_dw2)\n  weight1 = weight1 - learningRate*(dC_dw1)\n\n\nprint(\"New ouput\",a2)\n\n#---------------------------------------------------------------\n# Training is done, weight2 and weight2 are primed for output y\n#---------------------------------------------------------------\n\n# Lets test out, two ones in input and one zero, ouput should be One\nx = np.array([[1,0,1]])\nz1= np.dot(x,weight1)\na1 = sigmoid(z1) \nz2= np.dot(a1,weight2)\na2 = sigmoid(z2) \nprint(\"Ouput after Training is \\n\",a2)\nOutput\nIntial Ouput \n [[ 0.758]\n [ 0.771]\n [ 0.791]\n [ 0.801]]\nNew ouput [[ 0.028]\n [ 0.925]\n [ 0.925]\n [ 0.090]]\nOuput after Training is \n [[ 0.925]]\nWe have trained the NW for getting the output similar to \\(y\\); that is [0,1,0,1]\nThe code in Colab\nNext: Back Propagation Pass 3 (Matrix Calculus)",
    "crumbs": [
      "Neural Network Implementation"
    ]
  },
  {
    "objectID": "2_perceptron_training.html",
    "href": "2_perceptron_training.html",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "",
    "text": "Alex Punnen\n© All Rights Reserved\n\nContents",
    "crumbs": [
      "Perceptron Training"
    ]
  },
  {
    "objectID": "2_perceptron_training.html#perceptron-training-via-feature-vectors-hyperplane-split",
    "href": "2_perceptron_training.html#perceptron-training-via-feature-vectors-hyperplane-split",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "Perceptron Training via Feature Vectors & HyperPlane split",
    "text": "Perceptron Training via Feature Vectors & HyperPlane split\nLet’s follow from the previous chapter of the Perceptron neural network.\nWe have seen how the concept of splitting the hyper-plane of feature set separates one type of feature vectors from other.\n![cornellperceptron][6]",
    "crumbs": [
      "Perceptron Training"
    ]
  },
  {
    "objectID": "2_perceptron_training.html#how-are-the-weights-learned",
    "href": "2_perceptron_training.html#how-are-the-weights-learned",
    "title": "The Mathematical Intuition Behind Deep Learning",
    "section": "How are the weights learned?",
    "text": "How are the weights learned?\nYou may have heard about Gradient Descent, which is the backbone of training modern neural networks. However, for the classic Perceptron, the learning algorithm is much simpler and relies on a geometric intuition.\nThe goal is to find a weight vector \\(w\\) that defines a hyperplane separating the two classes of data (e.g., Positive and Negative).\nNote this term hyperplane is used in the context of feature vector space and is used throughout neural network learning.\n\nThe Intuition: Nudging the Vector\nImagine the weight vector \\(w\\) as a pointer. We want this pointer to be oriented such that: 1. It points generally in the same direction as Positive examples. 2. It points away from Negative examples.\nWe start with a random weight vector. Then, we iterate through our training data and check how the current \\(w\\) classifies each point.\n\nIf the classification is correct: We do nothing. The weight vector is already doing its job for this point.\nIf the classification is wrong: We need to “nudge” or rotate the weight vector to correct the error.\n\n\n\nThe Update Rules\nLet’s say we have an input vector \\(x\\).\nCase 1: False Negative The input \\(x\\) is a Positive example (\\(y=1\\)), but our current \\(w\\) classified it as negative (dot product \\(w \\cdot x &lt; 0\\)). * Action: We need to rotate \\(w\\) towards \\(x\\). * Update: \\(w_{new} = w_{old} + x\\) * Result: Adding \\(x\\) to \\(w\\) makes the new vector more aligned with \\(x\\), increasing the dot product for the next time.\nCase 2: False Positive The input \\(x\\) is a Negative example (\\(y=0\\) or \\(-1\\)), but our current \\(w\\) classified it as positive (dot product \\(w \\cdot x &gt; 0\\)). * Action: We need to rotate \\(w\\) away from \\(x\\). * Update: \\(w_{new} = w_{old} - x\\) * Result: Subtracting \\(x\\) from \\(w\\) pushes it in the opposite direction, decreasing the dot product.\n\n\nThe Formal Algorithm\nWe can combine these rules into a single update equation. We often introduce a learning rate \\(\\eta\\) (a small number like 0.1) to make the updates smoother, preventing the weight vector from jumping around too wildly.\nFor each training example \\((x, y_{target})\\): 1. Compute prediction: \\(\\hat{y} = \\text{step\\_function}(w \\cdot x)\\) 2. Calculate error: \\(error = y_{target} - \\hat{y}\\) 3. Update weights: \\[ w = w + \\eta \\cdot error \\cdot x \\]\nThis is known as the Perceptron Learning Rule.\n\\[\n\\Delta w_j = \\eta (y_{target} - \\text{prediction}) x_j\n\\]\n\nNote: This is distinct from Gradient Descent. Gradient Descent requires a differentiable activation function to compute gradients (slope). The Perceptron uses a “step function” (hard 0 or 1) which is not differentiable. However, this simple rule is guaranteed to converge if the data is linearly separable.\n\nA more rigorous explanation of the proof can be found in the book Neural Networks by R.Rojas or this article.\nNext: Gradient Descent and Optimization",
    "crumbs": [
      "Perceptron Training"
    ]
  }
]