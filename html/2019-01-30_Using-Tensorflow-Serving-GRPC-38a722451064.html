<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Using Tensorflow Serving GRPC</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Using Tensorflow Serving GRPC</h1>
</header>
<section data-field="subtitle" class="p-summary">
Once you have your Tensorflow or Keras based model trained, one needs to think on how to use it in production. You may want to Dockerize…
</section>
<section data-field="body" class="e-content">
<section name="e783" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="b844" id="b844" class="graf graf--h3 graf--leading graf--title">TF Serving -Auto Wrap your TF or Keras model &amp; Deploy it with a production-grade GRPC Interface</h3><h4 name="a119" id="a119" class="graf graf--h4 graf-after--h3 graf--subtitle">Plus-How to write a GRPC Client for the wrapped model</h4><p name="07f4" id="07f4" class="graf graf--p graf-after--h4">Once you have your Tensorflow or Keras based model trained, one needs to think on how to use it in, deploy it in production. You may want to Dockerize it as a micro-service, implementing a custom GRPC (or REST- <a href="https://hackernoon.com/rest-in-peace-grpc-for-micro-service-and-grpc-for-the-web-a-how-to-908cc05e1083" data-href="https://hackernoon.com/rest-in-peace-grpc-for-micro-service-and-grpc-for-the-web-a-how-to-908cc05e1083" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">or not</a>) interface. Then deploy this to server or Kubernetes cluster and have other client micro-services calling it. <a href="https://www.tensorflow.org/serving/" data-href="https://www.tensorflow.org/serving/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">Google Tensorflow Serving</strong></a><strong class="markup--strong markup--p-strong"> </strong>library helps here, to save your model to disk, and then load and serve a GRPC or RESTful interface to interact with it.</p><figure name="7b75" id="7b75" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 390px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 55.7%;"></div><img class="graf-image" data-image-id="0*-LMpEp7g_s9_plHF.jpg" data-width="1200" data-height="668" src="https://cdn-images-1.medium.com/max/800/0*-LMpEp7g_s9_plHF.jpg"></div><figcaption class="imageCaption"><a href="https://twitter.com/tensorflow/status/832008382408126464" data-href="https://twitter.com/tensorflow/status/832008382408126464" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">source</a></figcaption></figure><p name="4fa0" id="4fa0" class="graf graf--p graf-after--figure">I have been trying to use this and found the tutorials/ examples bit complex, especially if you are using a custom model, and so here is a simpler one. I am still not sure of the use case of TF Serving part, as it is not super difficult to have your own GRPC wrapper over your model. But TF Serving is a promising lot more. I guess will know it in time. For now, here we focus on saving a simple model and getting it working via a GRPC client.</p><p name="460b" id="460b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Step 1. Saving the model in TF Serving compatible way</strong></p><p name="2ee1" id="2ee1" class="graf graf--p graf-after--p">Let us have a simple model. I used the simplest MNIST image classifier, model. See this in Colab hosted notebook -<a href="https://colab.research.google.com/drive/1ioqL7hD-mrruOlAK-d2axenKVTHrU8gQ" data-href="https://colab.research.google.com/drive/1ioqL7hD-mrruOlAK-d2axenKVTHrU8gQ" class="markup--anchor markup--p-anchor" rel="nofollow noopener noopener noopener" target="_blank">https://colab.research.google.com/drive/1ioqL7hD-mrruOlAK-d2axenKVTHrU8gQ</a> (please ignore the many debug prints in the notebook). Once the NW is trained we need to save the weights.</p><pre name="ca0f" id="ca0f" class="graf graf--pre graf-after--p"># No need to train if weights are already there<br>from pathlib import Path<br>from keras.models import load_model<br>weightfile =&#39;/content/gdrive/My Drive/Colab Notebooks<strong class="markup--strong markup--pre-strong">/my_model_20.h5&#39;</strong><br>my_file = Path(weightfile)<br>if my_file.is_file():<br>  print(&quot;Weight file exisits&quot;)<br>  <strong class="markup--strong markup--pre-strong">model = load_model(weightfile) </strong><br>else:<br>  print(&quot;Weight file do not exist&quot;)<br>  # train the model and save it<br>  history =model.fit(X_train, Y_train,batch_size=32, epochs=20, <br>                   callbacks=[plot],<br>                   validation_data=(X_test, Y_test),<br>                   verbose=1)<br>  <strong class="markup--strong markup--pre-strong">model.save(weightfile)</strong></pre><p name="14d8" id="14d8" class="graf graf--p graf-after--pre">Note in the above snippet how the weight file is loaded /and or saved.</p><p name="33dc" id="33dc" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">What if we want to save the model and load the model so that we can get a REST or GRPC interface to it. This is what TF Serving does.</strong></p><p name="0e0e" id="0e0e" class="graf graf--p graf-after--p">First we can save it in a way that is compliant to TF Serving. Since I am using Google Colab, I am saving the model to my Google Drive, you can store it to disk or anywhere else.</p><pre name="9100" id="9100" class="graf graf--pre graf-after--p">import shutil <br>import os<br>tf.keras.backend.clear_session()<br># The export path contains the name and the version of the model<br>tf.keras.backend.set_learning_phase(0) # Ignore dropout at inference<br>weightfile =&#39;/content/gdrive/My Drive/Colab Notebooks/my_model_20.h5&#39;<br>export_path = &#39;/content/gdrive/My Drive/Colab Notebooks/ServingModel20&#39;</pre><pre name="8dcf" id="8dcf" class="graf graf--pre graf-after--pre">if os.path.exists(<strong class="markup--strong markup--pre-strong"> export_path+&quot;/1&quot;</strong>):<br>    shutil.rmtree( export_path+&quot;/1&quot;)<br>    <br>export_path = export_path+&quot;/1&quot;</pre><pre name="6675" id="6675" class="graf graf--pre graf-after--pre"># Fetch the Keras session and save the model<br>with tf.keras.backend.get_session() as sess:<br>    model = load_model(weightfile)<br>   <strong class="markup--strong markup--pre-strong"> tf.saved_model.simple_save</strong>(<br>        sess,<br>        <strong class="markup--strong markup--pre-strong">export_path,</strong><br>        inputs={&#39;input_image&#39;: model.input},<br>        outputs={t.name:t for t in model.outputs})</pre><p name="7091" id="7091" class="graf graf--p graf-after--pre">If successful the output would be like</p><pre name="417e" id="417e" class="graf graf--pre graf-after--p">WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/simple_save.py:85: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.<br>Instructions for updating:<br>Pass your op to the equivalent parameter main_op instead.<br>INFO:tensorflow:Assets added to graph.<br>INFO:tensorflow:No assets to write.<br>INFO:tensorflow:SavedModel written to: <strong class="markup--strong markup--pre-strong">/content/gdrive/My Drive/Colab Notebooks/ServingModel20/1/saved_model.pb</strong></pre><p name="8928" id="8928" class="graf graf--p graf-after--pre">Please note the convention of the folder path followed in the above snippet- ServingModel20/1 , where 1 stands for the first version of the model. TF Serving will watch the root directory and auto-deploy the later versions — that is one of its features. So after you have somehow done you ML CI (that is a post in itself), you can deploy your model to ServingModel20/2 — and presto the TF Serving container auto-loads the new model.</p><p name="be65" id="be65" class="graf graf--p graf-after--p">Now let us check if we can load the model with TF Serving. ( Note that model versions are stored by convention in subdirectories /1, /2 etc, which may be useful for model versioning).</p><pre name="cf98" id="cf98" class="graf graf--pre graf-after--p">!<strong class="markup--strong markup--pre-strong">saved_model_cli</strong> show --dir &#39;/content/gdrive/My Drive/Colab Notebooks/ServingModel20/1&#39; --all</pre><p name="2c06" id="2c06" class="graf graf--p graf-after--pre">You would get the model parameters if loading is successful as</p><pre name="5789" id="5789" class="graf graf--pre graf-after--p">MetaGraphDef with tag-set: &#39;serve&#39; contains the following SignatureDefs:<br><br><strong class="markup--strong markup--pre-strong">signature_def[&#39;serving_default&#39;]:</strong><br>  The given SavedModel SignatureDef contains the following input(s):<br>    <strong class="markup--strong markup--pre-strong">inputs</strong>[&#39;<strong class="markup--strong markup--pre-strong">input_image</strong>&#39;] tensor_info:<br>        dtype: DT_FLOAT<br>       <strong class="markup--strong markup--pre-strong"> shape: (-1, 1, 28, 28)</strong><br>        name: conv2d_1_input:0<br>  The given SavedModel SignatureDef contains the following output(s):<br>    <strong class="markup--strong markup--pre-strong">outputs</strong>[<strong class="markup--strong markup--pre-strong">&#39;dense_2/Softmax:0</strong>&#39;] tensor_info:<br>        dtype: DT_FLOAT<br>        <strong class="markup--strong markup--pre-strong">shape: (-1, 10</strong>)<br>        name: dense_2/Softmax:0<br>  Method name is: tensorflow/serving/predict</pre><p name="7f35" id="7f35" class="graf graf--p graf-after--pre">Note the bold highlights, as these are how the input and output interfaces for GRPC or REST once the model is served.</p><p name="6d80" id="6d80" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Step 2. Loading the saved model and Serving it with TF Serve</strong></p><p name="75e5" id="75e5" class="graf graf--p graf-after--p">I downloaded the model to my home folder and used <strong class="markup--strong markup--p-strong">tensorflow/serving:latest-gpu </strong>docker image from TF Serving team to load the model -<em class="markup--em markup--p-em">ServingModel20</em> from the volume mapped folder. (Note my laptop has a GPU, so I am using the Docker image with GPU tag (note the port mapping 8900 in local machine is GRPC and 8901 is REST interface)</p><pre name="a69b" id="a69b" class="graf graf--pre graf-after--p">TFServing server</pre><pre name="31e3" id="31e3" class="graf graf--pre graf-after--pre">sudo docker run  --runtime= -t --rm -p 8900:8500 -p 8901:8501 -v /home/alex/coding/IPython_neuralnet/<strong class="markup--strong markup--pre-strong">ServingModel20</strong>:/models/mnist -e MODEL_NAME=mnist <strong class="markup--strong markup--pre-strong">tensorflow/serving:latest-gpu</strong></pre><p name="3958" id="3958" class="graf graf--p graf-after--pre">If successful you would get output like below</p><pre name="5ebf" id="5ebf" class="graf graf--pre graf-after--p">2019-01-28 10:37:47.192669: I tensorflow_serving/model_servers/server.cc:286] Running gRPC ModelServer at 0.0.0.0:8500 ...<br>[warn] getaddrinfo: address family for nodename not supported<br>[evhttp_server.cc : 237] RAW: Entering the event loop ...<br>2019-01-28 10:37:47.194034: I tensorflow_serving/model_servers/server.cc:302] Exporting HTTP/REST API at:localhost:8501 .</pre><p name="e53b" id="e53b" class="graf graf--p graf-after--pre">Note that the REST is exposed in port 8501 and GRPC interface in port 8500 (port mapped to 8901 and 8900 in my machine),</p><blockquote name="0559" id="0559" class="graf graf--pullquote graf-after--p">That’s it! Only what&#39;s left is writing a GRPC Client</blockquote><p name="2a40" id="2a40" class="graf graf--p graf-after--pullquote"><strong class="markup--strong markup--p-strong">Step 3. Writing a GRPC Client.</strong></p><p name="56d0" id="56d0" class="graf graf--p graf-after--p">Though there is an example from TF Serving, took some time for me to figure things out in getting it to work. I have made it simpler for me for checking and here the python client.</p><p name="87f5" id="87f5" class="graf graf--p graf-after--p">Note the Input and Output handing and the model signature. Also the input reshaping parts. For other models, these would change.</p><pre name="a4ad" id="a4ad" class="graf graf--pre graf-after--p">channel = grpc.insecure_channel(hostport)                           stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)                           request = predict_pb2.PredictRequest()                           request.model_spec.name = &#39;<strong class="markup--strong markup--pre-strong">mnist</strong>&#39;                           request.model_spec.signature_name = &#39;<strong class="markup--strong markup--pre-strong">serving_default</strong>&#39;</pre><p name="c7aa" id="c7aa" class="graf graf--p graf-after--pre">Getting the output</p><pre name="46f1" id="46f1" class="graf graf--pre graf-after--p">result_future.result().outputs[<strong class="markup--strong markup--pre-strong">&#39;dense_2/Softmax:0&#39;</strong>].float_val)</pre><p name="14cf" id="14cf" class="graf graf--p graf-after--pre">These keys are are all in the model spec. <a href="https://gist.github.com/alexcpn/9a2d7c1d651235025624755681bcc2b2" data-href="https://gist.github.com/alexcpn/9a2d7c1d651235025624755681bcc2b2" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Here</a> is the code</p><figure name="1707" id="1707" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/alexcpn/9a2d7c1d651235025624755681bcc2b2.js"></script></figure><p name="aecc" id="aecc" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Step 4. Running the client</strong></p><p name="46f5" id="46f5" class="graf graf--p graf-after--p">Now we need to run the client. Easiest is to run in a docker container. I have used the same <strong class="markup--strong markup--p-strong">tensorflow_tfserving:latest-gpu </strong>Docker container to run this, but for easier MNIST handling and some python modules <strong class="markup--strong markup--p-strong">installed Keras</strong> and some other python libs and saved it as <strong class="markup--strong markup--p-strong">alexcpn/tfserving-dev-gpu.</strong> (I have saved mnist_client.py in /home/alex/coding in the snippet below)</p><pre name="a40b" id="a40b" class="graf graf--pre graf-after--p">TFServing Client GPRC</pre><pre name="7702" id="7702" class="graf graf--pre graf-after--pre">sudo docker run --runtime=nvidia  <strong class="markup--strong markup--pre-strong">--net=host</strong> -it -v /home/alex/coding:/coding --rm alexcpn/tfserving-dev-gpu bash</pre><p name="0c96" id="0c96" class="graf graf--p graf-after--pre">Now inside this container let us run the Python client and give the server ip where the TF Serving container is running</p><pre name="913c" id="913c" class="graf graf--pre graf-after--p">python mnist_client.py  --num_test=1000 --server=&lt;your server ip  ex 10.xx.xx.15&gt;:8901</pre><p name="45ae" id="45ae" class="graf graf--p graf-after--pre">If everything is successful you should get the output like below or similar</p><pre name="389c" id="389c" class="graf graf--pre graf-after--p">Using TensorFlow backend.<br>hello from TFServing client slim<br>Shape is  (28, 28)  Label is  7<br>Time to Send  20000  is  5.93926405907<br>[ 100 ] From Callback Predicted Result is  7 confidence=  1.0<br>[ 200 ] From Callback Predicted Result is  7 confidence=  1.0</pre><p name="0209" id="0209" class="graf graf--p graf-after--pre">Use nvidia-smi to check GPU Usage. Note that with REST the output should be slower as GRPC uses more efficient HTTP2</p><pre name="7267" id="7267" class="graf graf--pre graf-after--p">watch -n 0.5 nvidia-smi </pre><figure name="f232" id="f232" class="graf graf--figure graf-after--pre"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 352px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 50.3%;"></div><img class="graf-image" data-image-id="1*4ls2-f3QSzaVu4RFGxCW8A.png" data-width="950" data-height="478" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*4ls2-f3QSzaVu4RFGxCW8A.png"></div><figcaption class="imageCaption">nvidia-smi output</figcaption></figure><p name="be37" id="be37" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Somethings that needs a re-look</strong></p><p name="51e2" id="51e2" class="graf graf--p graf-after--p">Since the interface is giving the last layer output, you may need to add a micro-service to translate it to detected classes; in practice, this means that you may need another Micro service to wrap this and expose a more generic GRPC interface to clients.</p><p name="4415" id="4415" class="graf graf--p graf-after--p">Also, the input is a numpy array. So over normal networks for high definition images, this will add a lot of NW overhead. Though there is a slight overhead in computing resource and time to encode and send as jpeg or png and decode it at the receiver, this overhead may be compensated by the reduction in transfer speeds over sending raw numpy array over the network. So we need to see if the numpy array input format is really good for real-time detection-example from a video feed — when we use multiple nodes with GPU’s to scale horizontally.</p><p name="fa9f" id="fa9f" class="graf graf--p graf-after--p">Also, this is closely related to other projects like Kubeflow, See this excellent <a href="https://www.oliverwyman.com/content/dam/oliver-wyman/v2/events/2018/March/Google_London_Event/Public%20Introduction%20to%20Kubeflow.pdf" data-href="https://www.oliverwyman.com/content/dam/oliver-wyman/v2/events/2018/March/Google_London_Event/Public%20Introduction%20to%20Kubeflow.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">slide sets </a>from David Aronchick, Google PM and now head of ML Strategy at Microsoft. This describes how Kubeflow, based on Kubernetes and using TFServing, fits in the production pipeline.</p><p name="0189" id="0189" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Other links</strong></p><p name="45e6" id="45e6" class="graf graf--p graf-after--p">[1] Using TensorFlow Serving with Docker, <a href="https://www.tensorflow.org/serving/docker#gpu_serving_example" data-href="https://www.tensorflow.org/serving/docker#gpu_serving_example" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://www.tensorflow.org/serving/docker#gpu_serving_example</a> (Part of the offical document)</p><p name="f543" id="f543" class="graf graf--p graf-after--p">[2] Training and Serving ML models with tf.keras, <a href="https://medium.com/@sdcubber" data-href="https://medium.com/@sdcubber" class="markup--anchor markup--p-anchor" target="_blank">Stijn Decubber,</a> <a href="https://medium.com/tensorflow/training-and-serving-ml-models-with-tf-keras-fd975cc0fa27" data-href="https://medium.com/tensorflow/training-and-serving-ml-models-with-tf-keras-fd975cc0fa27" class="markup--anchor markup--p-anchor" target="_blank">https://medium.com/tensorflow/training-and-serving-ml-models-with-tf-keras-fd975cc0fa27</a> (Shows doing this via TFServing REST API)</p><p name="f8c7" id="f8c7" class="graf graf--p graf-after--p graf--trailing">[3] Optimizing TensorFlow Models for Serving, GCP, <a href="https://medium.com/@lramsey" data-href="https://medium.com/@lramsey" class="markup--anchor markup--p-anchor" target="_blank">Lukman Ramsey,</a> <a href="https://medium.com/google-cloud/optimizing-tensorflow-models-for-serving-959080e9ddbf" data-href="https://medium.com/google-cloud/optimizing-tensorflow-models-for-serving-959080e9ddbf" class="markup--anchor markup--p-anchor" target="_blank">https://medium.com/google-cloud/optimizing-tensorflow-models-for-serving-959080e9ddbf</a> (Describes why TF Serving is useful)</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@alexcpn" class="p-author h-card">Alex Punnen</a> on <a href="https://medium.com/p/38a722451064"><time class="dt-published" datetime="2019-01-30T08:19:40.230Z">January 30, 2019</time></a>.</p><p><a href="https://medium.com/@alexcpn/using-tensorflow-serving-grpc-38a722451064" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on August 22, 2019.</p></footer></article></body></html>