<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Optimizing any TensorFlow model using TensorFlow Transform Tools and using TensorRT</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Optimizing any TensorFlow model using TensorFlow Transform Tools and using TensorRT</h1>
</header>
<section data-field="subtitle" class="p-summary">
Model Optimization and reducing precision from FP32 to FP 16 for speedup and reducing graph size.
</section>
<section data-field="body" class="e-content">
<section name="4032" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="0a3c" id="0a3c" class="graf graf--h3 graf--leading graf--title">Optimizing any TensorFlow model using TensorFlow Transform Tools and using TensorRT</h3><h4 name="997f" id="997f" class="graf graf--h4 graf-after--h3 graf--subtitle">Model Optimization and reducing precision from FP32 to FP 16 for speedup and reducing graph size.</h4><h4 name="7b16" id="7b16" class="graf graf--h4 graf-after--h4">What is this article about? Whom is this meant for?</h4><p name="6f6a" id="6f6a" class="graf graf--p graf-after--h4">More than an article, this is basically <em class="markup--em markup--p-em">how to</em>, on optimizing a Tensorflow model, using TF Graph transformation tools and NVIDIA Tensor RT. This is a bit of a Heavy Reading and meant for Data Science Engineers than Data Scientist. Also, more than a how-to, it also illustrates the bugs and unexpected results I have encountered in doing this.</p><p name="8d4b" id="8d4b" class="graf graf--p graf-after--p">Also, these results may be unexpected to me in the role of a DS Engineer but may have a rational explanation if I understand or go deeper into the abstractions of the model or the TF framework that implements this model. I don’t have the skill yet and leave it to the reader to point out the mistakes I have made if any. I have raised these two bugs, one with TF Graph Transform and one with NVIDIA TensorRT transform tool, and it gets answered, then I will update this post</p><p name="b9fd" id="b9fd" class="graf graf--p graf-after--p"><a href="https://github.com/tensorflow/tensorflow/issues/28276" data-href="https://github.com/tensorflow/tensorflow/issues/28276" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://github.com/tensorflow/tensorflow/issues/28276</a></p><p name="d0f6" id="d0f6" class="graf graf--p graf-after--p"><a href="https://devtalk.nvidia.com/default/topic/1048485/tensorrt/no-speed-up-with-tensorrt-fp16-or-int8-on-nvidia-v100/post/5320989/#5320989" data-href="https://devtalk.nvidia.com/default/topic/1048485/tensorrt/no-speed-up-with-tensorrt-fp16-or-int8-on-nvidia-v100/post/5320989/#5320989" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://devtalk.nvidia.com/default/topic/1048485/tensorrt/no-speed-up-with-tensorrt-fp16-or-int8-on-nvidia-v100/post/5320989/#5320989</a></p><h4 name="9cc1" id="9cc1" class="graf graf--h4 graf-after--p">What was my expectation — More CUDA cores, TensorCores == ( I thought) Faster inference.</h4><p name="a4b9" id="a4b9" class="graf graf--p graf-after--h4">From NVIDIA TensorCore product description, I got the idea/ hope that if I can convert the FP 32 based Object detection models that we use in production to FP 16 or INT8 models and weights, then I will be able to run twice or four times as fast inference speeds; as advertised.</p><figure name="4cb4" id="4cb4" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 394px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 56.3%;"></div><img class="graf-image" data-image-id="1*4K4CPA7-Y1HYPMQKFoCi5Q.gif" data-width="736" data-height="414" src="https://cdn-images-1.medium.com/max/800/1*4K4CPA7-Y1HYPMQKFoCi5Q.gif"></div><figcaption class="imageCaption">source — <a href="https://www.nvidia.com/en-us/data-center/tesla-t4/" data-href="https://www.nvidia.com/en-us/data-center/tesla-t4/" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">https://www.nvidia.com/en-us/data-center/tesla-t4/</a></figcaption></figure><p name="8885" id="8885" class="graf graf--p graf-after--figure">We had one of the best GPU’s NVIDIA V100 32 GB for our experiments which supported all these modes- Tensor Cores, not to mention a set of other GPU’s server-grade as well as gaming grade, P40, GTX 1080, 1070 etc.</p><p name="7d43" id="7d43" class="graf graf--p graf-after--p">Before we started with these expensive GPU’s we used to run the model in GTX 1080 HP desktops. The first expectation was that the higher number of CUDA cores (~5k) in V100 will make our models run faster. I read blogs like below and knew that 1080 is pretty good and that NVIDIA prices and markets the server side GPU’s higher. ( In 1080 they throttle the FP16 which I guess is removed in 2080).</p><div name="5f5b" id="5f5b" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://lambdalabs.com/blog/best-gpu-tensorflow-2080-ti-vs-v100-vs-titan-v-vs-1080-ti-benchmark/" data-href="https://lambdalabs.com/blog/best-gpu-tensorflow-2080-ti-vs-v100-vs-titan-v-vs-1080-ti-benchmark/" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://lambdalabs.com/blog/best-gpu-tensorflow-2080-ti-vs-v100-vs-titan-v-vs-1080-ti-benchmark/"><strong class="markup--strong markup--mixtapeEmbed-strong">Deep Learning GPU Benchmarks - Tesla V100 vs RTX 2080 Ti vs GTX 1080 Ti vs Titan V</strong><br><em class="markup--em markup--mixtapeEmbed-em">At Lambda, we&#39;re often asked &quot;what&#39;s the best GPU for deep learning?&quot; In this post and accompanying white paper, we…</em>lambdalabs.com</a><a href="https://lambdalabs.com/blog/best-gpu-tensorflow-2080-ti-vs-v100-vs-titan-v-vs-1080-ti-benchmark/" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="1ce835874a224fcc4c0cc15bb35db542" data-thumbnail-img-id="0*983sNguVHMM8ek6J" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*983sNguVHMM8ek6J);"></a></div><p name="1634" id="1634" class="graf graf--p graf-after--mixtapeEmbed">Here is an excerpt from the above</p><blockquote name="a54f" id="a54f" class="graf graf--blockquote graf--startsWithDoubleQuote graf-after--p">“2080 Ti vs V100 — is the 2080 Ti really that fast?<br>If you absolutely need 32 GB of memory because your model size won’t fit into 11 GB of memory with a batch size of 1. If you are creating your own model architecture and it simply can’t fit even when you bring the batch size lower, the V100 could make sense. However, this is a pretty rare edge case. Fewer than 5% of our customers are using custom models. Most use something like ResNet, VGG, Inception, SSD, or Yolo.</blockquote><blockquote name="fa39" id="fa39" class="graf graf--blockquote graf-after--blockquote">So. You’re still wondering. Why would anybody buy the V100? It comes down to marketing.”</blockquote><p name="c7ba" id="c7ba" class="graf graf--p graf-after--blockquote">Also, I was not expecting a drastic speed up anyway with more CUDA cores as even with HD image frames, we found that the GPU utilisation could not touch 100 per cent meaning that the processing alone was not the bottleneck. Again things are grey here. There is a lot more scope of Engineering optimisations here.</p><p name="83ca" id="83ca" class="graf graf--p graf-after--p">Why we choose V100 was not because of ‘marketing’; it was the only GPU with that much memory 32 GB, which would enable us to batch more image frames in parallel, basically do real-time analytics of more HD video cameras on a single edge.</p><h4 name="1a06" id="1a06" class="graf graf--h4 graf-after--p">Reality regarding more CUDA Cores</h4><p name="3677" id="3677" class="graf graf--p graf-after--h4">The truth was that other than the advantage of processing more frames in parallel due to the higher memory, there was no speedup from 1080 GPU (~2.5k CUDA core). We have also tested this in Jetson TX2 which has much fewer (~256) CUDA cores and one older gaming GPU where it was very slow. So higher CUDA cores help to run faster but beyond some threshold, there is not much difference. Maybe this fact is known already and that’s why the newer models from NVIDIA like <a href="https://www.nvidia.com/en-us/data-center/tesla-t4/" data-href="https://www.nvidia.com/en-us/data-center/tesla-t4/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">T4</a> have around 2.5k CUDA cores, and these are available in GCP and other cloud providers for inference at much cheaper rates. The V100 seems to be used only for training models. It is not practical at least now to train models in lower precision. Maths is easier — gradient descent, back propagation with higher precision.</p><p name="8c65" id="8c65" class="graf graf--p graf-after--p">You can gain more insights from the post by Tom Dettmers regarding GPUs <a href="https://timdettmers.com/2019/04/03/which-gpu-for-deep-learning/" data-href="https://timdettmers.com/2019/04/03/which-gpu-for-deep-learning/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://timdettmers.com/2019/04/03/which-gpu-for-deep-learning/</a>, but do be wary of things like a number of frames you need to process in parallel etc than just raw inference speeds.</p><h4 name="49f2" id="49f2" class="graf graf--h4 graf-after--p">Reality regarding TensorCores, half precision/lower precision FP16, INT8</h4><p name="9113" id="9113" class="graf graf--p graf-after--h4">The field of Data Science Engineering is still nascent in that there is no clear distinction where Data Science ends and Engineering takes over. Frameworks like Tensorflow Serving and tools helps the Dev or DS Operations team to work on the model, develop generic clients and build on top useful applications on the model. But they treat the model without knowing too much in-depth. So when they take a model and do an Optimisation and get an error like below, they don’t know what to do, than get a real inferiority complex and make a mental note on understanding things better</p><pre name="fae3" id="fae3" class="graf graf--pre graf-after--p"><code class="markup--code markup--pre-code">details = &quot;input_max_range must be larger than input_min_range.<br>	 [[{{node Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_87/Area/mul_eightbit/Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_87/Area/sub_1/quantize}}]]<br>	 [[{{node Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/zeros_like_83}}]]&quot;</code></pre><p name="5bbe" id="5bbe" class="graf graf--p graf-after--pre">With that prelude, and based on the optimisations that worked (partially) on converting an Object detection model — Single Shot Detector from TF model zoo, here are the results I got running in V100. Basically, not much speed up, as many of the layers did not get converted**. I had initially done the experiment on a Keras converted model and got similar results, but then I thought that if I used a TF written model, it may be better, and hence the experiments on SSD model.</p><figure name="275f" id="275f" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 527px; max-height: 329px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 62.4%;"></div><img class="graf-image" data-image-id="1*s_XEPWK69qyuLcCH9ab4Jg.png" data-width="527" data-height="329" src="https://cdn-images-1.medium.com/max/800/1*s_XEPWK69qyuLcCH9ab4Jg.png"></div><figcaption class="imageCaption">Time for parsing an HD image — 800*1066 (3 channels)</figcaption></figure><pre name="05ef" id="05ef" class="graf graf--pre graf-after--figure"><code class="markup--code markup--pre-code u-paddingRight0 u-marginRight0">**tensorflow/contrib/tensorrt/segment/segment.cc:443] <strong class="markup--strong markup--pre-strong">There are 3962 ops of 51 different types in the graph that are not converted to TensorRT:</strong> TopKV2, NonMaxSuppressionV2, TensorArrayWriteV3, Const, Squeeze, ResizeBilinear, Maximum, Where, Add, Placeholder, Switch, TensorArrayGatherV3, NextIteration, Greater, TensorArraySizeV3, NoOp, TensorArrayV3, LoopCond, Less, StridedSlice, TensorArrayScatterV3, ExpandDims, Exit, Cast, Identity, Shape, RealDiv, TensorArrayReadV3, Reshape, Merge, Enter, Range, <strong class="markup--strong markup--pre-strong">Conv2D</strong>, Mul, Equal, Sub, Minimum, Tile, Pack, Split, ZerosLike, ConcatV2, Size, Unpack, Assert, DataFormatVecPermute, Transpose, Gather, Exp, Slice, Fill, (For more information see https://docs.nvidia.com/deeplearning/dgx/integrate-tf-trt/index.html#support-ops).</code></pre><p name="9832" id="9832" class="graf graf--p graf-after--pre graf--trailing">The rest of the article is more details on how I did this and which you can also follow step by step.</p></div></div></section><section name="fa06" class="section section--body"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="25b1" id="25b1" class="graf graf--p graf--leading">The post that has really helped me was these from Google team -</p><p name="7c84" id="7c84" class="graf graf--p graf-after--p">[1] <a href="https://medium.com/google-cloud/optimizing-tensorflow-models-for-serving-959080e9ddbf" data-href="https://medium.com/google-cloud/optimizing-tensorflow-models-for-serving-959080e9ddbf" class="markup--anchor markup--p-anchor" rel="nofollow" target="_blank">https://medium.com/google-cloud/optimizing-tensorflow-models-for-serving-959080e9ddbf</a></p><p name="254a" id="254a" class="graf graf--p graf-after--p">[2] <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms#optimizing-for-deployment" data-href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms#optimizing-for-deployment" class="markup--anchor markup--p-anchor" rel="nofollow noopener noopener noopener" target="_blank">https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms#optimizing-for-deployment</a></p><p name="732e" id="732e" class="graf graf--p graf-after--p">I am writing this post as a more detailed explanation to [1], as some parts were not clear when I started following the steps.</p><p name="c5ca" id="c5ca" class="graf graf--p graf-after--p">My Colab/Jupyter notebook for Optimization is given here; you can skip the article and follow the Notebook also as I have documented it in the notebook. The TF Serving and the Client parts are however in this article.</p><p name="fafb" id="fafb" class="graf graf--p graf-after--p graf--trailing"><a href="https://colab.research.google.com/drive/1wQpWoc40kf__WSjfTqDaReMx6fFjUn48" data-href="https://colab.research.google.com/drive/1wQpWoc40kf__WSjfTqDaReMx6fFjUn48" class="markup--anchor markup--p-anchor" rel="nofollow noopener noopener noopener" target="_blank">https://colab.research.google.com/drive/1wQpWoc40kf__WSjfTqDaReMx6fFjUn48</a></p></div></div></section><section name="f116" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="f80b" id="f80b" class="graf graf--p graf--leading">If you have a <em class="markup--em markup--p-em">frozen</em> TF graph you can use the following methods to optimize it before using it for inferences.</p><p name="45e8" id="45e8" class="graf graf--p graf-after--p">There are two types of optimization. One to make it faster or smaller in size to run inferences. And the other to change the weights from higher precision to lower precision. Usually from FP32 to FP16 or INT8. For the latter, the GPU should have the ability to run mixed precision operations (Tensor Cores). Usually, NVIDIA’s desktop or laptop class GTX 1080 or similar are restricted from running lower precision operations. NVIDIA’s server-class GPUs support this. Especially the newer GPUs V100, T4, etc. Not all server GPU’s support it.</p><p name="2e03" id="2e03" class="graf graf--p graf-after--p">The GPU I l use is NVIDIA V100 32 GB GPU which has support for mixed precision operations. Also, you need to run the optimization in the GPU that you are optimizing for. Especially if you are using TensorRT.</p><h4 name="9244" id="9244" class="graf graf--h4 graf-after--p">Step 0. The model, and the Docker Containers</h4><p name="ebb2" id="ebb2" class="graf graf--p graf-after--h4">The first thing that has to be done is to convert the TensorFlow graph to a Frozen Graph. If the graph is Kearns based it is the HD5 format and has to be converted to the TF model and then to the frozen graph. A frozen graph has the value of variables embedded in the graph itself. It is a GrpahDef/protocol buffer (pb) format like a Saved Model only it cannot be retrained.</p><div name="c9cf" id="c9cf" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://stackoverflow.com/questions/52934795/what-is-difference-frozen-inference-graph-pb-and-saved-model-pb" data-href="https://stackoverflow.com/questions/52934795/what-is-difference-frozen-inference-graph-pb-and-saved-model-pb" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://stackoverflow.com/questions/52934795/what-is-difference-frozen-inference-graph-pb-and-saved-model-pb"><strong class="markup--strong markup--mixtapeEmbed-strong">What is difference frozen_inference_graph.pb and saved_model.pb?</strong><br><em class="markup--em markup--mixtapeEmbed-em">frozen_inference_graph.pb, is a frozen graph that cannot be trained anymore, it defines the graphdef and is actually a…</em>stackoverflow.com</a><a href="https://stackoverflow.com/questions/52934795/what-is-difference-frozen-inference-graph-pb-and-saved-model-pb" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="1dd18dc5f5f599f99e05cf6f604164f7" data-thumbnail-img-id="0*Nf4U-ID-FFOWG7JF" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*Nf4U-ID-FFOWG7JF);"></a></div><p name="6fde" id="6fde" class="graf graf--p graf-after--mixtapeEmbed">The model that we are using is the SSD model <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">ssd_resnet_50_fpn_coco </em></strong>form TF model zoo -<a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md" data-href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md</a></p><p name="6b30" id="6b30" class="graf graf--p graf-after--p">Docker container used for the optimization is <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">tensorflow/tensorflow:1.13.0rc1-gpu-jupyter</em></strong></p><pre name="ee6c" id="ee6c" class="graf graf--pre graf-after--p">docker run <strong class="markup--strong markup--pre-strong">--entrypoint=/bin/bash</strong> --runtime=nvidia  -it --rm -p 8900:8500 -p 8901:8501 -v /usr/alex/:/coding --net=host tensorflow/tensorflow:1.13.0rc1-gpu-jupyter<br>once inside<br>cd /coding<br>jupyter notebook --allow-root &amp;</pre><p name="3e67" id="3e67" class="graf graf--p graf-after--pre">Note- I changed the entry point to something more convenient to me than default tf-notebook I believe.</p><p name="397d" id="397d" class="graf graf--p graf-after--p">After optimizing, to run inferences I am using the same docker image after installing on that TF serving API’s, as well as headless opencv-python version. This is because we will be converting the optimized model to a TF serving compatible model for inference.</p><pre name="a0b2" id="a0b2" class="graf graf--pre graf-after--p">docker run --entrypoint=/bin/bash --env http_proxy=&lt;my proxy&gt; --env https_proxy=&lt;my proxy&gt;  --runtime=nvidia  -it --rm -p 8900:8500 -p 8901:8501 -v /usr/alex/:/coding --net=host tensorflow/tensorflow:1.13.0rc1-gpu-jupyter</pre><pre name="7bf3" id="7bf3" class="graf graf--pre graf-after--pre">pip install tensorflow-serving-api<br>pip install opencv-python==3.3.0.9<br>cd coding<br>python ssd_client_1.py -num_tests=1 -server=127.0.0.1:8500 -batch_size=1 -img_path=&#39;../examples/google1.jpg/&#39;</pre><h4 name="b23a" id="b23a" class="graf graf--h4 graf-after--pre">Step 1. Get the output node names in the Tensorflow Graph</h4><p name="5150" id="5150" class="graf graf--p graf-after--h4">Why is this important? We need to find the output node names of the frozen graph as it is needed to optimize the graph. Note Tensorflow version that is used in TF 1.13</p><pre name="9dd3" id="9dd3" class="graf graf--pre graf-after--p"># To Freeze the Saved Model<br># We need to freeze the model to do further optimisation on it</pre><pre name="7fc6" id="7fc6" class="graf graf--pre graf-after--pre">from tensorflow.python.saved_model import tag_constants<br>from tensorflow.python.tools import freeze_graph<br>from tensorflow.python import ops<br>from tensorflow.tools.graph_transforms import TransformGraph</pre><pre name="72f5" id="72f5" class="graf graf--pre graf-after--pre">def freeze_model(saved_model_dir, output_node_names, output_filename):<br>  output_graph_filename = os.path.join(saved_model_dir, output_filename)<br>  initializer_nodes = &#39;&#39;<br>  freeze_graph.freeze_graph(<br>      input_saved_model_dir=saved_model_dir,<br>      output_graph=output_graph_filename,<br>      saved_model_tags = tag_constants.SERVING,<br>      <strong class="markup--strong markup--pre-strong">output_node_names=output_node_names,</strong><br>      initializer_nodes=initializer_nodes,<br>      input_graph=None,<br>      input_saver=False,<br>      input_binary=False,<br>      input_checkpoint=None,<br>      restore_op_name=None,<br>      filename_tensor_name=None,<br>      clear_devices=True,<br>      input_meta_graph=False,<br>  )</pre><p name="deef" id="deef" class="graf graf--p graf-after--pre">For this, we can plot the model in TF Board and see the output nodes, or print the nodes and grep on some keywords.</p><pre name="d471" id="d471" class="graf graf--pre graf-after--p"># Source <a href="https://medium.com/google-cloud/optimizing-tensorflow-models-for-serving-959080e9ddbf" data-href="https://medium.com/google-cloud/optimizing-tensorflow-models-for-serving-959080e9ddbf" class="markup--anchor markup--pre-anchor" rel="nofollow" target="_blank">https://medium.com/google-cloud/optimizing-tensorflow-models-for-serving-959080e9ddbf</a></pre><pre name="8367" id="8367" class="graf graf--pre graf-after--pre">def get_graph_def_from_file(graph_filepath):<br>  tf.reset_default_graph()<br>  with ops.Graph().as_default():<br>    with tf.gfile.GFile(graph_filepath, &#39;rb&#39;) as f:<br>      graph_def = tf.GraphDef()<br>      graph_def.ParseFromString(f.read())<br>      return graph_def</pre><p name="343f" id="343f" class="graf graf--p graf-after--pre">let us use the above helper to print the input and output nodes, input nodes via the for loop -</p><pre name="bc3b" id="bc3b" class="graf graf--pre graf-after--p">graph_def =get_graph_def_from_file(&#39;/coding/ssd_inception_v2_coco_2018_01_28/frozen_inference_graph.pb&#39;)<br>for node in graph_def.node:<br>    if node.op==&#39;Placeholder&#39;:<br>        print node # this will be the input node</pre><p name="febf" id="febf" class="graf graf--p graf-after--pre">and output nodes by plotting it in a format readable by Tensor Board.</p><pre name="461e" id="461e" class="graf graf--pre graf-after--p">with tf.Session(graph=tf.Graph()) as session:<br>    mygraph = tf.import_graph_def(graph_def, name=&#39;&#39;)<br>    writer = tf.summary.FileWriter(<strong class="markup--strong markup--pre-strong">logdir=&#39;/coding/log_tb/1&#39;</strong>, graph=session.graph)<br>    writer.flush()</pre><p name="63ab" id="63ab" class="graf graf--p graf-after--pre">Let us invoke Tensor board.</p><pre name="d818" id="d818" class="graf graf--pre graf-after--p">#ssh -L 6006:127.0.0.1:6006 root@&lt;remoteip&gt; # for tensor board - in your local machine type 127.0.0.1</pre><pre name="a16b" id="a16b" class="graf graf--pre graf-after--pre">tensorboard <strong class="markup--strong markup--pre-strong">--logdir &#39;/coding/log_tb/1&#39;</strong></pre><p name="cda6" id="cda6" class="graf graf--p graf-after--pre">From this, I could make out the output nodes. Note that if you are building the graph yourself you don’t need to do this circus. Since I am using a model that is opensourced and with less documentation I am using this. Sometimes for auto converted/TF imported graphs, the names will be pretty long. You can then print the nodes in a for a loop as I did for Placeholder and from the output, shape make out ( for detections class, score, rectangle coordinates)</p><pre name="f0ad" id="f0ad" class="graf graf--pre graf-after--p"># These are the output names. Add a index usually 0 for graph nodes. # You can print the node details by nodenames</pre><pre name="947f" id="947f" class="graf graf--pre graf-after--pre">output_node_names = [&#39;detection_boxes&#39;,&#39;detection_scores&#39;,&#39;detection_classes&#39;,&#39;num_detections&#39;]<br>outputs =  [&#39;detection_boxes:0&#39;,&#39;detection_scores:0&#39;,&#39;detection_classes:0&#39;,&#39;num_detections:0&#39;]</pre><h4 name="0a53" id="0a53" class="graf graf--h4 graf-after--pre">Step 3 Optimise using TF Graph Transform Tools</h4><p name="cdff" id="cdff" class="graf graf--p graf-after--h4">The snippet below illustrates how you can optimize a graph after reading it from disk.</p><pre name="6bdd" id="6bdd" class="graf graf--pre graf-after--p"># Source <a href="https://medium.com/google-cloud/optimizing-tensorflow-models-for-serving-959080e9ddbf" data-href="https://medium.com/google-cloud/optimizing-tensorflow-models-for-serving-959080e9ddbf" class="markup--anchor markup--pre-anchor" rel="nofollow" target="_blank">https://medium.com/google-cloud/optimizing-tensorflow-models-for-serving-959080e9ddbf</a></pre><pre name="1289" id="1289" class="graf graf--pre graf-after--pre">#<a href="https://gist.github.com/lukmanr" data-href="https://gist.github.com/lukmanr" class="markup--anchor markup--pre-anchor" rel="noopener" target="_blank">https://gist.github.com/lukmanr</a></pre><pre name="aa44" id="aa44" class="graf graf--pre graf-after--pre"># Optimizing the graph via TensorFlow library<br><strong class="markup--strong markup--pre-strong">from tensorflow.tools.graph_transforms import TransformGraph</strong></pre><pre name="4043" id="4043" class="graf graf--pre graf-after--pre">def optimize_graph(model_dir, graph_filename, transforms, output_names, outname=&#39;optimized_model.pb&#39;):<br>  input_names = [&#39;input_image&#39;,] # change this as per how you have saved the model<br>  <strong class="markup--strong markup--pre-strong">graph_def</strong> = <strong class="markup--strong markup--pre-strong">get_graph_def_from_file</strong>(os.path.join(model_dir, graph_filename))<br>  <strong class="markup--strong markup--pre-strong">optimized_graph_def</strong> = <strong class="markup--strong markup--pre-strong">TransformGraph</strong>(<br>      <strong class="markup--strong markup--pre-strong">graph_def</strong>,<br>      input_names,  <br>      output_names,<br>      <strong class="markup--strong markup--pre-strong">transforms</strong>)<br>  tf.train.write_graph(<strong class="markup--strong markup--pre-strong">optimized_graph_def</strong>,<br>                      logdir=model_dir,<br>                      as_text=False,<br>                      name=outname)<br>  print(&#39;Graph optimized!&#39;)</pre><p name="6959" id="6959" class="graf graf--p graf-after--pre">Let us use the above helper to optimize the graph first <strong class="markup--strong markup--p-strong">quantize_weights</strong></p><pre name="7de6" id="7de6" class="graf graf--pre graf-after--p"># Optimization without Qunatization - Reduce the size of the model<br># speed may actually be slower<br># see <a href="https://medium.com/google-cloud/optimizing-tensorflow-models-for-serving-959080e9ddbf" data-href="https://medium.com/google-cloud/optimizing-tensorflow-models-for-serving-959080e9ddbf" class="markup--anchor markup--pre-anchor" rel="nofollow" target="_blank">https://medium.com/google-cloud/optimizing-tensorflow-models-for-serving-959080e9ddbf</a></pre><pre name="6334" id="6334" class="graf graf--pre graf-after--pre">transforms = [&#39;remove_nodes(op=Identity)&#39;, \<br> &#39;<strong class="markup--strong markup--pre-strong">merge_duplicate_nodes</strong>&#39;, \<br> &#39;<strong class="markup--strong markup--pre-strong">strip_unused_nodes</strong>&#39;,<br> &#39;<strong class="markup--strong markup--pre-strong">fold_constants</strong>(ignore_errors=true)&#39;,<br> &#39;<strong class="markup--strong markup--pre-strong">fold_batch_norms</strong>&#39;,<br> &#39;<strong class="markup--strong markup--pre-strong">quantize_weights</strong>&#39;] #this reduces the size, but there is no speed up , actaully slows down, see below</pre><pre name="aafe" id="aafe" class="graf graf--pre graf-after--pre"><strong class="markup--strong markup--pre-strong">optimize_graph</strong>(&#39;/coding/ssd_inception_v2_coco_2018_01_28&#39;, &#39;frozen_inference_graph.pb&#39; ,<br>               transforms, output_node_names,outname=&#39;optimized_model_small.pb&#39;)</pre><p name="f9eb" id="f9eb" class="graf graf--p graf-after--pre">Let’s then convert the optimized model to TF serving compatible format.</p><pre name="82ce" id="82ce" class="graf graf--pre graf-after--p">#lets convert this to a s TF Serving compatible mode;<br><strong class="markup--strong markup--pre-strong">convert_graph_def_to_saved_model</strong>(&#39;/coding/ssd_inception_v2_coco_2018_01_28/2&#39;,<br>                                 &#39;/coding/ssd_inception_v2_coco_2018_01_28/optimized_model_small.pb&#39;,outputs)</pre><p name="edd8" id="edd8" class="graf graf--p graf-after--pre">The helper that does this is given below</p><pre name="67b0" id="67b0" class="graf graf--pre graf-after--p"># Source <a href="https://medium.com/google-cloud/optimizing-tensorflow-models-for-serving-959080e9ddbf" data-href="https://medium.com/google-cloud/optimizing-tensorflow-models-for-serving-959080e9ddbf" class="markup--anchor markup--pre-anchor" rel="nofollow" target="_blank">https://medium.com/google-cloud/optimizing-tensorflow-models-for-serving-959080e9ddbf</a></pre><pre name="a5fb" id="a5fb" class="graf graf--pre graf-after--pre">#<a href="https://gist.github.com/lukmanr" data-href="https://gist.github.com/lukmanr" class="markup--anchor markup--pre-anchor" rel="noopener" target="_blank">https://gist.github.com/lukmanr</a></pre><pre name="84d9" id="84d9" class="graf graf--pre graf-after--pre">def convert_graph_def_to_saved_model(export_dir, graph_filepath,outputs):</pre><pre name="a6c4" id="a6c4" class="graf graf--pre graf-after--pre">graph_def = <strong class="markup--strong markup--pre-strong">get_graph_def_from_file</strong>(graph_filepath)<br>  with tf.Session(graph=tf.Graph()) as session:<br>    tf.import_graph_def(graph_def, name=&#39;&#39;)<br>    <strong class="markup--strong markup--pre-strong">tf.saved_model.simple_save</strong>(<br>        session,<br>        export_dir,# change input_image to node.name if you know the name<br>        inputs={&#39;<strong class="markup--strong markup--pre-strong">input_image</strong>&#39;: session.graph.get_tensor_by_name(&#39;{}:0&#39;.format(node.name))<br>            for node in graph_def.node if node.op==&#39;<strong class="markup--strong markup--pre-strong">Placeholder</strong>&#39;},<br>        outputs={t:session.graph.get_tensor_by_name(t) for t in outputs}<br>                        <br>                <br>    )<br>    print(&#39;Optimized graph converted to SavedModel!&#39;)</pre><p name="a74b" id="a74b" class="graf graf--p graf-after--pre">And then <strong class="markup--strong markup--p-strong">‘quantize_weights’ and ‘quantize_nodes’.</strong></p><p name="19fc" id="19fc" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">This should really covert also the calculation to lower precision - but does not work as of now.</strong></p><p name="03ee" id="03ee" class="graf graf--p graf-after--p"><em class="markup--em markup--p-em">&quot;This process converts all the operations in the graph that have eight-bit quantized equivalents and leaves the rest in floating point. Only a subset of ops are supported and on many platforms, the quantized code may actually be slower than the float equivalents, but this is a way of increasing performance substantially when all the circumstances are right.”<br></em><a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms#optimizing-for-deployment" data-href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms#optimizing-for-deployment" class="markup--anchor markup--p-anchor" rel="nofollow noopener noopener" target="_blank"><em class="markup--em markup--p-em">https://github.com/tensorflow/tensorflow/tree/master/tensorflow/tools/graph_transforms#optimizing-for-deployment</em></a></p><pre name="6c15" id="6c15" class="graf graf--pre graf-after--p">transforms = [&#39;add_default_attributes&#39;, \<br>  &#39;strip_unused_nodes&#39;, \<br>  &#39;remove_nodes(op=Identity, op=CheckNumerics)&#39;,\<br>  &#39;fold_constants(ignore_errors=true)&#39;,<br>  &#39;fold_batch_norms&#39;,<br>  &#39;fold_old_batch_norms&#39;,<br>  <strong class="markup--strong markup--pre-strong">&#39;quantize_weights&#39;,</strong><br>  <strong class="markup--strong markup--pre-strong">&#39;quantize_nodes&#39;,</strong><br>  &#39;strip_unused_nodes&#39;,<br>  &#39;sort_by_execution_order&#39;]</pre><pre name="b2ce" id="b2ce" class="graf graf--pre graf-after--pre">optimize_graph(&#39;/coding/ssd_inception_v2_coco_2018_01_28&#39;, &#39;frozen_inference_graph.pb&#39; ,<br>               transforms, output_node_names,outname=&#39;optimized_model_weight_quant.pb&#39;)</pre><p name="de65" id="de65" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">However this does not work in the sense, inference using this optimized model gives the error.</strong> I had tried with a Keras model earlier and got another error message. This seems to be a bug as now this model is a pure Tensorflow model and I have not changed anything here</p><pre name="c170" id="c170" class="graf graf--pre graf-after--p">(‘Got an error’, &lt;_Rendezvous of RPC that terminated with:<br> status = StatusCode.INVALID_ARGUMENT<br> details = “input_max_range must be larger than input_min_range.<br> [[{{node Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_87/Area/<strong class="markup--strong markup--pre-strong">mul_eightbit</strong>/Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_87/Area/sub_1/quantize}}]]<br> [[{{node Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/zeros_like_83}}]]”<br> debug_error_string = <strong class="markup--strong markup--pre-strong">“{“created”:”</strong><a href="http://twitter.com/1555723203" data-href="http://twitter.com/1555723203" class="markup--anchor markup--pre-anchor" title="Twitter profile for @1555723203" rel="noopener" target="_blank"><strong class="markup--strong markup--pre-strong">@1555723203</strong></a><strong class="markup--strong markup--pre-strong">.356344655&quot;,”description”:”Error received from peer”,”file”:”src/core/lib/surface/call.cc”,”file_line”:1036,”grpc_message”:”input_max_range must be larger than input_min_range.\n\t [[{{node </strong>Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_87/Area/mul_eightbit/Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ClipToWindow_87/Area/sub_1/quantize}}]]\n\t [[{{node Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/zeros_like_83}}]]”,”grpc_status”:3}”<br>&gt;)<br>Response Received Exiting</pre><h4 name="a4f1" id="a4f1" class="graf graf--h4 graf-after--pre">Step 4 Optimise using NVIDIA TenosrRT</h4><p name="9280" id="9280" class="graf graf--p graf-after--h4">Base reference for this is these two posts</p><p name="54e3" id="54e3" class="graf graf--p graf-after--p"><a href="https://docs.nvidia.com/deeplearning/dgx/integrate-tf-trt/index.html" data-href="https://docs.nvidia.com/deeplearning/dgx/integrate-tf-trt/index.html" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://docs.nvidia.com/deeplearning/dgx/integrate-tf-trt/index.html</a></p><p name="2a91" id="2a91" class="graf graf--p graf-after--p"><a href="https://developers.googleblog.com/2018/03/tensorrt-integration-with-tensorflow.html" data-href="https://developers.googleblog.com/2018/03/tensorrt-integration-with-tensorflow.html" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://developers.googleblog.com/2018/03/tensorrt-integration-with-tensorflow.html</a></p><p name="c497" id="c497" class="graf graf--p graf-after--p">Inference with TF-TRT `SavedModel` workflow: we are using the TF Serving model.</p><pre name="ff8b" id="ff8b" class="graf graf--pre graf-after--p">import tensorflow.contrib.tensorrt as trt<br>tf.reset_default_graph()<br>graph = tf.Graph()<br>sess = tf.Session()<br># Create a TensorRT inference graph from a SavedModel:<br>with graph.as_default():<br>    with tf.Session() as sess:<br>        trt_graph = trt.create_inference_graph(<br>            input_graph_def=None,<br>            outputs=outputs,<br>            <strong class="markup--strong markup--pre-strong">input_saved_model_dir</strong>=<strong class="markup--strong markup--pre-strong">&#39;/coding/ssd_inception_v2_coco_2018_01_28/01&#39;</strong>,<br>            input_saved_model_tags=[&#39;<strong class="markup--strong markup--pre-strong">serve</strong>&#39;],<br>            max_batch_size=1,<br>            max_workspace_size_bytes=7000000000,<br>            <strong class="markup--strong markup--pre-strong">precision_mode=&#39;FP16&#39;)</strong> <br>            #precision_mode=&#39;FP32&#39;)<br>            #precision_mode=&#39;<strong class="markup--strong markup--pre-strong">INT8</strong>&#39;)<br>        output_node=tf.import_graph_def(trt_graph, return_elements=outputs)<br>        #sess.run(output_node)<br>        tf.saved_model.simple_save(sess,<br>        &quot;/coding/ssd_inception_v2_coco_2018_01_28/4&quot;,<br>        inputs={&#39;input_image&#39;: graph.get_tensor_by_name(&#39;{}:0&#39;.format(node.name))<br>            for node in graph.as_graph_def().node if node.op==&#39;Placeholder&#39;},<br>        outputs={t:graph.get_tensor_by_name(&#39;import/&#39;+t) for t in outputs}<br>    <br>    )</pre><p name="ef62" id="ef62" class="graf graf--p graf-after--pre"><strong class="markup--strong markup--p-strong">Inference with TF-TRT `Frozen` graph workflow:</strong></p><p name="3e21" id="3e21" class="graf graf--p graf-after--p">Reference <a href="https://medium.com/tensorflow/speed-up-tensorflow-inference-on-gpus-with-tensorrt-13b49f3db3fa" data-href="https://medium.com/tensorflow/speed-up-tensorflow-inference-on-gpus-with-tensorrt-13b49f3db3fa" class="markup--anchor markup--p-anchor" rel="nofollow" target="_blank">https://medium.com/tensorflow/speed-up-tensorflow-inference-on-gpus-with-tensorrt-13b49f3db3fa</a></p><pre name="a9d9" id="a9d9" class="graf graf--pre graf-after--p">#Lets load a frozen model and reset the graph and use<br>gdef =<strong class="markup--strong markup--pre-strong">get_graph_def_from_file</strong>(‘/coding/ssd_inception_v2_coco_2018_01_28/frozen_inference_graph.pb’)<br>tf.<strong class="markup--strong markup--pre-strong">reset_default_graph</strong>()<br>graph = tf.Graph()<br>sess = tf.Session()<br># Create a TensorRT inference graph from a SavedModel:<br>with graph.as_default():<br> with tf.Session() as sess:<br> trt_graph = <strong class="markup--strong markup--pre-strong">trt</strong>.<strong class="markup--strong markup--pre-strong">create_inference_graph</strong>(<br> <strong class="markup--strong markup--pre-strong">input_graph_def=gdef,</strong><br> outputs=outputs,<br> max_batch_size=8,<br> max_workspace_size_bytes=7000000000,<br> is_dynamic_op=True,<br> #precision_mode=’FP16&#39;) <br> #precision_mode=’FP32&#39;)<br> <strong class="markup--strong markup--pre-strong">precision_mode=’INT8&#39;)</strong><br> <br> output_node=tf.import_graph_def(trt_graph, return_elements=outputs)<br> #sess.run(output_node)<br> tf.saved_model.simple_save(sess,<br> “/coding/ssd_inception_v2_coco_2018_01_28/5”,<br> inputs={‘input_image’: graph.get_tensor_by_name(‘{}:0’.format(node.name))<br> for node in graph.as_graph_def().node if node.op==’Placeholder’},<br> outputs={t:graph.get_tensor_by_name(‘import/’+t) for t in outputs}<br> <br> )</pre><h4 name="cd2b" id="cd2b" class="graf graf--h4 graf-after--pre"><strong class="markup--strong markup--h4-strong">Step 5: Pause and Check the models</strong></h4><p name="336a" id="336a" class="graf graf--p graf-after--h4">The outputs of the various models are given below. You can see that the model size reduces after optimizations.</p><pre name="8e4d" id="8e4d" class="graf graf--pre graf-after--p"><strong class="markup--strong markup--pre-strong">Original model </strong>(&#39;/coding/ssd_inception_v2_coco_2018_01_28/frozen_inference_graph.pb&#39;, &#39;&#39;) <br>Model size: 99591.409 KB<br> Variables size: 0.0 KB<br> <strong class="markup--strong markup--pre-strong">Total Size: 99591.409 KB </strong><br>---------Tensorflow Transform Optimised model <strong class="markup--strong markup--pre-strong">Weights Quantised </strong>(&#39;/coding/ssd_inception_v2_coco_2018_01_28/2/saved_model.pb&#39;, &#39;&#39;) Model size: 26193.27 KB<br> Variables size: 0.0 KB<br><strong class="markup--strong markup--pre-strong"> Total Size: 26193.27 KB</strong><br> ---------Tensorflow Transform Optimised model Weights and Nodes Quantised (&#39;/coding/ssd_inception_v2_coco_2018_01_28/3/saved_model.pb&#39;, &#39;&#39;) Model size: 29265.284 KB<br> Variables size: 0.0 KB<br> Total Size: 29265.284 KB<br> ---------NVIDIA RT Optimised model <strong class="markup--strong markup--pre-strong">FP16</strong> (&#39;/coding/ssd_inception_v2_coco_2018_01_28/4/saved_model.pb&#39;, &#39;&#39;) Model size: 178564.229 KB<br> Variables size: 0.0 KB<br> <strong class="markup--strong markup--pre-strong">Total Size: 178564.229 KB</strong><br> ---------NVIDIA RT Optimised model <strong class="markup--strong markup--pre-strong">INT8</strong> (&#39;/coding/ssd_inception_v2_coco_2018_01_28/5/saved_model.pb&#39;, &#39;&#39;) Model size: 178152.834 KB<br> Variables size: 0.0 KB<br> <strong class="markup--strong markup--pre-strong">Total Size: 178152.834 KB</strong></pre><h4 name="e62c" id="e62c" class="graf graf--h4 graf-after--pre">Step 6: Ready the TF Serving container to server these models</h4><p name="c72e" id="c72e" class="graf graf--p graf-after--h4">Note the container we are using here — Client</p><pre name="91f7" id="91f7" class="graf graf--pre graf-after--p">docker run --entrypoint=/bin/bash --env http_proxy=&lt;my proxy&gt; --env https_proxy=&lt;my proxy&gt;  --runtime=nvidia  -it --rm -p 8900:8500 -p 8901:8501 -v /usr/alex/:/coding --net=host tensorflow/tensorflow:1.13.0rc1-gpu-jupyter</pre><pre name="8b59" id="8b59" class="graf graf--pre graf-after--pre">pip install tensorflow-serving-api<br>pip install opencv-python==3.3.0.9<br>cd coding<br>python ssd_client_1.py -num_tests=1 -server=127.0.0.1:8500 -batch_size=1 -img_path=&#39;../examples/google1.jpg/&#39;</pre><p name="09d8" id="09d8" class="graf graf--p graf-after--pre">Server -This is pasted from Step 0. This is run in the V100 32 GB Linux/machine.</p><pre name="629f" id="629f" class="graf graf--pre graf-after--p">docker run  --net=host --runtime=nvidia  -it --rm -p 8900:8500 -p 8901:8501 -v /usr/alex/:/models  tensorflow/serving:1.13.0-gpu --rest_api_port=0  --enable_batching=true --model_config_file=/models/ssd_inception_v3_coco.json</pre><p name="c532" id="c532" class="graf graf--p graf-after--pre">where the <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">config json</em></strong> is like below. Since I have placed the different models in folders under “/models/ssd_inception_v2_coco_2018_01_28/” as 01 — original model, 2-TF Graph Transform Weight Quantized, 3- TF Graph Transform Weight and Node Quantized,4-TensorRT FP16,5-TensorRT INT8; I just change the versions in the file to load different servables for each test.</p><pre name="4f95" id="4f95" class="graf graf--pre graf-after--p">model_config_list {<br> config {<br>    name: &quot;ssd_inception_v2_coco&quot;,<br>    base_path: &quot;/models/ssd_inception_v2_coco_2018_01_28/&quot;,<br>   model_version_policy: {<br>      specific: { <br>        <strong class="markup--strong markup--pre-strong">versions:[01]</strong><br>      } <br>    },<br>    model_platform:&quot;tensorflow&quot;,<br>  }<br>}</pre><h4 name="5aed" id="5aed" class="graf graf--h4 graf-after--pre">Step 7: Write a TF Serving Client for tests</h4><p name="82d4" id="82d4" class="graf graf--p graf-after--h4">I have written about this in detail in a previous post.</p><div name="42c4" id="42c4" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://towardsdatascience.com/productising-tensorflow-keras-models-via-tensorflow-serving-69e191cb1f37" data-href="https://towardsdatascience.com/productising-tensorflow-keras-models-via-tensorflow-serving-69e191cb1f37" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://towardsdatascience.com/productising-tensorflow-keras-models-via-tensorflow-serving-69e191cb1f37"><strong class="markup--strong markup--mixtapeEmbed-strong">Writing a Generic Tensorflow Serving Client for Tensorflow Serving model</strong><br><em class="markup--em markup--mixtapeEmbed-em">For CNN based object detection models</em>towardsdatascience.com</a><a href="https://towardsdatascience.com/productising-tensorflow-keras-models-via-tensorflow-serving-69e191cb1f37" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="97594ea6918742a3a10674c98b15e039" data-thumbnail-img-id="1*cntKPCdIsck5ia2jDrhg9w.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*cntKPCdIsck5ia2jDrhg9w.png);"></a></div><p name="40f5" id="40f5" class="graf graf--p graf-after--mixtapeEmbed">The saved model of the SSD is like below You can use the saved model CLI to view it</p><pre name="e333" id="e333" class="graf graf--pre graf-after--p">saved_model_cli show --dir &#39;/coding/ssd_inception_v2_coco_2018_01_28/3&#39; --all</pre><pre name="6231" id="6231" class="graf graf--pre graf-after--pre">MetaGraphDef with tag-set: &#39;serve&#39; contains the following SignatureDefs:</pre><pre name="a77f" id="a77f" class="graf graf--pre graf-after--pre">signature_def[&#39;serving_default&#39;]:<br>  The given SavedModel SignatureDef contains the following input(s):<br>   <strong class="markup--strong markup--pre-strong"> inputs[&#39;input_image&#39;]</strong> tensor_info:<br>        dtype: DT_UINT8<br>        shape: (<strong class="markup--strong markup--pre-strong">-1, -1, -1, 3</strong>)<br>        name: image_tensor:0<br>  The given SavedModel SignatureDef contains the following output(s):<br>    outputs[&#39;<strong class="markup--strong markup--pre-strong">detection_boxes:0</strong>&#39;] tensor_info:<br>        dtype: DT_FLOAT<br>        shape: unknown_rank<br>        name: detection_boxes:0<br>    outputs[&#39;<strong class="markup--strong markup--pre-strong">detection_classes:0</strong>&#39;] tensor_info:<br>        dtype: DT_FLOAT<br>        shape: unknown_rank<br>        name: detection_classes:0<br>    outputs[&#39;<strong class="markup--strong markup--pre-strong">detection_scores:0</strong>&#39;] tensor_info:<br>        dtype: DT_FLOAT<br>        shape: unknown_rank<br>        name: detection_scores:0<br>    outputs[&#39;<strong class="markup--strong markup--pre-strong">num_detections:0</strong>&#39;] tensor_info:<br>        dtype: DT_FLOAT<br>        shape: unknown_rank<br>        name: num_detections:0<br>  Method name is: tensorflow/serving/predict</pre><p name="9c99" id="9c99" class="graf graf--p graf-after--pre">Note that in this, the input and output node names are slightly different from the original model- whose input is ‘inputs’ and output is ‘detection_boxes’,’detection_classes’,’detection_scores’ (without the :0 part- which is a deficiency in the conversion scripts that I have used- but can be rectified easily)</p><p name="40a7" id="40a7" class="graf graf--p graf-after--p">Original model</p><pre name="f331" id="f331" class="graf graf--pre graf-after--p">root@ndn-oe:/coding/tfclient# saved_model_cli show - dir /coding/ssd_inception_v2_coco_2018_01_28/01/ - all<br>MetaGraphDef with tag-set: &#39;serve&#39; contains the following SignatureDefs:<br>signature_def[&#39;serving_default&#39;]:<br> The given SavedModel SignatureDef contains the following input(s):<br> inputs[&#39;inputs&#39;] tensor_info:<br> dtype: DT_UINT8<br> shape: (-1, -1, -1, 3)<br> name: image_tensor:0<br> The given SavedModel SignatureDef contains the following output(s):<br> outputs[&#39;detection_boxes&#39;] tensor_info:<br> dtype: DT_FLOAT<br> shape: (-1, 100, 4)<br> name: detection_boxes:0<br> outputs[&#39;detection_classes&#39;] tensor_info:<br> dtype: DT_FLOAT<br> shape: (-1, 100)<br> name: detection_classes:0<br> outputs[&#39;detection_scores&#39;] tensor_info:<br> dtype: DT_FLOAT<br> shape: (-1, 100)<br> name: detection_scores:0<br> outputs[&#39;num_detections&#39;] tensor_info:<br> dtype: DT_FLOAT<br> shape: (-1)<br> name: num_detections:0<br> Method name is: tensorflow/serving/predict</pre><p name="04bf" id="04bf" class="graf graf--p graf-after--pre">The TF Serving client is given here -<a href="https://gist.github.com/alexcpn/d7c28230af437dafb0d2cc7f50140eed" data-href="https://gist.github.com/alexcpn/d7c28230af437dafb0d2cc7f50140eed" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://gist.github.com/alexcpn/d7c28230af437dafb0d2cc7f50140eed</a></p><p name="db26" id="db26" class="graf graf--p graf-after--p">The rest of the imports are here, the client is slightly different, the names of inputs and outputs, that’s why it is on gist <a href="https://github.com/alexcpn/tf_serving_clients" data-href="https://github.com/alexcpn/tf_serving_clients" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://github.com/alexcpn/tf_serving_clients</a></p><p name="675c" id="675c" class="graf graf--p graf-after--p">The image file used for the test is <a href="https://github.com/fizyr/keras-retinanet/blob/master/examples/000000008021.jpg" data-href="https://github.com/fizyr/keras-retinanet/blob/master/examples/000000008021.jpg" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://github.com/fizyr/keras-retinanet/blob/master/examples/000000008021.jpg</a></p><h4 name="ac92" id="ac92" class="graf graf--h4 graf-after--p">Step 8: The Output from various models</h4><p name="5cda" id="5cda" class="graf graf--p graf-after--h4">Basically, there is hardly any difference between the optimized and non-optimized model. Batch size is one here.</p><figure name="8bfe" id="8bfe" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 527px; max-height: 329px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 62.4%;"></div><img class="graf-image" data-image-id="1*s_XEPWK69qyuLcCH9ab4Jg.png" data-width="527" data-height="329" src="https://cdn-images-1.medium.com/max/800/1*s_XEPWK69qyuLcCH9ab4Jg.png"></div><figcaption class="imageCaption">Time for parsing an HD image — 800*1066 (3 channels)</figcaption></figure><p name="c33e" id="c33e" class="graf graf--p graf-after--figure">More details below</p><blockquote name="5ef3" id="5ef3" class="graf graf--blockquote graf-after--p"><strong class="markup--strong markup--blockquote-strong">Original Model</strong></blockquote><blockquote name="d15c" id="d15c" class="graf graf--blockquote graf-after--blockquote">Invocaiton :</blockquote><blockquote name="e4ab" id="e4ab" class="graf graf--blockquote graf-after--blockquote">coding/tfclient# python ssd_client_1.py -num_tests=1 -server=127.0.0.1:8500 -batch_size=1 -img_path=’../examples/000000008021.jpg’</blockquote><blockquote name="df91" id="df91" class="graf graf--blockquote graf-after--blockquote">(‘Image path’, ‘../examples/000000008021.jpg’)<br>(‘original image shape=’, (480, 640, 3))<br>(‘Input-s shape’, (1, 800, 1066, 3)) → This is the size of input tensor</blockquote><blockquote name="d1ef" id="d1ef" class="graf graf--blockquote graf-after--blockquote">Ouput <br>(‘Label’, u’person’, ‘ at ‘, array([412, 171, 740, 624]), ‘ Score ‘, 0.9980476)<br>(‘Label’, u’person’, ‘ at ‘, array([ 6, 423, 518, 788]), ‘ Score ‘, 0.94931936)<br>(‘Label’, u’person’, ‘ at ‘, array([ 732, 473, 1065, 793]), ‘ Score ‘, 0.88419175)<br>(‘Label’, u’tie’, ‘ at ‘, array([529, 337, 565, 494]), ‘ Score ‘, 0.40442815)<br>(‘Time for ‘, 1, ‘ is ‘, <strong class="markup--strong markup--blockquote-strong">0.5993821620941162</strong>)</blockquote><blockquote name="3678" id="3678" class="graf graf--blockquote graf-after--blockquote"><strong class="markup--strong markup--blockquote-strong">Tensorflow Transform Optimised model Weights Quantized</strong></blockquote><blockquote name="705f" id="705f" class="graf graf--blockquote graf-after--blockquote">(‘Label’, u’person’, ‘ at ‘, array([409, 174, 741, 626]), ‘ Score ‘, 0.99797523)<br>(‘Label’, u’person’, ‘ at ‘, array([ 4, 424, 524, 790]), ‘ Score ‘, 0.9549346)<br>(‘Label’, u’person’, ‘ at ‘, array([ 725, 472, 1064, 793]), ‘ Score ‘, 0.8900732)<br>(‘Label’, u’tie’, ‘ at ‘, array([527, 338, 566, 494]), ‘ Score ‘, 0.3943166)<br><strong class="markup--strong markup--blockquote-strong">(‘Time for ‘, 1, ‘ is ‘, 0.6182711124420 → This is higher a model size is reduced and during inference the higher precision coversion has to be done</strong></blockquote><p name="edc9" id="edc9" class="graf graf--p graf-after--blockquote"><em class="markup--em markup--p-em">You should see that the size of the output graph is about a quarter of the original. The downside to this approach compared to round_weights is that extra decompression ops are inserted to convert the eight-bit values back into floating point, but optimizations in TensorFlow’s runtime should ensure these results are cached and so you shouldn’t see the graph run any more slowly.- </em><a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md" data-href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md</a></p><blockquote name="bbc7" id="bbc7" class="graf graf--blockquote graf-after--p"><strong class="markup--strong markup--blockquote-strong">TensorRT FP 16 Converted model</strong><br><br>(‘Label’, u’person’, ‘ at ‘, array([412, 171, 740, 624]), ‘ Score ‘, 0.9980476)<br>(‘Label’, u’person’, ‘ at ‘, array([ 6, 423, 518, 788]), ‘ Score ‘, 0.9493193)<br>(‘Label’, u’person’, ‘ at ‘, array([ 732, 473, 1065, 793]), ‘ Score ‘, 0.8841917)<br>(‘Label’, u’tie’, ‘ at ‘, array([529, 337, 565, 494]), ‘ Score ‘, 0.40442812)<br><strong class="markup--strong markup--blockquote-strong">(‘Time for ‘, 1, ‘ is ‘, 0.5885560512542725) →</strong></blockquote><blockquote name="d251" id="d251" class="graf graf--blockquote graf-after--blockquote">I was hoping this would be half the original value — twice as fast. But during optimization TensorRT was telling it could convert only a few of the supported* operations -<code class="markup--code markup--blockquote-code u-paddingRight0 u-marginRight0"> &quot;<strong class="markup--strong markup--blockquote-strong">There are 3962 ops of 51 different types in the graph that are not converted to TensorRT -Conv2D&quot;</strong></code> though Convolution operation is shown as supported here →<a href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-support-matrix/index.html" data-href="https://docs.nvidia.com/deeplearning/sdk/tensorrt-support-matrix/index.html" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">https://docs.nvidia.com/deeplearning/sdk/tensorrt-support-matrix/index.html</a>. Bug raised for this by me <a href="https://devtalk.nvidia.com/default/topic/1048485/tensorrt/no-speed-up-with-tensorrt-fp16-or-int8-on-nvidia-v100/post/5320989/#5320989" data-href="https://devtalk.nvidia.com/default/topic/1048485/tensorrt/no-speed-up-with-tensorrt-fp16-or-int8-on-nvidia-v100/post/5320989/#5320989" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">here</a></blockquote><pre name="4315" id="4315" class="graf graf--pre graf-after--blockquote"><code class="markup--code markup--pre-code u-paddingRight0 u-marginRight0">2019-04-14 08:32:31.357592: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:<br>2019-04-14 08:32:31.357620: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 <br>2019-04-14 08:32:31.357645: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N <br>2019-04-14 08:32:31.358154: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30480 MB memory) -&gt; physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:b3:00.0, compute capability: 7.0)<br>2019-04-14 08:32:34.582872: I tensorflow/core/grappler/devices.cc:51] Number of eligible GPUs (core count &gt;= 8): 1<br>2019-04-14 08:32:34.583019: I tensorflow/core/grappler/clusters/single_machine.cc:359] Starting new session<br>2019-04-14 08:32:34.583578: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0<br>2019-04-14 08:32:34.583610: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:<br>2019-04-14 08:32:34.583636: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 <br>2019-04-14 08:32:34.583657: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N <br>2019-04-14 08:32:34.583986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30480 MB memory) -&gt; physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:b3:00.0, compute capability: 7.0)<br>2019-04-14 08:32:36.713848: I tensorflow/contrib/tensorrt/segment/segment.cc:443] <strong class="markup--strong markup--pre-strong">There are 3962 ops of 51 different types in the graph that are not converted to TensorRT:</strong> TopKV2, NonMaxSuppressionV2, TensorArrayWriteV3, Const, Squeeze, ResizeBilinear, Maximum, Where, Add, Placeholder, Switch, TensorArrayGatherV3, NextIteration, Greater, TensorArraySizeV3, NoOp, TensorArrayV3, LoopCond, Less, StridedSlice, TensorArrayScatterV3, ExpandDims, Exit, Cast, Identity, Shape, RealDiv, TensorArrayReadV3, Reshape, Merge, Enter, Range, <strong class="markup--strong markup--pre-strong">Conv2D</strong>, Mul, Equal, Sub, Minimum, Tile, Pack, Split, ZerosLike, ConcatV2, Size, Unpack, Assert, DataFormatVecPermute, Transpose, Gather, Exp, Slice, Fill, (For more information see https://docs.nvidia.com/deeplearning/dgx/integrate-tf-trt/index.html#support-ops).<br>2019-04-14 08:32:36.848171: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:913] Number of TensorRT candidate segments: 4<br>2019-04-14 08:32:37.129266: W tensorflow/contrib/tensorrt/convert/convert_nodes.cc:3710] Validation failed for TensorRTInputPH_0 and input slot 0: Input tensor with shape [?,?,?,3] has an unknown non-batch dimension at dim 1<br>2019-04-14 08:32:37.129330: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:1021] TensorRT node TRTEngineOp_0 added for segment 0 consisting of 707 nodes failed: Invalid argument: Validation failed for TensorRTInputPH_0 and input slot 0: Input tensor with shape [?,?,?,3] has an unknown non-batch dimension at dim 1. Fallback to TF...<br>2019-04-14 08:32:37.129838: W tensorflow/contrib/tensorrt/convert/convert_nodes.cc:3710] Validation failed for TensorRTInputPH_0 and input slot 0: Input tensor with shape [?,546,?,?] has an unknown non-batch dimension at dim 2<br>2019-04-14 08:32:37.129859: W tensorflow/contrib/tensorrt/convert/convert_graph.cc:1021] TensorRT node TRTEngineOp_1 added for segment 1 consisting of 3 nodes failed: Invalid argument: Validation failed for TensorRTInputPH_0 and input slot 0: Input tensor with shape [?,546,?,?] has an unknown non-batch dimension at dim 2. Fallback to TF...<br>2019-04-14 08:32:38.309554: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:1015] TensorRT node TRTEngineOp_2 added for segment 2 consisting of 3 nodes succeeded.<br>2019-04-14 08:32:38.420585: I tensorflow/contrib/tensorrt/convert/convert_graph.cc:1015] TensorRT node TRTEngineOp_3 added for segment 3 consisting of 4 nodes succeeded.<br>2019-04-14 08:32:38.644767: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:581] Optimization results for grappler item: tf_graph<br>2019-04-14 08:32:38.644837: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:583]   constant folding: Graph size after: 6411 nodes (-1212), 10503 edges (-1352), time = 848.996ms.<br>2019-04-14 08:32:38.644858: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:583]   layout: Graph size after: 6442 nodes (31), 10535 edges (32), time = 225.361ms.<br>2019-04-14 08:32:38.644874: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:583]   constant folding: Graph size after: 6432 nodes (-10), 10535 edges (0), time = 559.352ms.<br>2019-04-14 08:32:38.644920: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:583]   TensorRTOptimizer: Graph size after: 6427 nodes (-5), 10530 edges (-5), time = 2087.5769ms.</code></pre><blockquote name="9b33" id="9b33" class="graf graf--blockquote graf-after--pre"><strong class="markup--strong markup--blockquote-strong">TensorRT INT 8 Converted model</strong></blockquote><p name="5e29" id="5e29" class="graf graf--p graf-after--blockquote">One can see from the V100 server logs some Tensor Core magic happening</p><pre name="5726" id="5726" class="graf graf--pre graf-after--p">2019–04–20 01:30:39.563827: I external/org_tensorflow/tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:574] Starting calibration thread on device 0, Calibration Resource @ 0x7f4c341ac570<br>2019–04–20 01:30:39.563982: I external/org_tensorflow/tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:574] Starting calibration thread on device 0, Calibration Resource @ 0x7f4ce8008e60</pre><blockquote name="2040" id="2040" class="graf graf--blockquote graf-after--pre">(‘Label’, u’person’, ‘ at ‘, array([412, 171, 740, 624]), ‘ Score ‘, 0.9980476)<br>(‘Label’, u’person’, ‘ at ‘, array([ 6, 423, 518, 788]), ‘ Score ‘, 0.9493195)<br>(‘Label’, u’person’, ‘ at ‘, array([ 732, 473, 1065, 793]), ‘ Score ‘, 0.8841919)<br>(‘Label’, u’tie’, ‘ at ‘, array([529, 337, 565, 494]), ‘ Score ‘, 0.40442798)<br><strong class="markup--strong markup--blockquote-strong">(‘Time for ‘, 1, ‘ is ‘, 0.5967140197753906)</strong></blockquote><p name="a6cc" id="a6cc" class="graf graf--p graf-after--blockquote"><strong class="markup--strong markup--p-strong">With batch size 2 there is an error/ out of memory for TensorCores</strong></p><pre name="6f16" id="6f16" class="graf graf--pre graf-after--p">python ssd_client_1.py -num_tests=1 -server=127.0.0.1:8500 -<strong class="markup--strong markup--pre-strong">batch_size=2 </strong>-img_path=’../examples/000000008021.jpg’<br>2019–04–20 01:34:25.042337: F external/org_tensorflow/tensorflow/contrib/tensorrt/kernels/<strong class="markup--strong markup--pre-strong">trt_engine_op.cc:227] Check failed: t.TotalBytes() == device_tensor-&gt;TotalBytes()</strong> (788424 vs. 394212)<br>2019–04–20 01:34:25.042373: F external/org_tensorflow/tensorflow/contrib/tensorrt/kernels/trt_engine_op.cc:227] Check failed: t.TotalBytes() == device_tensor-&gt;TotalBytes() (34656 vs. 17328)<br>/usr/bin/tf_serving_entrypoint.sh: line 3: 6 Aborted (core dumped)</pre><h4 name="2f78" id="2f78" class="graf graf--h4 graf-after--pre">Results from other models (and Comparison with different GPU’s)</h4><p name="79f5" id="79f5" class="graf graf--p graf-after--h4">Here are some results from other tests and models</p><p name="f91f" id="f91f" class="graf graf--p graf-after--p">Details here — <a href="https://docs.google.com/spreadsheets/d/1Sl7K6sa96wub1OXcneMk1txthQfh63b0H5mwygyVQlE/edit?usp=sharing" data-href="https://docs.google.com/spreadsheets/d/1Sl7K6sa96wub1OXcneMk1txthQfh63b0H5mwygyVQlE/edit?usp=sharing" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://docs.google.com/spreadsheets/d/1Sl7K6sa96wub1OXcneMk1txthQfh63b0H5mwygyVQlE/edit?usp=sharing</a></p><p name="74e6" id="74e6" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Model — Resnet_50 FP 32 and FP16</strong></p><p name="ab67" id="ab67" class="graf graf--p graf-after--p">FP32 = <a href="http://download.tensorflow.org/models/official/20181001_resnet/savedmodels/resnet_v2_fp32_savedmodel_NCHW.tar.gz" data-href="http://download.tensorflow.org/models/official/20181001_resnet/savedmodels/resnet_v2_fp32_savedmodel_NCHW.tar.gz" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">http://download.tensorflow.org/models/official/20181001_resnet/savedmodels/resnet_v2_fp32_savedmodel_NCHW.tar.gz</a></p><p name="6685" id="6685" class="graf graf--p graf-after--p">FP16 = <a href="http://download.tensorflow.org/models/official/20181001_resnet/savedmodels/resnet_v2_fp16_savedmodel_NCHW.tar.gz" data-href="http://download.tensorflow.org/models/official/20181001_resnet/savedmodels/resnet_v2_fp16_savedmodel_NCHW.tar.gz" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">http://download.tensorflow.org/models/official/20181001_resnet/savedmodels/resnet_v2_fp16_savedmodel_NCHW.tar.gz</a></p><figure name="0d5b" id="0d5b" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 529px; max-height: 332px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 62.8%;"></div><img class="graf-image" data-image-id="1*2JBhiRShdRSl0_CU5qCU9g.png" data-width="529" data-height="332" src="https://cdn-images-1.medium.com/max/800/1*2JBhiRShdRSl0_CU5qCU9g.png"></div><figcaption class="imageCaption">Resnet 50</figcaption></figure><p name="8e18" id="8e18" class="graf graf--p graf-after--figure">You can see that there is a slight difference, V100 32 GB takes slightly less time than the consumer grade GTX 1070 8GB, when the batch size increases the more memory resource of V100 stands out; but not the number of CUDA cores. It seems as is noted in other blogs, that simply having more CUDA cores does not automatically mean that an inference will run faster. It may depend on memory and the model characteristics also.</p><p name="231e" id="231e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Model Retinanet</strong></p><figure name="9051" id="9051" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 530px; max-height: 307px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 57.9%;"></div><img class="graf-image" data-image-id="1*wGj0FVEzjXUmpGj3E3gb4w.png" data-width="530" data-height="307" src="https://cdn-images-1.medium.com/max/800/1*wGj0FVEzjXUmpGj3E3gb4w.png"></div></figure><p name="1a80" id="1a80" class="graf graf--p graf-after--figure">One can see here that there is not much difference. Actually, this was my first experiment, but this was a Keras model that was converted to a TF frozen model and optimised. So I thought maybe I would get better results from a pure TF written model like SSD. But did not make much difference.</p><h4 name="b675" id="b675" class="graf graf--h4 graf-after--p"><strong class="markup--strong markup--h4-strong">Summary</strong></h4><p name="6169" id="6169" class="graf graf--p graf-after--h4">One can see that there are no drastic improvements in the inference time between the models. Also, TF GraphTransform for Model Quantization has not worked for me in this nor one other model I tried. Will raise a bug for that.TensorRT is better but is only able to convert a few layers to lower precision- have raised a bug/clarification for this, and if that works, hopefully, we can see the models runs twice as fast as advertised in Tensor Cores.</p><h4 name="4f1e" id="4f1e" class="graf graf--h4 graf-after--p">Main References</h4><p name="114d" id="114d" class="graf graf--p graf-after--h4"><a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md" data-href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tools/graph_transforms/README.md</a></p><p name="281e" id="281e" class="graf graf--p graf-after--p"><a href="https://medium.com/google-cloud/optimizing-tensorflow-models-for-serving-959080e9ddbf" data-href="https://medium.com/google-cloud/optimizing-tensorflow-models-for-serving-959080e9ddbf" class="markup--anchor markup--p-anchor" target="_blank">https://medium.com/google-cloud/optimizing-tensorflow-models-for-serving-959080e9ddbf</a></p><p name="3713" id="3713" class="graf graf--p graf-after--p"><a href="https://colab.research.google.com/drive/1wQpWoc40kf__WSjfTqDaReMx6fFjUn48" data-href="https://colab.research.google.com/drive/1wQpWoc40kf__WSjfTqDaReMx6fFjUn48" class="markup--anchor markup--p-anchor" rel="nofollow noopener noopener" target="_blank">https://colab.research.google.com/drive/1wQpWoc40kf__WSjfTqDaReMx6fFjUn48</a></p><p name="5e25" id="5e25" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Other related posts</strong></p><div name="4d85" id="4d85" class="graf graf--mixtapeEmbed graf-after--p"><a href="https://towardsdatascience.com/productising-tensorflow-keras-models-via-tensorflow-serving-69e191cb1f37" data-href="https://towardsdatascience.com/productising-tensorflow-keras-models-via-tensorflow-serving-69e191cb1f37" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://towardsdatascience.com/productising-tensorflow-keras-models-via-tensorflow-serving-69e191cb1f37"><strong class="markup--strong markup--mixtapeEmbed-strong">Writing a Generic Tensorflow Serving Client for Tensorflow Serving model</strong><br><em class="markup--em markup--mixtapeEmbed-em">For CNN based object detection models</em>towardsdatascience.com</a><a href="https://towardsdatascience.com/productising-tensorflow-keras-models-via-tensorflow-serving-69e191cb1f37" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="97594ea6918742a3a10674c98b15e039" data-thumbnail-img-id="1*cntKPCdIsck5ia2jDrhg9w.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*cntKPCdIsck5ia2jDrhg9w.png);"></a></div><div name="13c9" id="13c9" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed"><a href="https://towardsdatascience.com/using-tensorflow-serving-grpc-38a722451064" data-href="https://towardsdatascience.com/using-tensorflow-serving-grpc-38a722451064" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://towardsdatascience.com/using-tensorflow-serving-grpc-38a722451064"><strong class="markup--strong markup--mixtapeEmbed-strong">Using Tensorflow Serving GRPC</strong><br><em class="markup--em markup--mixtapeEmbed-em">Once you have your Tensorflow or Keras based model trained, one needs to think on how to use it in production. You may…</em>towardsdatascience.com</a><a href="https://towardsdatascience.com/using-tensorflow-serving-grpc-38a722451064" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="2c3a019684fbe53c06f39167c38b1273" data-thumbnail-img-id="0*VwrMbE3uAnRdS3n0" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*VwrMbE3uAnRdS3n0);"></a></div><div name="67de" id="67de" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed graf--trailing"><a href="https://medium.com/techlogs/running-tensor-flow-and-keras-in-jupyter-notebook-via-docker-b4e0999b8c8a" data-href="https://medium.com/techlogs/running-tensor-flow-and-keras-in-jupyter-notebook-via-docker-b4e0999b8c8a" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/techlogs/running-tensor-flow-and-keras-in-jupyter-notebook-via-docker-b4e0999b8c8a"><strong class="markup--strong markup--mixtapeEmbed-strong">Running Tensor Flow and Keras in Jupyter notebook via Docker</strong><br><em class="markup--em markup--mixtapeEmbed-em">Assuming that you have NVIDIA GPU in your machine and NVIDIA Drivers installed. Please read the below before you install…</em>medium.com</a><a href="https://medium.com/techlogs/running-tensor-flow-and-keras-in-jupyter-notebook-via-docker-b4e0999b8c8a" class="js-mixtapeImage mixtapeImage mixtapeImage--empty u-ignoreBlock" data-media-id="954fe3f498b37160f6dfa0e48cae71b1"></a></div></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@alexcpn" class="p-author h-card">Alex Punnen</a> on <a href="https://medium.com/p/1cc190cafe1f"><time class="dt-published" datetime="2019-04-25T15:52:18.473Z">April 25, 2019</time></a>.</p><p><a href="https://medium.com/@alexcpn/optimizing-any-tensorflow-model-using-tensorflow-transform-tools-and-using-tensorrt-1cc190cafe1f" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on August 22, 2019.</p></footer></article></body></html>