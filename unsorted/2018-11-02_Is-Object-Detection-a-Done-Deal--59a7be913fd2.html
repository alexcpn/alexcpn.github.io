<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Is Object Detection a Done Deal?</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Is Object Detection a Done Deal?</h1>
</header>
<section data-field="subtitle" class="p-summary">
A few years back it was widely known that Object Detection was a hard problem to solve. The comic below was just a few years back. Things…
</section>
<section data-field="body" class="e-content">
<section name="62d3" class="section section--body section--first"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="4273" id="4273" class="graf graf--h3 graf--leading graf--title">CNN based Object Detection - Current Challenges</h3><p name="cd0f" id="cd0f" class="graf graf--p graf-after--h3">A few years back it was widely known that Object Detection was a hard problem to solve. The comic below was just a few years back. Things have changed in this short time quite drastically.</p><figure name="c3ef" id="c3ef" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 267px; max-height: 448px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 167.79999999999998%;"></div><img class="graf-image" data-image-id="0*GoXh814mdyi4aDul" data-width="267" data-height="448" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/0*GoXh814mdyi4aDul"></div><figcaption class="imageCaption">comic by <a href="http://xkcd.com/1425/" data-href="http://xkcd.com/1425/" class="markup--anchor markup--figure-anchor" rel="noopener" target="_blank">XKCD</a></figcaption></figure><p name="b6f4" id="b6f4" class="graf graf--p graf-after--figure">With the advent of Deep Neural Network Architecture -Convolutional Neural Network (CNN) in particular, as well as the development of the CUDA library that started to use the multicore characteristic of the gaming/rendering GPU’s and the open collaborative research, things have changed drastically for the better.</p><p name="af54" id="af54" class="graf graf--p graf-after--p">Not only would it be possible to recognize a bird, but which bird as well. Here is a snapshot from Google Cloud API Demo link — <a href="https://cloud.google.com/vision/docs/drag-and-drop" data-href="https://cloud.google.com/vision/docs/drag-and-drop" class="markup--anchor markup--p-anchor" rel="nofollow noopener noopener" target="_blank">https://cloud.google.com/vision/docs/drag-and-drop</a></p><figure name="4ddd" id="4ddd" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 632px; max-height: 375px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 59.3%;"></div><img class="graf-image" data-image-id="1*tY2U95U5Tf_wyiFlvImPFw.png" data-width="632" data-height="375" src="https://cdn-images-1.medium.com/max/800/1*tY2U95U5Tf_wyiFlvImPFw.png"></div></figure><p name="bd0b" id="bd0b" class="graf graf--p graf-after--figure">The Google vision API is 99 percent confident that the image is of a bird, and 75% confident of which family of birds it belongs to !! <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">Coraciiformes</em></strong><em class="markup--em markup--p-em"> are a group of usually colorful </em><a href="https://en.wikipedia.org/wiki/Bird" data-href="https://en.wikipedia.org/wiki/Bird" class="markup--anchor markup--p-anchor" title="Bird" rel="noopener" target="_blank"><em class="markup--em markup--p-em">birds</em></a><em class="markup--em markup--p-em"> including the </em><a href="https://en.wikipedia.org/wiki/Kingfisher" data-href="https://en.wikipedia.org/wiki/Kingfisher" class="markup--anchor markup--p-anchor" title="Kingfisher" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">kingfishers</em></strong></a><em class="markup--em markup--p-em">, the </em><a href="https://en.wikipedia.org/wiki/Bee-eater" data-href="https://en.wikipedia.org/wiki/Bee-eater" class="markup--anchor markup--p-anchor" title="Bee-eater" rel="noopener" target="_blank"><em class="markup--em markup--p-em">bee-eaters</em></a><em class="markup--em markup--p-em">, the </em><a href="https://en.wikipedia.org/wiki/Roller" data-href="https://en.wikipedia.org/wiki/Roller" class="markup--anchor markup--p-anchor" title="Roller" rel="noopener" target="_blank"><em class="markup--em markup--p-em">rollers</em></a><em class="markup--em markup--p-em">, the </em><a href="https://en.wikipedia.org/wiki/Motmot" data-href="https://en.wikipedia.org/wiki/Motmot" class="markup--anchor markup--p-anchor" title="Motmot" rel="noopener" target="_blank"><em class="markup--em markup--p-em">motmots</em></a><em class="markup--em markup--p-em">, and the </em><a href="https://en.wikipedia.org/wiki/Tody" data-href="https://en.wikipedia.org/wiki/Tody" class="markup--anchor markup--p-anchor" title="Tody" rel="noopener" target="_blank"><em class="markup--em markup--p-em">todies</em></a><em class="markup--em markup--p-em"> — </em><a href="https://en.wikipedia.org/wiki/Coraciiformes" data-href="https://en.wikipedia.org/wiki/Coraciiformes" class="markup--anchor markup--p-anchor" rel="nofollow noopener noopener" target="_blank">https://en.wikipedia.org/wiki/Coraciiformes</a>)</p><p name="e39e" id="e39e" class="graf graf--p graf-after--p">Things that would have taken a 15-year research team to do are now rapidly becoming reality. But there are caveats here.</p><h4 name="b24c" id="b24c" class="graf graf--h4 graf-after--p">All is not so Great</h4><p name="bb1c" id="bb1c" class="graf graf--p graf-after--h4">I am not a researcher but have been basically using open source algorithms and frameworks for Object detection for about two years now. Started from the ML-based HOG and HAAR in OpenCV, then the faster version of that via CUDA and GPU and finally since tuning the parameters of these systems to works across different videos was proving to be futile, went ahead with the neural network based method; I wanted to write this as there is a tendency by many who have used the opensource implementations like Yolo, to think that it is a done deal; also heavy marketing by a lot of small and specialized companies, who follow similar thinking, and promising visual automation, either customizable or customized for some vertical. (does it remind one of <a href="https://gizmodo.com/why-everyone-is-hating-on-watson-including-the-people-w-1797510888" data-href="https://gizmodo.com/why-everyone-is-hating-on-watson-including-the-people-w-1797510888" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">IBM Watson marketing</a> and the place it occupies now).</p><p name="06ac" id="06ac" class="graf graf--p graf-after--p">Maybe when we humans see a system is able to detect and/or classify some images perfectly, we have a tendency to imagine and extend the capability to all scenarios; because we, humans, are great in generalizing; and with CNN we have something similar, better in generalizing features, but nowhere great yet. Read on.</p><h4 name="b475" id="b475" class="graf graf--h4 graf-after--p">The Good</h4><p name="e9b4" id="e9b4" class="graf graf--p graf-after--h4">Here is a result from a photo I took a while back; I used the Google <a href="https://cloud.google.com/vision/?utm_source=google&amp;utm_medium=cpc&amp;utm_campaign=japac-IN-all-en-dr-bkws-all-super-trial-e-dr-1003987&amp;utm_content=text-ad-none-none-DEV_c-CRE_256563224835-ADGP_Hybrid+%7C+AW+SEM+%7C+BKWS+~+T1+%7C+EXA+%7C+ML+%7C+1:1+%7C+IN+%7C+en+%7C+Vision+%7C+google+cloud+vision+api-KWID_43700023274811677-kwd-314933838631&amp;userloc_9062039&amp;utm_term=KW_google%20cloud%20vision%20api&amp;ds_rl=1264446&amp;gclid=Cj0KCQjwguDeBRDCARIsAGxuU8askpUZb4_vPr5nHGn_Z21eZoey7UNSPa2OX9fZz7RM4yuyxT1ZyywaAkkdEALw_wcB" data-href="https://cloud.google.com/vision/?utm_source=google&amp;utm_medium=cpc&amp;utm_campaign=japac-IN-all-en-dr-bkws-all-super-trial-e-dr-1003987&amp;utm_content=text-ad-none-none-DEV_c-CRE_256563224835-ADGP_Hybrid+%7C+AW+SEM+%7C+BKWS+~+T1+%7C+EXA+%7C+ML+%7C+1:1+%7C+IN+%7C+en+%7C+Vision+%7C+google+cloud+vision+api-KWID_43700023274811677-kwd-314933838631&amp;userloc_9062039&amp;utm_term=KW_google%20cloud%20vision%20api&amp;ds_rl=1264446&amp;gclid=Cj0KCQjwguDeBRDCARIsAGxuU8askpUZb4_vPr5nHGn_Z21eZoey7UNSPa2OX9fZz7RM4yuyxT1ZyywaAkkdEALw_wcB" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Cloud Vision API Demo page</a> to upload and check. I choose Google API because they are the best in this/ or one of the best. Wow! the results are amazing; not only has it detected that it is a Flower with 86 percent confidence (did it miss the beetle?), but it has also correctly identified the family <a href="https://en.wikipedia.org/wiki/Morning_glory" data-href="https://en.wikipedia.org/wiki/Morning_glory" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Morning Glory Family</a>! This is some information! I am really impressed. Anyone would be. Same like IBM <a href="https://en.wikipedia.org/wiki/Watson_%28computer%29" data-href="https://en.wikipedia.org/wiki/Watson_(computer)" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Watson </a>beating the human contenders in Jeopardy, or the Go playing <a href="https://en.wikipedia.org/wiki/AlphaGo" data-href="https://en.wikipedia.org/wiki/AlphaGo" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">AlphaMind </a>from Google DeepMind.</p><figure name="fbcb" id="fbcb" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 336px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 48%;"></div><img class="graf-image" data-image-id="1*txnUoE07WB987nPSSdn9tQ.png" data-width="902" data-height="433" src="https://cdn-images-1.medium.com/max/800/1*txnUoE07WB987nPSSdn9tQ.png"></div></figure><p name="28a6" id="28a6" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">When we see output like this our expectation increases exponentially.</strong> <strong class="markup--strong markup--p-strong">We tend to equate the system with human-like abilities for vision, with computer-like fastness and correlation to digitized information</strong>. The perfect marriage. Imagine what a trained network can do in medical scans. Every vision related problem seems to be generalized as a possibility and then automated and augmented with information to create a system.</p><p name="02b8" id="02b8" class="graf graf--p graf-after--p">This is partly true, but there are gaps, large gaps, not unbridgeable, but which requires work. Let us see a few.</p><h4 name="3ed2" id="3ed2" class="graf graf--h4 graf-after--p">The Bad &amp; The U..ly</h4><p name="b91c" id="b91c" class="graf graf--p graf-after--h4"><em class="markup--em markup--p-em">Note that in the technical architecture of CNN there is more complexity in </em><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">object detection</em></strong><em class="markup--em markup--p-em"> than </em><strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">object classification</em></strong><em class="markup--em markup--p-em">. Image classifiers have very high accuracy in test’s compared to detectors (they need to also detect the position of the object and draw a bounding box on the image). The Google Vision API is doing image classification.</em></p><p name="70fb" id="70fb" class="graf graf--p graf-after--p">A system that can classify a flower into its family would surely be able to decipher the details from the below photo?</p><figure name="a9b7" id="a9b7" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 334px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 47.699999999999996%;"></div><img class="graf-image" data-image-id="1*C6dwZoSQteRdTu94J240sA.png" data-width="883" data-height="421" src="https://cdn-images-1.medium.com/max/800/1*C6dwZoSQteRdTu94J240sA.png"></div></figure><p name="c0ce" id="c0ce" class="graf graf--p graf-after--figure">However, not a single Car is ‘detected’ (technically this is a classifier). Let us give another clearer shot to the system.</p></div><div class="section-inner sectionLayout--outsetColumn"><figure name="e2c0" id="e2c0" class="graf graf--figure graf--layoutOutsetCenter graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 898px; max-height: 426px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 47.4%;"></div><img class="graf-image" data-image-id="1*keEclBSYMFMdcqM2F3xG6g.png" data-width="898" data-height="426" src="https://cdn-images-1.medium.com/max/1200/1*keEclBSYMFMdcqM2F3xG6g.png"></div></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="4c90" id="4c90" class="graf graf--p graf-after--figure">Still, no Car detected. Let us make the car/cars slightly bigger</p><figure name="0a8c" id="0a8c" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder"><img class="graf-image" data-image-id="1*3K5bQstV6TPBhG2tvtkIGw.png" src="https://cdn-images-1.medium.com/max/800/1*3K5bQstV6TPBhG2tvtkIGw.png"></div></figure><p name="f571" id="f571" class="graf graf--p graf-after--figure">Now it is detecting cars with 96% confidence. Why is that?</p><h4 name="6ee1" id="6ee1" class="graf graf--h4 graf-after--p">Scale Invariance</h4><p name="4197" id="4197" class="graf graf--p graf-after--h4">Let us make the car a lot bigger</p></div><div class="section-inner sectionLayout--outsetRow" data-paragraph-count="2"><figure name="7524" id="7524" class="graf graf--figure graf--layoutOutsetRow is-partialWidth graf-after--p" style="width: 68.386%;"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 46.800000000000004%;"></div><img class="graf-image" data-image-id="1*RZMpr-Cl2FFC8MfEVBVvhQ.png" data-width="888" data-height="416" src="https://cdn-images-1.medium.com/max/800/1*RZMpr-Cl2FFC8MfEVBVvhQ.png"></div></figure><figure name="3431" id="3431" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 31.614%;"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 101.29999999999998%;"></div><img class="graf-image" data-image-id="1*Xl65gF_nY3BlOc1zv8cLNg.png" data-width="384" data-height="389" src="https://cdn-images-1.medium.com/max/400/1*Xl65gF_nY3BlOc1zv8cLNg.png"></div></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="fdb6" id="fdb6" class="graf graf--p graf-after--figure">It not only is 99% sure that it is a car but also detects it is a Toyota Corolla with 52% confidence.</p><p name="6fe7" id="6fe7" class="graf graf--p graf-after--p">So if the car image is small, like from an areal shot, it is not able to detect at all and if it is of a reasonable size it is 99 percent confident that it is a car. <strong class="markup--strong markup--p-strong">Are CNN’s really scale invariant? Short answer -No. </strong>Max pooling in the CNN helps a bit; but not for large changes. Read on for more explanation.</p><blockquote name="7d90" id="7d90" class="graf graf--blockquote graf-after--p">Convolution is<strong class="markup--strong markup--blockquote-strong"> not naturally</strong> <strong class="markup--strong markup--blockquote-strong">equivariant</strong> to some other transformations, <strong class="markup--strong markup--blockquote-strong">such as changes in the scale or rotation of an image. — </strong><em class="markup--em markup--blockquote-em">Deep Learning </em>book by <a href="https://en.wikipedia.org/wiki/Ian_Goodfellow" data-href="https://en.wikipedia.org/wiki/Ian_Goodfellow" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">Ian Goodfellow</a> and <a href="https://en.wikipedia.org/wiki/Yoshua_Bengio" data-href="https://en.wikipedia.org/wiki/Yoshua_Bengio" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">Yoshua Bengio</a> and <a href="https://scholar.google.com/citations?user=km6CP8cAAAAJ&amp;hl=en" data-href="https://scholar.google.com/citations?user=km6CP8cAAAAJ&amp;hl=en" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">Aaron Courvill</a></blockquote><h4 name="f740" id="f740" class="graf graf--h4 graf-after--blockquote">Rotation Invariance</h4><p name="d76e" id="d76e" class="graf graf--p graf-after--h4"><strong class="markup--strong markup--p-strong">CNN&#39;s (or the current CNN networks) are not rotation invariant, </strong>it has to be trained for that (data augmentation) to get the effect. Even in a highly trained NW like Google Vision API we can see some effects of this. Let us do a small change in the above test image of a car; I invert the above car picture and try.</p></div><div class="section-inner sectionLayout--outsetColumn"><figure name="ee5a" id="ee5a" class="graf graf--figure graf--layoutOutsetCenter graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 797px; max-height: 573px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 71.89999999999999%;"></div><img class="graf-image" data-image-id="1*YSp8SZVum4heuXDpEkvLHw.png" data-width="797" data-height="573" src="https://cdn-images-1.medium.com/max/1200/1*YSp8SZVum4heuXDpEkvLHw.png"></div></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="359f" id="359f" class="graf graf--p graf-after--figure">It is detecting as a Car with high accuracy still; no problem with that; The car make -Toyota is now detected wrongly as BMW (not visible in above but you can see the same in the picture below)</p><p name="3437" id="3437" class="graf graf--p graf-after--p">I rotate it still 90 degrees.</p><figure name="3740" id="3740" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder"><img class="graf-image" data-image-id="1*k2nGZjlq40hfzReD0HRi7Q.png" src="https://cdn-images-1.medium.com/max/800/1*k2nGZjlq40hfzReD0HRi7Q.png"></div></figure><p name="153e" id="153e" class="graf graf--p graf-after--figure">And then it loses the BMW confidence too, but Wheels and other auto parts are gaining in confidence.</p><p name="6282" id="6282" class="graf graf--p graf-after--p">This will be more evident with images that are less common. Here is an image of a tap rotated 90 degrees and you could see the confidence changing (Chair ?)</p></div><div class="section-inner sectionLayout--outsetRow" data-paragraph-count="2"><figure name="e5be" id="e5be" class="graf graf--figure graf--layoutOutsetRow is-partialWidth graf-after--p" style="width: 44.202%;"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 61%;"></div><img class="graf-image" data-image-id="1*HiKhyvvO42RqVAYdW7KAMw.png" data-width="926" data-height="565" src="https://cdn-images-1.medium.com/max/600/1*HiKhyvvO42RqVAYdW7KAMw.png"></div></figure><figure name="a6ed" id="a6ed" class="graf graf--figure graf--layoutOutsetRowContinue is-partialWidth graf-after--figure" style="width: 55.798%;"><div class="aspectRatioPlaceholder is-locked"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 48.3%;"></div><img class="graf-image" data-image-id="1*LP3QaTcTTIXSooYTB5RxuQ.png" data-width="873" data-height="422" src="https://cdn-images-1.medium.com/max/600/1*LP3QaTcTTIXSooYTB5RxuQ.png"></div></figure></div><div class="section-inner sectionLayout--insetColumn"><p name="6aa3" id="6aa3" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">During training, each image is usually </strong><a href="https://medium.com/nanonets/how-to-use-deep-learning-when-you-have-limited-data-part-2-data-augmentation-c26971dc8ced" data-href="https://medium.com/nanonets/how-to-use-deep-learning-when-you-have-limited-data-part-2-data-augmentation-c26971dc8ced" class="markup--anchor markup--p-anchor" target="_blank"><strong class="markup--strong markup--p-strong">augmented</strong></a><strong class="markup--strong markup--p-strong"> via transformations to avoid these sort of errors, minimize translation invariance.</strong></p><p name="4998" id="4998" class="graf graf--p graf-after--p">But as you can see, when the angle changes from the training set, due to camera angle or taking random pictures from the wild, the output changes too. I have just used random images; my aim here is not to show how bad the system is and confuse the system with difficult images, but to point out certain aspects.</p><p name="e636" id="e636" class="graf graf--p graf-after--p graf--trailing">This may seem surprising to many. CNN’s are supposed to be scale invariant and translation and rotation invariant. Or is that just loose trade talk. Going slightly technical, let’s dig further</p></div></div></section><section name="29e3" class="section section--body section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="6aee" id="6aee" class="graf graf--p graf--leading">There is a general conception that <a href="https://www.google.co.in/search?q=cnn+pooling&amp;oq=CNN+pooling&amp;aqs=chrome.0.0l6.8295j0j7&amp;sourceid=chrome&amp;ie=UTF-8" data-href="https://www.google.co.in/search?q=cnn+pooling&amp;oq=CNN+pooling&amp;aqs=chrome.0.0l6.8295j0j7&amp;sourceid=chrome&amp;ie=UTF-8" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Pooling</a> (Max Pooling) provides scale and translation invariance. This is both true and false. What needs to be understood that pooling helps in ‘<strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">learning</em></strong>’ invariance, and for learning the NW should be trained with images. <strong class="markup--strong markup--p-strong">Also, CNN’s are invariant to translation</strong>. I guess there are few who think this means invariant to rotation also. But translation here means shifting the position of the object left, right , up or down. (shown <a href="https://stats.stackexchange.com/a/208949/191675" data-href="https://stats.stackexchange.com/a/208949/191675" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">here</a> clearly, the picture below)</p><figure name="9ae3" id="9ae3" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 567px; max-height: 769px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 135.60000000000002%;"></div><img class="graf-image" data-image-id="0*4-I1dHiMVTUbf7MI.png" data-width="567" data-height="769" src="https://cdn-images-1.medium.com/max/800/0*4-I1dHiMVTUbf7MI.png"></div><figcaption class="imageCaption">source <a href="https://stats.stackexchange.com/a/208949/191675" data-href="https://stats.stackexchange.com/a/208949/191675" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">https://medium.com/r/?url=https%3A%2F%2Fstats.stackexchange.com%2Fa%2F208949%2F191675</a></figcaption></figure><p name="df75" id="df75" class="graf graf--p graf-after--figure"><a href="https://www.quora.com/What-is-max-pooling-in-convolutional-neural-networks" data-href="https://www.quora.com/What-is-max-pooling-in-convolutional-neural-networks" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Maxpooling</a> helps here. This <a href="https://dsp.stackexchange.com/a/29154/28800" data-href="https://dsp.stackexchange.com/a/29154/28800" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">answer</a> illustrates this lucidly. Here no data augmentation is needed. Assuming that a CNN is good in detecting a picture of a cat, it will detect a cat <em class="markup--em markup--p-em">translated</em> anywhere in the frame.</p><p name="d482" id="d482" class="graf graf--p graf-after--p">Here is from a very reputed source the <em class="markup--em markup--p-em">Deep Learning </em>book by <a href="https://en.wikipedia.org/wiki/Ian_Goodfellow" data-href="https://en.wikipedia.org/wiki/Ian_Goodfellow" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Ian Goodfellow</a> and <a href="https://en.wikipedia.org/wiki/Yoshua_Bengio" data-href="https://en.wikipedia.org/wiki/Yoshua_Bengio" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Yoshua Bengio</a> and <a href="https://scholar.google.com/citations?user=km6CP8cAAAAJ&amp;hl=en" data-href="https://scholar.google.com/citations?user=km6CP8cAAAAJ&amp;hl=en" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Aaron Courvill</a>. Along with <a href="https://en.wikipedia.org/wiki/Geoffrey_Hinton" data-href="https://en.wikipedia.org/wiki/Geoffrey_Hinton" class="markup--anchor markup--p-anchor" title="Geoffrey Hinton" rel="noopener" target="_blank">Geoffrey Hinton</a> and <a href="https://en.wikipedia.org/wiki/Yann_LeCun" data-href="https://en.wikipedia.org/wiki/Yann_LeCun" class="markup--anchor markup--p-anchor" title="Yann LeCun" rel="noopener" target="_blank">Yann Lecun</a>, Bengio is considered one of the three people most responsible for the advancement of deep learning during the 1990s and 2000s”</p><blockquote name="5090" id="5090" class="graf graf--blockquote graf-after--p">In all cases, pooling helps to make the representation<strong class="markup--strong markup--blockquote-strong"> approximately invariant </strong>to <strong class="markup--strong markup--blockquote-strong">small</strong> <strong class="markup--strong markup--blockquote-strong">translations</strong> of the input. Invariance to translation means that if we translate the input by a small amount, the values of most of the pooled outputs do not change — Deep Learning book , <a href="http://www.deeplearningbook.org/" data-href="http://www.deeplearningbook.org/" class="markup--anchor markup--blockquote-anchor" rel="nofollow noopener noopener noopener noopener" target="_blank">http://www.deeplearningbook.org/</a></blockquote><p name="ffdf" id="ffdf" class="graf graf--p graf-after--blockquote"><strong class="markup--strong markup--p-strong">But regarding scale invariance and rotation invariance; here is from the same book</strong></p><blockquote name="ce82" id="ce82" class="graf graf--blockquote graf-after--p">Convolution is<strong class="markup--strong markup--blockquote-strong"> not naturally</strong> <strong class="markup--strong markup--blockquote-strong">equivariant</strong> to some other transformations, <strong class="markup--strong markup--blockquote-strong">such as changes in the scale or rotation of an image.</strong></blockquote><p name="c5c2" id="c5c2" class="graf graf--p graf-after--blockquote">And there are other papers that have tested current networks and reported the same. Here is a quote from a Dec 2017 paper (<a href="http://.https://arxiv.org/pdf/1801.01450.pdf" data-href="http://.https://arxiv.org/pdf/1801.01450.pdf" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">2</a>)</p><blockquote name="3b87" id="3b87" class="graf graf--blockquote graf--startsWithDoubleQuote graf-after--p">“We obtain the surprising result that architectural choices such as the number of pooling layers and the convolution filter size have only a secondary effect on the translation invariance of a network. <strong class="markup--strong markup--blockquote-strong">Our analysis identifies training data augmentation as the most important factor in obtaining translation-invariant representations of images using convolutional neural networks</strong>.” From <em class="markup--em markup--blockquote-em">“Quantifying Translation-Invariance in Convolutional Neural Networks (Eric Kauderer-Abrams Stanford University) “</em></blockquote><p name="08d5" id="08d5" class="graf graf--p graf-after--blockquote">And from another recent paper May 2018</p><blockquote name="2f1a" id="2f1a" class="graf graf--blockquote graf-after--p">Deep convolutional neural networks (CNNs) have revolutionized computer vision. P<strong class="markup--strong markup--blockquote-strong">erhaps the most dramatic success is in the area of object recognition, where performance is now described as “superhuman”</strong> [20]. …</blockquote><blockquote name="9e74" id="9e74" class="graf graf--blockquote graf-after--blockquote">Despite the excellent performance of CNNs on object recognition, the vulnerability to adversarial attacks <strong class="markup--strong markup--blockquote-strong">suggests that superficial changes can result in highly non-human shifts in prediction</strong> …</blockquote><blockquote name="142a" id="142a" class="graf graf--blockquote graf-after--blockquote">Obviously, not any data augmentation is sufficient for the networks to learn invariances. To understand the failure of data augmentation, it is again instructive to consider the subsampling factor. <strong class="markup--strong markup--blockquote-strong">Since in modern networks the subsampling factor is approximately 45, then for a system to learn complete invariance to translation only, it would need to see 452 = 2025 augmented versions of each training example</strong>. If we also add invariance to rotations and scalings, the number grows exponentially with the number of irrelevant transformations</blockquote><blockquote name="6490" id="6490" class="graf graf--blockquote graf-after--blockquote"><em class="markup--em markup--blockquote-em">From </em><strong class="markup--strong markup--blockquote-strong">Why do deep convolutional networks generalize so poorly to small image transformations? </strong>Yair Weiss, Aharon Azulay ELSC Hebrew University of Jerusalem <a href="https://arxiv.org/pdf/1805.12177.pdf" data-href="https://arxiv.org/pdf/1805.12177.pdf" class="markup--anchor markup--blockquote-anchor" rel="nofollow noopener noopener noopener" target="_blank">https://arxiv.org/pdf/1805.12177.pdf</a></blockquote><figure name="2eaa" id="2eaa" class="graf graf--figure graf--iframe graf-after--blockquote"><iframe src="https://www.youtube.com/embed/M4ys8c2NtsE?feature=oembed" width="700" height="393" frameborder="0" scrolling="no"></iframe></figure><p name="bbf5" id="bbf5" class="graf graf--p graf-after--figure">If that is the case, how do the Google API been able to recognizance the inverted and rotated car in the tests that we showed earlier ? (notice that it got the car pretty high, and only missed on the other details like brand, which it may have not trained that strong).</p><h4 name="0cd6" id="0cd6" class="graf graf--h4 graf-after--p">Data Augmentation is the Key</h4><p name="58e6" id="58e6" class="graf graf--p graf-after--h4">The key is data augmentation. Basically the input image is used along with rotations, scaling, noise etc generated from the image as other images to the training. Some good explanation is here <a href="https://medium.com/ymedialabs-innovation/data-augmentation-techniques-in-cnn-using-tensorflow-371ae43d5be9" data-href="https://medium.com/ymedialabs-innovation/data-augmentation-techniques-in-cnn-using-tensorflow-371ae43d5be9" class="markup--anchor markup--p-anchor" rel="nofollow" target="_blank">https://medium.com/ymedialabs-innovation/data-augmentation-techniques-in-cnn-using-tensorflow-371ae43d5be9</a>.</p><p name="2c10" id="2c10" class="graf graf--p graf-after--p">CNN’s are scale invariant to some level,<strong class="markup--strong markup--p-strong"> if it is trained to be</strong>; as pooling implementation will then be able to handle that<strong class="markup--strong markup--p-strong">. Also rotational invariance has to be trained in.</strong></p><p name="744a" id="744a" class="graf graf--p graf--startsWithSingleQuote graf-after--p"><strong class="markup--strong markup--p-strong">‘Learning’ Invariance to Rotation via Pooling</strong></p><p name="c630" id="c630" class="graf graf--p graf-after--p">Let us see rotational invariance first , how a CNN can be trained for that first as it is bit easier. Here is the illustration from the Deep Learning book.</p><figure name="3edc" id="3edc" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 556px; max-height: 325px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 58.5%;"></div><img class="graf-image" data-image-id="1*JM5MUg5yetVF9z8xmkZyuw.png" data-width="556" data-height="325" src="https://cdn-images-1.medium.com/max/800/1*JM5MUg5yetVF9z8xmkZyuw.png"></div><figcaption class="imageCaption">source pg 338 <a href="http://www.deeplearningbook.org/contents/convnets.html" data-href="http://www.deeplearningbook.org/contents/convnets.html" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">http://www.deeplearningbook.org/contents/convnets.html</a></figcaption></figure><blockquote name="52d3" id="52d3" class="graf graf--blockquote graf-after--figure">Example of learned invariances. A pooling unit that pools over multiple features that are learned with separate parameters can learn to be invariant to transformations of the input. Here we show how a set of <strong class="markup--strong markup--blockquote-strong">three learned ﬁlters</strong> and a max pooling unit <strong class="markup--strong markup--blockquote-strong">can learn to become invariant to rotation</strong>. All three ﬁlters are intended to detect a hand written 5.Each ﬁlter attempts to match a slightly diﬀerent orientation of the 5. When a 5 appears in the input, the corresponding ﬁlter will match it and cause a large activation in a detector unit. The<strong class="markup--strong markup--blockquote-strong"> max pooling unit then has a large activation regardless of which detector unit was activated</strong>…</blockquote><blockquote name="3cd0" id="3cd0" class="graf graf--blockquote graf-after--blockquote">pg 338 <a href="http://www.deeplearningbook.org/contents/convnets.html" data-href="http://www.deeplearningbook.org/contents/convnets.html" class="markup--anchor markup--blockquote-anchor" rel="nofollow noopener" target="_blank">http://www.deeplearningbook.org/contents/convnets.html</a></blockquote><p name="2e8a" id="2e8a" class="graf graf--p graf-after--blockquote">Basically we need to either augment the training images by rotating or get a data pool of images which are taken at different angles and use them for training the CNN. (also see alternative method — where a learnable transofrmation module is added to the CNN, which take in the input image and is learned to apply tranformations to it to improve detection <a href="https://towardsdatascience.com/convnets-series-spatial-transformer-networks-cff47565ae81" data-href="https://towardsdatascience.com/convnets-series-spatial-transformer-networks-cff47565ae81" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Spatial Transformer NW</a> Google DeepMind — not used myself)</p><p name="b790" id="b790" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Drill Down — The problme of CNN invariance to Scale</strong></p><p name="96d9" id="96d9" class="graf graf--p graf-after--p">This is a little more complex. For real-time detection, we use a CNN called a Single Shot Detector. Single shot detectors sacrifice some accuracy for performance.</p><p name="35be" id="35be" class="graf graf--p graf-after--p">Here is one picture you may have seen from the YOLO home page.</p><figure name="f60f" id="f60f" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 591px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 84.39999999999999%;"></div><img class="graf-image" data-image-id="0*AgkQokRX28dWSXPM.png" data-width="1760" data-height="1486" src="https://cdn-images-1.medium.com/max/800/0*AgkQokRX28dWSXPM.png"></div><figcaption class="imageCaption">from <a href="https://pjreddie.com/darknet/yolo/" data-href="https://pjreddie.com/darknet/yolo/" class="markup--anchor markup--figure-anchor" rel="nofollow noopener" target="_blank">https://pjreddie.com/darknet/yolo/</a></figcaption></figure><p name="d951" id="d951" class="graf graf--p graf-after--figure">Multi object detection . Note- Detection is different or more difficult from classification in that it needs to also predict the bounding boxes that the object is present in.</p><p name="b86c" id="b86c" class="graf graf--p graf-after--p">Here is the output on a previous version of Yolo (Yolo v2, the current v3 seems to have improved a lot) on a pictures taken at a height.</p><figure name="4021" id="4021" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 675px; max-height: 450px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 66.7%;"></div><img class="graf-image" data-image-id="1*MjIj42PNmHa39MLFYfVYfw.png" data-width="675" data-height="450" src="https://cdn-images-1.medium.com/max/800/1*MjIj42PNmHa39MLFYfVYfw.png"></div><figcaption class="imageCaption">Yolo v2 from an arieal picture- Image resolution problem</figcaption></figure><p name="7c46" id="7c46" class="graf graf--p graf-after--figure">And if you think these type of pictures or use cases are rare -many real world use cases are very similar to this. This is one problem in using Object Detection for real world products; it is very hard to test; You may have a highly trained person detector- but are you sure you have enough images for all the skin tones, facial features and attires that it will work very well across the world?</p><p name="2912" id="2912" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Most of the things that work very well in a demo is sometimes useless in production or for a particular customers use case; and one reason what has prompted me to write this.</strong></p><p name="0732" id="0732" class="graf graf--p graf-after--p">As I said why the NW does not detect small sizes though trained well for large can be due to two reasons.</p><p name="819b" id="819b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Problem 1: Limit of Input resolution</strong></p><p name="ba99" id="ba99" class="graf graf--p graf-after--p">In YoloV2 it scales images down; the input image was a frame from a HD video feed. Scaling it from input (1280*720) down to (416*416) <strong class="markup--strong markup--p-strong">immediately destroys lot of features, especially of small objects. </strong>This is the first problem. Lesson learned -use a NW implementation that will take higher resolution images, plus have a decent GPU with enough memory (GTX 1080 should do for a start). Note that each Convolution and Max Pooling layer is further reducing features.<a href="https://datascience.stackexchange.com/questions/14122/why-convolute-if-max-pooling-is-just-going-to-downsample-the-image-anyway" data-href="https://datascience.stackexchange.com/questions/14122/why-convolute-if-max-pooling-is-just-going-to-downsample-the-image-anyway" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"> Max pooling is essential for postional invariance</a>; but the final effect of detecting small objectes is very bad.</p><p name="6ab5" id="6ab5" class="graf graf--p graf-after--p">If we cut the above frame into 4 frames and give it to Yolo v2 individually and then stitch together, it performs well (a good solution at that time by one of my team mate Sai Narasimha Vennamaneni). There is a cost involved here; one of speed; and then the complexity overhead of removing overlapping boundary boxes; as a straight slicing may cut the objects itself in the boundaries; so the logic of overlapped cutting and then ignoring possible duplicates has to be done.</p><figure name="8848" id="8848" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 463px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 66.10000000000001%;"></div><img class="graf-image" data-image-id="1*c_ojwsQYJhcS25JGP9w3jQ.png" data-width="800" data-height="529" src="https://cdn-images-1.medium.com/max/800/1*c_ojwsQYJhcS25JGP9w3jQ.png"></div><figcaption class="imageCaption">obfuscated image for demo</figcaption></figure><p name="f634" id="f634" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Problem 2: CNN layers removes feature; not good news for small object detection with <em class="markup--em markup--p-em">deep neural networks.</em></strong></p><p name="b203" id="b203" class="graf graf--p graf-after--p">This is a bigger problem. Each convolution layer basically looks for some patterns while losing some details; so at some depth, all these small cars features completely vanish.</p><blockquote name="6f83" id="6f83" class="graf graf--blockquote graf-after--p">SSD uses layers already deep down into the convolutional network to detect objects. If we redraw the diagram closer to scale, we should realize the spatial resolution has dropped significantly and may already miss the opportunity in locating small objects that are too hard to detect in low resolution. <strong class="markup--strong markup--blockquote-strong">If such problem exists, we need to increase the resolution of the input image.</strong></blockquote><blockquote name="fb46" id="fb46" class="graf graf--blockquote graf-after--blockquote">from <a href="https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d" data-href="https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d" class="markup--anchor markup--blockquote-anchor" rel="nofollow" target="_blank">https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d</a></blockquote><div name="8960" id="8960" class="graf graf--mixtapeEmbed graf-after--blockquote"><a href="https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d" data-href="https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d"><strong class="markup--strong markup--mixtapeEmbed-strong">What do we learn from single shot object detectors (SSD, YOLOv3), FPN &amp; Focal loss (RetinaNet)?</strong><br><em class="markup--em markup--mixtapeEmbed-em">In part 2, we will have a comprehensive review of single shot object detectors including SSD and YOLO (YOLOv2 and…</em>medium.com</a><a href="https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="cdcbeaaa130904b8ecf949412d7fa66e" data-thumbnail-img-id="0*OG_cQNTRdw0CYGyU." style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*OG_cQNTRdw0CYGyU.);"></a></div><p name="f1c6" id="f1c6" class="graf graf--p graf-after--mixtapeEmbed">Here is a little more technical explanation from a recent published paper</p><blockquote name="ebcf" id="ebcf" class="graf graf--blockquote graf-after--p">Since feature maps of layers closer to the input are of higher resolution and often contain complementary information (wrt. conv5), these features<br>are either combined with shallower layers (like conv4, conv3) [23, 31, 1, 31] or independent predictions are made at layers of different resolutions [36, 27, 3]. Methods like SDP [36], SSH [29] or MS-CNN [3], which make independent predictions at different layers, also ensure that smaller objects are trained on higher resolution layers (like conv3) while larger objects are trained on lower resolution layers (like conv5).</blockquote><blockquote name="a201" id="a201" class="graf graf--blockquote graf-after--blockquote"><strong class="markup--strong markup--blockquote-strong">An Analysis of Scale Invariance in Object Detection </strong>— SNIP<br>Bharat Singh Larry S. Davis University of Maryland, College Park</blockquote><blockquote name="c5fe" id="c5fe" class="graf graf--blockquote graf-after--blockquote"><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Singh_An_Analysis_of_CVPR_2018_paper.pdf" data-href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Singh_An_Analysis_of_CVPR_2018_paper.pdf" class="markup--anchor markup--blockquote-anchor" rel="nofollow noopener" target="_blank">http://openaccess.thecvf.com/content_cvpr_2018/papers/Singh_An_Analysis_of_CVPR_2018_paper.pdf</a></blockquote><p name="95b5" id="95b5" class="graf graf--p graf-after--blockquote">Excellent blogs from <a href="https://medium.com/u/bd51f1a63813" data-href="https://medium.com/u/bd51f1a63813" data-anchor-type="2" data-user-id="bd51f1a63813" data-action-value="bd51f1a63813" data-action="show-user-card" data-action-type="hover" class="markup--user markup--p-user" target="_blank">Jonathan Hui</a> ; he explains here how Yolo <strong class="markup--strong markup--p-strong">v3</strong> overcomes this problem with <strong class="markup--strong markup--p-strong">Feature Pyramid;</strong> so this may not be too much of a problem now, also other NW like Retina net perform well as well for small objects. But one needs to test and check.</p><p name="8255" id="8255" class="graf graf--p graf-after--p">Here is from another paper April 2018</p><blockquote name="76dd" id="76dd" class="graf graf--blockquote graf-after--p">We provide an illustration of the motivation of the paper …. Pedestrian instances in the automotive images (e.g., Caltech dataset [11]) often have very small sizes….. Accurately localizing these small-size pedestrian instances is quite challenging due to the following difficulties. <strong class="markup--strong markup--blockquote-strong">Firstly, most of the small-size instances appear with blurred boundaries and obscure appearance</strong>. It is difficult to distinguish them from the background clutters and other overlapped instances. Secondly, the large-size pedestrian instances typically <em class="markup--em markup--blockquote-em">exhibit dramatically different visual characteristics from the small-size ones</em></blockquote><figure name="cd24" id="cd24" class="graf graf--figure graf-after--blockquote"><div class="aspectRatioPlaceholder is-locked" style="max-width: 592px; max-height: 538px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 90.9%;"></div><img class="graf-image" data-image-id="1*-p-fSqbGkA0MbiEP8vKAHQ.png" data-width="592" data-height="538" src="https://cdn-images-1.medium.com/max/800/1*-p-fSqbGkA0MbiEP8vKAHQ.png"></div><figcaption class="imageCaption">source <a href="https://ieeexplore.ieee.org/abstract/document/8060595" data-href="https://ieeexplore.ieee.org/abstract/document/8060595" class="markup--anchor markup--figure-anchor" rel="nofollow noopener noopener" target="_blank">https://ieeexplore.ieee.org/abstract/document/8060595</a></figcaption></figure><blockquote name="7e5f" id="7e5f" class="graf graf--blockquote graf-after--figure">For instance, body skeletons of the large-size instances can provide rich information for pedestrian detection while skeletons of the small-size instances cannot be recognized so easily. <strong class="markup--strong markup--blockquote-strong">S<em class="markup--em markup--blockquote-em">uch differences can also be verified by comparing the generated feature maps for large-size and small-size pedestrians, as shown in Fig. 1</em></strong>.</blockquote><blockquote name="f6d5" id="f6d5" class="graf graf--blockquote graf-after--blockquote">From Scale-Aware Fast R-CNN for Pedestrian Detection By <a href="https://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=%22First%20Name%22:%22Jianan%22&amp;searchWithin=%22Last%20Name%22:%22Li%22&amp;newsearch=true" data-href="https://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=%22First%20Name%22:%22Jianan%22&amp;searchWithin=%22Last%20Name%22:%22Li%22&amp;newsearch=true" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">Jianan Li </a>; <a href="https://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=%22First%20Name%22:%22Xiaodan%22&amp;searchWithin=%22Last%20Name%22:%22Liang%22&amp;newsearch=true" data-href="https://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=%22First%20Name%22:%22Xiaodan%22&amp;searchWithin=%22Last%20Name%22:%22Liang%22&amp;newsearch=true" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">Xiaodan Liang </a>; <a href="https://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=%22First%20Name%22:%22Shengmei%22&amp;searchWithin=%22Last%20Name%22:%22Shen%22&amp;newsearch=true" data-href="https://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=%22First%20Name%22:%22Shengmei%22&amp;searchWithin=%22Last%20Name%22:%22Shen%22&amp;newsearch=true" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">Shengmei Shen </a>; <a href="https://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=%22First%20Name%22:%22Tingfa%22&amp;searchWithin=%22Last%20Name%22:%22Xu%22&amp;newsearch=true" data-href="https://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=%22First%20Name%22:%22Tingfa%22&amp;searchWithin=%22Last%20Name%22:%22Xu%22&amp;newsearch=true" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">Tingfa Xu </a>; <a href="https://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=%22First%20Name%22:%22Jiashi%22&amp;searchWithin=%22Last%20Name%22:%22Feng%22&amp;newsearch=true" data-href="https://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=%22First%20Name%22:%22Jiashi%22&amp;searchWithin=%22Last%20Name%22:%22Feng%22&amp;newsearch=true" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">Jiashi Feng </a>; <a href="https://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=%22First%20Name%22:%22Shuicheng%22&amp;searchWithin=%22Last%20Name%22:%22Yan%22&amp;newsearch=true" data-href="https://ieeexplore.ieee.org/search/searchresult.jsp?searchWithin=%22First%20Name%22:%22Shuicheng%22&amp;searchWithin=%22Last%20Name%22:%22Yan%22&amp;newsearch=true" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">Shuicheng Yan</a></blockquote><p name="f091" id="f091" class="graf graf--p graf-after--blockquote"><strong class="markup--strong markup--p-strong">Scale Invariance- Training it in</strong></p><p name="0d3e" id="0d3e" class="graf graf--p graf-after--p">From my experience, the CNN’s currently are not scale invariant. It may be do to the above two factors, feature loss when the target image is small ,compounded with the features loss in deep neural network. However we have found that if we are able to prepare a training data set that have both small and large objects the current network is able to detect different scales with the same class as long as it can work on input images without scaling down much.So training the network has become sort of a skill now.</p><h4 name="fa9f" id="fa9f" class="graf graf--h4 graf-after--p">The Elephant in the room; Need for a large good quality human annotated image set for Training</h4><p name="b038" id="b038" class="graf graf--p graf-after--h4">Here is the most painful thing about CNN’s today - you need thousands to literally hundred thousands of good annotated images of an object for training; that is good enough generalization without <strong class="markup--strong markup--p-strong">over-fitting.</strong></p><p name="9e1c" id="9e1c" class="graf graf--p graf-after--p">The presence of <a href="http://cocodataset.org/#home" data-href="http://cocodataset.org/#home" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">COCO</a> image set is for image detection, what <a href="http://www.image-net.org/" data-href="http://www.image-net.org/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">ImageNet</a> is for image classification.</p><p name="5ded" id="5ded" class="graf graf--p graf-after--p">However there is a high chance that the object you want to detect is not one of the 80 classes of images in COCO.</p><p name="7f5c" id="7f5c" class="graf graf--p graf-after--p">Why is this so important ? For this, we need to understand a bit about <strong class="markup--strong markup--p-strong">generalization</strong>.</p><blockquote name="fa0a" id="fa0a" class="graf graf--blockquote graf-after--p">The central challenge in machine learning is that our algorithm must perform well on new, previously unseen inputs — not just those on which our model was trained. <strong class="markup--strong markup--blockquote-strong">The ability to perform well on previously unobserved inputs is called generalization</strong> Chapter 5.2 Deep Learning book</blockquote><p name="8ca5" id="8ca5" class="graf graf--p graf-after--blockquote">When a neural net trains, it uses the divergence from the <strong class="markup--strong markup--p-strong">test data</strong> to <a href="https://www.youtube.com/watch?v=IHZwWFHWa-w" data-href="https://www.youtube.com/watch?v=IHZwWFHWa-w" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><em class="markup--em markup--p-em">learn</em></a><em class="markup--em markup--p-em"> </em>the correct weights via <a href="https://www.youtube.com/watch?v=Ilg3gGewQ5U" data-href="https://www.youtube.com/watch?v=Ilg3gGewQ5U" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">back propagation</a>. If there are only few images* to train on, the NW will learn too well (or be too specific) to the training data, and will perform worse on data in the wild. To reduce this, there are <strong class="markup--strong markup--p-strong">regularization</strong> techniques used. Instead of just train and test, there is also a third set of images called <strong class="markup--strong markup--p-strong">validation set</strong>, and if the results starts to diverge too much from validation set, though it matches the test set more, then it is an indication to do a<strong class="markup--strong markup--p-strong"> ‘</strong><a href="https://en.wikipedia.org/wiki/Early_stopping" data-href="https://en.wikipedia.org/wiki/Early_stopping" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">early stop</strong></a>’ of the training.</p><p name="b0e1" id="b0e1" class="graf graf--p graf-after--p">The other regularization option is <strong class="markup--strong markup--p-strong">drop-out.</strong></p><blockquote name="275e" id="275e" class="graf graf--blockquote graf-after--p">Simply put, dropout refers to ignoring units (i.e. neurons) during the training phase of certain set of neurons which is chosen at random. <a href="https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5" data-href="https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5" class="markup--anchor markup--blockquote-anchor" rel="nofollow" target="_blank">https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5</a></blockquote><p name="484c" id="484c" class="graf graf--p graf-after--blockquote">A lot of questions abound on the internet regarding how to prevent over-fitting <a href="https://github.com/keras-team/keras/issues/4325" data-href="https://github.com/keras-team/keras/issues/4325" class="markup--anchor markup--p-anchor" rel="nofollow noopener noopener noopener" target="_blank">https://github.com/keras-team/keras/issues/4325</a>. Also see this excellent article with sample — <a href="https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/" data-href="https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/</a></p><p name="cf9d" id="cf9d" class="graf graf--p graf-after--p">Apart from the methods above, the main part is to have enough data points to train with.</p><blockquote name="bce5" id="bce5" class="graf graf--blockquote graf-after--p">To prevent overfitting, the best solution is to use more training data. A model trained on more data will naturally generalize better. When that is no longer possible, the next best solution is to use techniques like regularization <a href="https://www.tensorflow.org/tutorials/keras/overfit_and_underfit" data-href="https://www.tensorflow.org/tutorials/keras/overfit_and_underfit" class="markup--anchor markup--blockquote-anchor" rel="nofollow noopener" target="_blank">https://www.tensorflow.org/tutorials/keras/overfit_and_underfit</a></blockquote><p name="23f9" id="23f9" class="graf graf--p graf-after--blockquote">Since we are on the topic of generalization ( and the role of regularisation techniques to prevent generalisation), it may also be the place to mention about the popular paper <a href="https://www.youtube.com/watch?v=kCj51pTQPKI" data-href="https://www.youtube.com/watch?v=kCj51pTQPKI" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Understanding deep learning requires rethinking generalization</a> by Chiyuan Zhung et al. It shows that DNN especially CNN’s can ‘memorize’ instead of generalizing see this <a href="https://www.quora.com/Why-is-the-paper-%E2%80%9CUnderstanding-Deep-Learning-Requires-Rethinking-Generalization%E2%80%9D-important" data-href="https://www.quora.com/Why-is-the-paper-%E2%80%9CUnderstanding-Deep-Learning-Requires-Rethinking-Generalization%E2%80%9D-important" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">quora answer</a> which basically means that if you train with say faces and which has some images with random noise instead of proper faces, the NW will still converge — test accuracy nearing 100 percent. Which means there are lot of parts of this ‘learning’ process which have popular explanations, but also has unknown explanations. Wanted to put this in even though slighly confusing in that, sometimes you may find your trained network not detecting a particular image and then you need to hunt for theories to explain better and then try some solution- object classification itself is not a done deal yet.</p><p name="0729" id="0729" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">A word about Transfer Learning</strong></p><p name="088a" id="088a" class="graf graf--p graf-after--p">In the recent Google NEXT event AutoML was presented.If using AutoML for Vision, it was claimed that ten to twenty images of leaves are all what is needed for training. I am not sure of the internals of AutoML, but my inference is that, it could be from <a href="https://machinelearningmastery.com/transfer-learning-for-deep-learning/" data-href="https://machinelearningmastery.com/transfer-learning-for-deep-learning/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">transfer-learning</a> (practically for a NW like Retinanet described <a href="https://github.com/fizyr/keras-retinanet/issues/409" data-href="https://github.com/fizyr/keras-retinanet/issues/409" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">here</a>).</p><p name="618c" id="618c" class="graf graf--p graf-after--p">Here is the same sentiment from another source</p><blockquote name="c5e1" id="c5e1" class="graf graf--blockquote graf-after--p">The origin of the 1,000-image magic number comes from the original <a href="http://image-net.org/" data-href="http://image-net.org/" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">ImageNet</a> classification challenge, where the dataset had 1,000 categories, each with a bit less than 1,000 images for each class (…. This was good enough to train the early generations of image classifiers like AlexNet, and so proves that around 1,000 images is enough.</blockquote><blockquote name="3701" id="3701" class="graf graf--blockquote graf-after--blockquote">Can you get away with less though? Anecdotally, based on my experience, you can in some cases but once you get into the low hundreds it seems to get trickier to train a model from scratch. The biggest exception is when you’re using <a href="https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0" data-href="https://codelabs.developers.google.com/codelabs/tensorflow-for-poets/#0" class="markup--anchor markup--blockquote-anchor" rel="noopener" target="_blank">transfer learning</a> on an already-trained model. Because you’re using a network that has already seen a lot of images and learned to distinguish between the classes, you can usually teach it new classes in the same domain with as few as ten or twenty examples.From <a href="https://petewarden.com/2017/12/14/how-many-images-do-you-need-to-train-a-neural-network/" data-href="https://petewarden.com/2017/12/14/how-many-images-do-you-need-to-train-a-neural-network/" class="markup--anchor markup--blockquote-anchor" rel="nofollow noopener" target="_blank">https://petewarden.com/2017/12/14/how-many-images-do-you-need-to-train-a-neural-network/</a></blockquote><p name="5cc1" id="5cc1" class="graf graf--p graf-after--blockquote">But if we have to detect for an object class of an image that is not in the same domain as other images on it is trained for, this transfer-learning will not work. To give a simple example — it is definitely possible to train a system to detect based on few images of say nails; but then it will see everything as nails- literally. Basically since CNN’s are very deep neural networks, they need a lot of data( read images) to generalize .This calls for lot of work in collecting the required images, and then annotating it; and then training the network in a way, and till such time as to get the optimal result,preventing underfitting or overfitting.</p><p name="c9cd" id="c9cd" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The Future</strong></p><p name="00f3" id="00f3" class="graf graf--p graf-after--p graf--trailing">If you can see a glimpse of light, you can already start imagining the sky; I guess very soon we will be out of this tunnel going by <a href="http://moreisdifferent.com/2017/09/hinton-whats-wrong-with-CNNs" data-href="http://moreisdifferent.com/2017/09/hinton-whats-wrong-with-CNNs" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">research</a> -<a href="https://hackernoon.com/what-is-a-capsnet-or-capsule-network-2bfbe48769cc" data-href="https://hackernoon.com/what-is-a-capsnet-or-capsule-network-2bfbe48769cc" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">CapsuleNet</a> coming from <a href="https://en.wikipedia.org/wiki/Geoffrey_Hinton" data-href="https://en.wikipedia.org/wiki/Geoffrey_Hinton" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">Geoffrey Hinton</a> (one of the ‘Godfather’ of recent AI resurgence — one <a href="https://www.youtube.com/watch?v=-eyhCTvrEtE" data-href="https://www.youtube.com/watch?v=-eyhCTvrEtE" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">who co-introduced back propogation</a>) about <a href="https://www.youtube.com/watch?time_continue=21&amp;v=rTawFwUvnLE" data-href="https://www.youtube.com/watch?time_continue=21&amp;v=rTawFwUvnLE" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">whats wrong with current CNN’s</a> and how his idea of CapsuleNet architecture would be better.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@alexcpn" class="p-author h-card">Alex Punnen</a> on <a href="https://medium.com/p/59a7be913fd2"><time class="dt-published" datetime="2018-11-02T16:49:50.274Z">November 2, 2018</time></a>.</p><p><a href="https://medium.com/@alexcpn/is-object-detection-a-done-deal-59a7be913fd2" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on August 22, 2019.</p></footer></article></body></html>