<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Writing a Generic Tensorflow Serving Client for Tensorflow Serving model</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Writing a Generic Tensorflow Serving Client for Tensorflow Serving model</h1>
</header>
<section data-field="subtitle" class="p-summary">
For CNN based object detection models
</section>
<section data-field="description" class="p-summary">
Shows a framework to develop a generic Tensorflow serving client for TF serving models
</section>
<section data-field="body" class="e-content">
<section name="1b2c" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="7a30" id="7a30" class="graf graf--h3 graf--leading graf--title">Writing a Generic TensorFlow Serving Client for Tensorflow Serving models</h3><h4 name="d9c8" id="d9c8" class="graf graf--h4 graf-after--h3 graf--subtitle">For CNN based object detection models in this case.</h4><p name="868d" id="868d" class="graf graf--p graf-after--h4">If you are in SW Engineering and trying to work with Deep Learning models, chances are that you would be using an OpenSourced Deep Learning model, like SSD, FasterRCNN or the like and building your application on top. There are many ways to do this. Mostly you could build a standalone Tensorflow or Keras application that loads your model to the GPU and create a custom REST or GRPC interface and write custom clients. Depending on the model that you have used, your server and client will vary. This is how most of the systems are in use. There is a better way called TensorFlow serving and that is an elegant way of serving your TensorFlow or Keras models. More than that once DL matures or if you want to productize it, you may need some sort of verification and continuous deployment (CD) for your models as well. The TF Serving pipeline can help here as well.</p><p name="7329" id="7329" class="graf graf--p graf-after--p">Also, you could use model optimization and quantization tools from TF package or use the more functional NVIDIA TensorRT optimization tools to optimize your model and covert it from FP 32 to FP 16, INT 8 models that would theoretically run twice as fast on TensorCore GPU/Mixed precision GPUs like NVIDIA V100 or NVIDIA T4. This is by optimizing the models straight out of GitHub without knowing in-depth of the model. However, things do not quite work as expected now with these tools and <a href="https://devtalk.nvidia.com/default/topic/1048485/tensorrt/no-speed-up-with-tensorrt-fp16-or-int8-on-nvidia-v100/post/5320989/#5320989" data-href="https://devtalk.nvidia.com/default/topic/1048485/tensorrt/no-speed-up-with-tensorrt-fp16-or-int8-on-nvidia-v100/post/5320989/#5320989" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">I am trying to figure out how yet</a>. So that will be for a future post.</p><p name="fe0c" id="fe0c" class="graf graf--p graf-after--p">For this post, we will take some popular trained object detection models from the wild and show how to write a<strong class="markup--strong markup--p-strong"> generic client</strong> for TF Serving use the same client for different detection models. This would give an idea of the usefulness of TF Serving in using these models in SW productions. Since we are using this for production we as mature SW engineers would want to use containers/docker all the way. For serving as well as clients.</p><p name="04be" id="04be" class="graf graf--p graf-after--p">The flow is given below for developing the TF client</p><figure name="5457" id="5457" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 409px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 58.4%;"></div><img class="graf-image" data-image-id="1*cntKPCdIsck5ia2jDrhg9w.png" data-width="900" data-height="526" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*cntKPCdIsck5ia2jDrhg9w.png"></div></figure><p name="57e9" id="57e9" class="graf graf--p graf-after--figure">Note — Even Keras trained models (in binary HD5 format) can be loaded by tf.keras and saved to TF model (PB Protobuffer format) and served via Tensorflow serving. I have shown this in a <a href="https://towardsdatascience.com/using-tensorflow-serving-grpc-38a722451064" data-href="https://towardsdatascience.com/using-tensorflow-serving-grpc-38a722451064" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">previous post</a> for a simple image classification example. The same method applies for Object detection models as well.</p><p name="05e1" id="05e1" class="graf graf--p graf-after--p">The system diagram looks like below. Note that there other than the one-time writing of the TF Serving Client, rest all are pre-packaged Docker images available from Docker hub. Developers can focus on the Visual automation part based on the analysis results from TF Serving client. Adding a Kubernetes layer will allow it to scale across multiple nodes having GPUs as well.</p><figure name="91ca" id="91ca" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 434px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 62%;"></div><img class="graf-image" data-image-id="1*_he_ZzlF7qm3kHU2fwXO6Q.png" data-width="1108" data-height="687" src="https://cdn-images-1.medium.com/max/800/1*_he_ZzlF7qm3kHU2fwXO6Q.png"></div><figcaption class="imageCaption">TF Serving Client</figcaption></figure><p name="bda2" id="bda2" class="graf graf--p graf-after--figure">Before we dive into the steps, we need to select a model. Let us use the SSD models and FasterRCNN model from Tensorflow model Zoo -<a href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md" data-href="https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md</a></p><p name="889d" id="889d" class="graf graf--p graf-after--p">Let us take the <a href="http://download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2018_01_28.tar.gz" data-href="http://download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2018_01_28.tar.gz" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">ssd_inception_v2_coco</a> model as an example. This unzipped will give you a folder with frozen weights proto buffer (which is the base format to optimize or convert to FP16, INT8, formats ) as well as a saved_model folder which contains the model and weights in TF Serving compatible model- basically, the folder has a saved_model.pb and optionally a folder containing variables. Since TF Serving has a concept of versioning, it will check for numerical folders under the base folder to serve. You can provide a configuration file giving specific version also which I will show below. I am so renaming the saved_folder to ‘01’ so that TF Serving can find it. No other changes. Before we go to Step 1 to 4, we need to see what inputs and outputs the model is taking. We use the ‘saved_model_cli’ command to do that. This gives us how to frame the input and how to get the output.</p><p name="4a23" id="4a23" class="graf graf--p graf-after--p">(You can run this on a dev docker container for tf-serving. I have modified a bit and tagged it. You can use this as well for dev, if you want)</p><pre name="42fc" id="42fc" class="graf graf--pre graf-after--p">docker run -it --runtime=nvidia  --net=host   -v /home/alex/coding/IPython_neuralnet:/coding --rm alexcpn/tfserving-keras-retinanet-dev-gpu</pre><p name="6a03" id="6a03" class="graf graf--p graf-after--pre">If you do not want to use my container; you can use the official one with fee installations for opencv and related libs like below.</p><pre name="0c75" id="0c75" class="graf graf--pre graf-after--p">docker run --entrypoint=/bin/bash --env http_proxy=&lt;my proxy&gt; --env https_proxy=&lt;my proxy&gt;  --runtime=nvidia  -it --rm -p 8900:8500 -p 8901:8501 -v /usr/alex/:/coding --net=host tensorflow/tensorflow:1.13.0rc1-gpu-jupyter</pre><pre name="7bf3" id="7bf3" class="graf graf--pre graf-after--pre">pip install tensorflow-serving-api<br>pip install opencv-python==3.3.0.9<br>cd coding<br>python ssd_client_1.py -num_tests=1 -server=127.0.0.1:8500 -batch_size=1 -img_path=&#39;../examples/google1.jpg/&#39;</pre><p name="8275" id="8275" class="graf graf--p graf-after--pre">Here is the output for saved_model_cli. I have highlighted the relevant part. You can guess how it is relevant from the code.</p><pre name="38f2" id="38f2" class="graf graf--pre graf-after--p">root@drone-OMEN:/coding/tf_serving_clients# saved_model_cli show --dir &#39;/coding/models/ssd_inception_v2_coco_2018_01_28/01&#39; --all</pre><pre name="fb00" id="fb00" class="graf graf--pre graf-after--pre">MetaGraphDef with tag-set: &#39;serve&#39; contains the following SignatureDefs:</pre><pre name="a52c" id="a52c" class="graf graf--pre graf-after--pre">signature_def[&#39;serving_default&#39;]:<br>  The given SavedModel SignatureDef contains the following input(s):<br>    inputs[&#39;inputs&#39;] tensor_info:<br>        <strong class="markup--strong markup--pre-strong">dtype: DT_UINT8</strong><br>        <strong class="markup--strong markup--pre-strong">shape: (-1, -1, -1, 3)</strong><br>        name: image_tensor:0<br>  The given SavedModel SignatureDef contains the following output(s):<br>    outputs[&#39;<strong class="markup--strong markup--pre-strong">detection_boxes</strong>&#39;] tensor_info:<br>        dtype: DT_FLOAT<br>        shape: (-1, 100, 4)<br>        name: detection_boxes:0<br>    outputs[&#39;<strong class="markup--strong markup--pre-strong">detection_classes</strong>&#39;] tensor_info:<br>        dtype: DT_FLOAT<br>        shape: (-1, 100)<br>        name: detection_classes:0<br>    outputs[&#39;<strong class="markup--strong markup--pre-strong">detection_scores</strong>&#39;] tensor_info:<br>        dtype: DT_FLOAT<br>        shape: (-1, 100)<br>        name: detection_scores:0<br>    outputs[&#39;num_detections&#39;] tensor_info:<br>        dtype: DT_FLOAT<br>        shape: (-1)<br>        name: num_detections:0<br> <strong class="markup--strong markup--pre-strong"> Method name is</strong>: tensorflow/serving/<strong class="markup--strong markup--pre-strong">predict</strong></pre><h4 name="6114" id="6114" class="graf graf--h4 graf-after--pre"><strong class="markup--strong markup--h4-strong">Step 0. Serve your model via TF Serving</strong></h4><pre name="2608" id="2608" class="graf graf--pre graf-after--h4">docker run  --net=host --runtime=nvidia  -it --rm -p 8900:8500 -p 8901:8501 -v /home/alex/coding/IPython_neuralnet/models:/models  <strong class="markup--strong markup--pre-strong">tensorflow/serving:latest-gpu</strong> --rest_api_port=0  --enable_batching=true  --model_config_file=/models/model_configs/ssd.json</pre><p name="d1c4" id="d1c4" class="graf graf--p graf-after--pre">where the configuration file can be like below</p><pre name="3532" id="3532" class="graf graf--pre graf-after--p">model_config_list {<br> config {<br>    name: &quot;ssd_inception_v2_coco&quot;,<br>    base_path: &quot;/models/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03/&quot;,<br>   model_version_policy: {<br>        all: {}<br>    },<br>    model_platform:&quot;tensorflow&quot;,<br>  }<br>}</pre><p name="29f8" id="29f8" class="graf graf--p graf-after--pre">To give a specific version (not for this example)</p><pre name="2790" id="2790" class="graf graf--pre graf-after--p">model_config_list {<br> config {<br>    name: &quot;retinanet&quot;,<br>    base_path: &quot;/models/retinanet/&quot;,<br>  <strong class="markup--strong markup--pre-strong"> model_version_policy: {<br>        specific: { <br>            versions:[4] <br>       }</strong><br>    },<br>    model_platform:&quot;tensorflow&quot;,<br>  }<br>}</pre><p name="c66c" id="c66c" class="graf graf--p graf-after--pre">Another way to start is to give the base model path like below.</p><pre name="7b15" id="7b15" class="graf graf--pre graf-after--p">docker run  --net=host --runtime=nvidia  -it --rm -p 8900:8500 -p 8901:8501 -v /home/alex/coding/IPython_neuralnet/models:/models <strong class="markup--strong markup--pre-strong">tensorflow/serving:latest-gpu</strong> --rest_api_port=0  --enable_batching=true  --model_name=&quot;default&quot; --<strong class="markup--strong markup--pre-strong">model_base_path</strong>=/models/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03</pre><p name="6eab" id="6eab" class="graf graf--p graf-after--pre">Assuming it is started (and it should start and run if you have a GPU, failing which it will try to run on CPU; do confirm with nvidia-smi command to check if it is really running on GPU)</p><h4 name="9e70" id="9e70" class="graf graf--h4 graf-after--p">Step 1: Pre-process the Image for the Network</h4><p name="839f" id="839f" class="graf graf--p graf-after--h4">There are multiple ways of loading in and pre-processing the image. I have listed some here <a href="https://medium.com/techlogs/image-pre-processing-for-tf-serving-via-opencv-pillow-tensorflow-tf-image-decode-876c8a14c67a" data-href="https://medium.com/techlogs/image-pre-processing-for-tf-serving-via-opencv-pillow-tensorflow-tf-image-decode-876c8a14c67a" class="markup--anchor markup--p-anchor" rel="nofollow" target="_blank">https://medium.com/techlogs/image-pre-processing-for-tf-serving-via-opencv-pillow-tensorflow-tf-image-decode-876c8a14c67a</a></p><p name="cc35" id="cc35" class="graf graf--p graf-after--p">From experiments, the way the image is pre-processed, for example the input dimensions, the resizing, whether it is passed in as BGR or RGB format has a very high impact on the detection results. Also, it varies at the size (zoom) of objects in the image. This depends highly on what type of images the NW is trained on, and for each NW there is some setting which is better. But again depending on the context of objects in the images, the results can vary within a model. That is, some networks or most networks are not good in detecting small objects. Usually, for this, the image resolution that is fed to the model should be high. That is scaling it down to 222x222 or similar will give poor results. But with high resolution, the GPU should have sufficient memory and also detection will be slow. This is a rabbit hole and I guess many articles can be written on that. But here I will give a snippet that I wrote that I found does the job well across models. ( Please use the deocde_image_opencv one, it is better than the tensorflow based processing)</p><figure name="6cbf" id="6cbf" class="graf graf--figure graf--iframe graf-after--p"><script src="https://gist.github.com/alexcpn/dd476e1aa3ec2c97ea75248794a5b8cc.js"></script></figure><p name="1622" id="1622" class="graf graf--p graf-after--figure">Most of it will be clear except maybe the VGG_MEAN subtraction part. The article here explains it very cleary — <a href="https://www.pyimagesearch.com/2017/11/06/deep-learning-opencvs-blobfromimage-works/" data-href="https://www.pyimagesearch.com/2017/11/06/deep-learning-opencvs-blobfromimage-works/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://www.pyimagesearch.com/2017/11/06/deep-learning-opencvs-blobfromimage-works/</a></p><p name="5b94" id="5b94" class="graf graf--p graf-after--p">Note that the scale of the image and RGB, BGR format, and also mean subtraction impacts the predictions, and the impact ranges from minimal to high, depending on the model and the image. The easy way is to test on a few images and this will give an idea. Or read about how the feature extraction layer for your CNN model is trained on. This will give an understanding of if the NW works best in RGB or BGR, the input resolution and whether the pixel intensities (VGG/IMAGENET Mean) need to be subtracted.</p><p name="b89b" id="b89b" class="graf graf--p graf-after--p">Also, the exact input dimension is not important for a fully CNN network (<a href="https://stackoverflow.com/a/53392772/429476" data-href="https://stackoverflow.com/a/53392772/429476" class="markup--anchor markup--p-anchor" rel="nofollow noopener" target="_blank">https://stackoverflow.com/a/53392772/429476</a>). You see the input dimension as [-1,-1,-1,3] = [ number of image, width,height,RGB/BGR color channels]. The first parameter is 1 in case of single image and n — in case of a batched image.</p><pre name="3509" id="3509" class="graf graf--pre graf-after--p">inputs[&#39;inputs&#39;] tensor_info:<br>        <strong class="markup--strong markup--pre-strong">dtype: DT_UINT8</strong><br>        <strong class="markup--strong markup--pre-strong">shape: (-1, -1, -1, 3)</strong></pre><p name="6c68" id="6c68" class="graf graf--p graf-after--pre">Also, the above method gives the output in Floating point format, but the model expects it in unsigned int format. This can be converted. Some models accept in floating point format.</p><h4 name="5af7" id="5af7" class="graf graf--h4 graf-after--p">Step 2 Optional — Batch the Images</h4><p name="f3ec" id="f3ec" class="graf graf--p graf-after--h4">If your GPU has more memory this may speed up; or not. I did not see much difference in limited tests in 32 GB V100. I am going to let the code do the talking. We are basically stacking the image arrays up. Note that these are not encoded but raw pixels and transferring them over NW will incur cost depending on your NW. The better idea is to colocate the video to image conversion in the node where TF Serving container is running.</p><p name="91a2" id="91a2" class="graf graf--p graf-after--p">If you print the shape of ‘inputs’ you would see the first dimension equal to the batch size. In this example, I am loading the <em class="markup--em markup--p-em">same image</em> over and over again just for experiments with batch size and speed. But you can change this for multiple images.</p><pre name="8ec5" id="8ec5" class="graf graf--pre graf-after--p">input = image <br>inputs = input<br>for _ in range(batch_size-1):<br>   inputs = np.append(inputs, input, axis=0)</pre><h4 name="349e" id="349e" class="graf graf--h4 graf-after--pre">Step 3 Send images to TF Serving via GRPC</h4><p name="3d5c" id="3d5c" class="graf graf--p graf-after--h4">Again need the code to do the talking here. Note that irrespective of the NW architecture all these parts are generic. We can use this with a range of different models. <strong class="markup--strong markup--p-strong">The interface to TF Serving is the same. This is the small but very very very important point; the first step in making generic clients that don’t have to be tied to the model.</strong></p><pre name="b69a" id="b69a" class="graf graf--pre graf-after--p">print(&quot;Input-s shape&quot;,inputs.shape)<br>      request.inputs[&#39;inputs&#39;].CopyFrom(tf.contrib.util.make_tensor_proto<br>            (inputs, shape=inputs.shape))<br>      <br>      # call back way - this is faster<br>      result_future = stub.Predict.future(request, 60.25)  # Intial takes time  <br>      result_future.add_done_callback(_callback)</pre><pre name="6bc4" id="6bc4" class="graf graf--pre graf-after--pre"># request reponse way - this is slower<br>      # result = stub.Predict(request, 10.25)  #  seconds  <br>      # parse_result(result)<br>      _response_awaiting = True</pre><pre name="6e0b" id="6e0b" class="graf graf--pre graf-after--pre">#print(&quot;Send the request&quot;)<br>      # End for loop</pre><pre name="d8df" id="d8df" class="graf graf--pre graf-after--pre">while(_response_awaiting):<br>      time.sleep(.000010)<br>    print(&quot;Response Received Exiting&quot;)</pre><h4 name="bf56" id="bf56" class="graf graf--h4 graf-after--pre">Step 4 Pre-process the output</h4><p name="9a31" id="9a31" class="graf graf--p graf-after--h4">Remember the output format</p><pre name="0751" id="0751" class="graf graf--pre graf-after--p">The given SavedModel SignatureDef contains the following output(s):<br>    outputs[&#39;<strong class="markup--strong markup--pre-strong">detection_boxes</strong>&#39;] tensor_info:<br>        dtype: DT_FLOAT<br>        shape: (-1, 100, 4)<br>        name: detection_boxes:0<br>    outputs[&#39;<strong class="markup--strong markup--pre-strong">detection_classes</strong>&#39;] tensor_info:<br>        dtype: DT_FLOAT<br>        shape: (-1, 100)<br>        name: detection_classes:0<br>    outputs[&#39;<strong class="markup--strong markup--pre-strong">detection_scores</strong>&#39;] tensor_info:<br>        dtype: DT_FLOAT<br>        shape: (-1, 100)<br>        name: detection_scores:0<br>    outputs[&#39;num_detections&#39;] tensor_info:<br>        dtype: DT_FLOAT<br>        shape: (-1)<br>        name: num_detections:0</pre><p name="e809" id="e809" class="graf graf--p graf-after--pre">We need to parse this output. Snippet below</p><pre name="2aae" id="2aae" class="graf graf--pre graf-after--p">boxes = result_future.\<br>          outputs[&#39;detection_boxes&#39;]<br>    scores = result_future.\<br>          outputs[&#39;detection_scores&#39;]<br>    labels = result_future.\<br>          outputs[&#39;detection_classes&#39;]<br>    num_detections= result_future.\<br>          outputs[&#39;num_detections&#39;]</pre><pre name="ba70" id="ba70" class="graf graf--pre graf-after--pre">boxes= tf.make_ndarray(boxes)<br>    scores= tf.make_ndarray(scores)<br>    labels= tf.make_ndarray(labels)<br>    num_detections= tf.make_ndarray(num_detections)</pre><pre name="ac1a" id="ac1a" class="graf graf--pre graf-after--pre"># visualize detections hints from <br>    # # <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb" data-href="https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb" class="markup--anchor markup--pre-anchor" rel="nofollow noopener" target="_blank">https://github.com/tensorflow/models/blob/master/research/object_detection/object_detection_tutorial.ipynb</a></pre><pre name="842d" id="842d" class="graf graf--pre graf-after--pre">for box, score, label in zip(boxes[0], scores[0], labels[0]):<br>        # scores are sorted so we can break<br>        if score &lt; 0.3:<br>            break<br>        #dim = image.shape[0:2]<br>        dim = _draw.shape<br>        #print(&quot;Label-raw&quot;,labels_to_names[label-1],&quot; at &quot;,box,&quot; Score &quot;,score)<br>        box = box_normal_to_pixel(box, dim)<br>        b = box.astype(int)<br>        class_label = get_label(int(label))<br>        print(&quot;Label&quot;,class_label ,&quot; at &quot;,b,&quot; Score &quot;,score)<br>        # draw the image and write out<br>        cv2.rectangle(_draw,(b[0],b[1]),(b[2],b[3]),(0,0,255),1)<br>        cv2.putText(_draw,class_label + &quot;-&quot;+str(round(score,2)), (b[0]+2,b[1]+8),\<br>           cv2.FONT_HERSHEY_SIMPLEX, .45, (0,0,255))</pre><p name="fe9d" id="fe9d" class="graf graf--p graf-after--pre">Note that the class id of detection is an index. This has to be mapped to a human-readable format. Either this label to index can be hand-coded, or in case of SSD I found this in the Model Zoo dir and read it in via <strong class="markup--strong markup--p-strong"><em class="markup--em markup--p-em">google.protobuf import text_format </em></strong>after seeing the model format used — in this case — <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/protos/string_int_label_map.proto" data-href="https://github.com/tensorflow/models/blob/master/research/object_detection/protos/string_int_label_map.proto" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">https://github.com/tensorflow/models/blob/master/research/object_detection/protos/string_int_label_map.proto</a></p><pre name="7e9f" id="7e9f" class="graf graf--pre graf-after--p">protoc --proto-path=. --python_out=. string_int_label_map.proto</pre><p name="1548" id="1548" class="graf graf--p graf-after--pre">to generate string_int_label_map_pb2</p><pre name="8230" id="8230" class="graf graf--pre graf-after--p">import string_int_label_map_pb2 as labelmap<br>s = open(&#39;mscoco_complete_label_map.pbtxt&#39;,&#39;r&#39;).read()<br>mymap =labelmap.StringIntLabelMap()<br>global _label_map<br>_label_map = text_format.Parse(s,mymap</pre><p name="eb3a" id="eb3a" class="graf graf--p graf-after--pre">The other part of the puzzle is drawing the box. Took some hours from the weekend to figure this out. The model that I was using give absolute co-ordinated. But the SSD and other models in the TF model zoo gives in <em class="markup--em markup--p-em">relative format</em> and has to be converted first to absolute format. Snippet below.</p><pre name="1992" id="1992" class="graf graf--pre graf-after--p">def box_normal_to_pixel(box, dim,scalefactor=1):<br>    # <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py" data-href="https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py" class="markup--anchor markup--pre-anchor" rel="nofollow noopener" target="_blank">https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py</a><br>    height, width = dim[0], dim[1]<br>    ymin = int(box[0]*height*scalefactor)<br>    xmin = int(box[1]*width*scalefactor)</pre><pre name="0535" id="0535" class="graf graf--pre graf-after--pre">ymax = int(box[2]*height*scalefactor)<br>    xmax= int(box[3]*width*scalefactor)<br>    return np.array([xmin,ymin,xmax,ymax])</pre><p name="75ac" id="75ac" class="graf graf--p graf-after--pre">The label mapping can be easily made generic. Also the box drawing parts. All this can be wired together in a TF Serving Client framework. Also now the models are zipped and stored but can be added to a docker image and copied to a mapped folder. This will make creating model catalogs easy.</p><p name="cbc0" id="cbc0" class="graf graf--p graf-after--p">Here are some outputs with running the same client with different models. Just give the <em class="markup--em markup--p-em">model_base path</em> as an argument to TF Serving container like this to run different models. (Or use the configuration file described above)</p><pre name="4254" id="4254" class="graf graf--pre graf-after--p">docker run  --net=host --runtime=nvidia  -it --rm -p 8900:8500 -p 8901:8501 -v /home/alex/coding/IPython_neuralnet/models:/models  tensorflow/serving:latest-gpu --rest_api_port=0  --enable_batching=true  --model_name=&quot;default&quot; --model_base_path=/models/faster_rcnn_resnet101_coco_2018_01_28</pre><figure name="3de6" id="3de6" class="graf graf--figure graf-after--pre"><div class="aspectRatioPlaceholder is-locked" style="max-width: 486px; max-height: 392px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 80.7%;"></div><img class="graf-image" data-image-id="1*fZ64YJVoHN6O2TBl1HrRHA.png" data-width="486" data-height="392" src="https://cdn-images-1.medium.com/max/800/1*fZ64YJVoHN6O2TBl1HrRHA.png"></div></figure><p name="63cd" id="63cd" class="graf graf--p graf-after--figure">The Good- with a model that we are using now (opensource but cannot divulge details)</p><figure name="d784" id="d784" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder"><img class="graf-image" data-image-id="1*6s1T3ayOsHuMzWUXqgqpnQ.png" src="https://cdn-images-1.medium.com/max/800/1*6s1T3ayOsHuMzWUXqgqpnQ.png"></div></figure><p name="3913" id="3913" class="graf graf--p graf-after--figure">The bad- I guess the same model, but different image scaling and I am not sure now, was experimenting on BGR, RGB as well as VGG_MEAN. This shows that just reducing the intensity is changing the detections and scores. Also unlike in the above image, the objects are very near to the camera.</p><figure name="876f" id="876f" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder"><img class="graf-image" data-image-id="1*rb5qtW_nZswyN_4_AiwCkw.png" src="https://cdn-images-1.medium.com/max/800/1*rb5qtW_nZswyN_4_AiwCkw.png"></div></figure><p name="786e" id="786e" class="graf graf--p graf-after--figure">The below from SSD with RGB input and scale as height 800 (width autoscaled)</p><figure name="ac13" id="ac13" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 464px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 66.3%;"></div><img class="graf-image" data-image-id="1*PzkGIj05uKKeRMYtF5zteQ.png" data-width="1206" data-height="800" src="https://cdn-images-1.medium.com/max/800/1*PzkGIj05uKKeRMYtF5zteQ.png"></div></figure><p name="b3ef" id="b3ef" class="graf graf--p graf-after--figure">And the ugly — same SSD model with IMAGENET mean subtracted (It need not be for SSD, I did not know when I did this first. However, it illustrates the importance of correct image pre-processing -Object detection i<a href="https://hackernoon.com/is-object-detection-a-done-deal-59a7be913fd2" data-href="https://hackernoon.com/is-object-detection-a-done-deal-59a7be913fd2" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">s not a done deal yet</a>.</p><figure name="dd34" id="dd34" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 464px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 66.3%;"></div><img class="graf-image" data-image-id="1*AMOSg8Qc5noGgkTLpR0dnQ.png" data-width="905" data-height="600" src="https://cdn-images-1.medium.com/max/800/1*AMOSg8Qc5noGgkTLpR0dnQ.png"></div></figure><p name="a02e" id="a02e" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">Faster_rcnn_resnet101_coco_2018_01_28 output (</strong>using the same client)<a href="http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet50_coco_2018_01_28.tar.gz" data-href="http://download.tensorflow.org/models/object_detection/faster_rcnn_resnet50_coco_2018_01_28.tar.gz" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">[model zoo link]</a></p><figure name="021d" id="021d" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 466px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 66.60000000000001%;"></div><img class="graf-image" data-image-id="1*KgJN0r2SmlpPQwhv243ljw.png" data-width="1201" data-height="800" src="https://cdn-images-1.medium.com/max/800/1*KgJN0r2SmlpPQwhv243ljw.png"></div><figcaption class="imageCaption">pretty good detections (RGB,800*x)</figcaption></figure><figure name="8591" id="8591" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 502px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 71.7%;"></div><img class="graf-image" data-image-id="1*u2crV3_Zp1mCgjQRPnOmFA.png" data-width="1115" data-height="800" src="https://cdn-images-1.medium.com/max/800/1*u2crV3_Zp1mCgjQRPnOmFA.png"></div><figcaption class="imageCaption">some false positives when objects are bigger- RGB,800*x</figcaption></figure><p name="0c49" id="0c49" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03</strong> output ( from same client)[Model zoo <a href="http://download.tensorflow.org/models/object_detection/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.tar.gz" data-href="http://download.tensorflow.org/models/object_detection/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03.tar.gz" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">link</a>]</p><figure name="7ec5" id="7ec5" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 464px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 66.3%;"></div><img class="graf-image" data-image-id="1*tWaO35yrnjCXtV-ADoXGTQ.png" data-width="1206" data-height="800" src="https://cdn-images-1.medium.com/max/800/1*tWaO35yrnjCXtV-ADoXGTQ.png"></div><figcaption class="imageCaption">RGB input</figcaption></figure><figure name="ebca" id="ebca" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 466px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 66.60000000000001%;"></div><img class="graf-image" data-image-id="1*KhYF-rCTNcFhNt3WjV09yQ.png" data-width="1201" data-height="800" src="https://cdn-images-1.medium.com/max/800/1*KhYF-rCTNcFhNt3WjV09yQ.png"></div><figcaption class="imageCaption">not very accurate for smaller objects</figcaption></figure><p name="1fd2" id="1fd2" class="graf graf--p graf-after--figure"><strong class="markup--strong markup--p-strong">ssd_inception_v2_coco_2018_01_28 output [model zoo </strong><a href="http://download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2018_01_28.tar.gz" data-href="http://download.tensorflow.org/models/object_detection/ssd_inception_v2_coco_2018_01_28.tar.gz" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank"><strong class="markup--strong markup--p-strong">link</strong></a><strong class="markup--strong markup--p-strong">]</strong></p><figure name="f6c3" id="f6c3" class="graf graf--figure graf-after--p"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 466px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 66.60000000000001%;"></div><img class="graf-image" data-image-id="1*PPI7qQqueJDmXBMXg_TPMw.png" data-width="1201" data-height="800" src="https://cdn-images-1.medium.com/max/800/1*PPI7qQqueJDmXBMXg_TPMw.png"></div><figcaption class="imageCaption">RGB input</figcaption></figure><figure name="8d1b" id="8d1b" class="graf graf--figure graf-after--figure"><div class="aspectRatioPlaceholder is-locked" style="max-width: 700px; max-height: 464px;"><div class="aspectRatioPlaceholder-fill" style="padding-bottom: 66.3%;"></div><img class="graf-image" data-image-id="1*qXfCTJHMZIFR2WpNimEerQ.jpeg" data-width="1206" data-height="800" src="https://cdn-images-1.medium.com/max/800/1*qXfCTJHMZIFR2WpNimEerQ.jpeg"></div><figcaption class="imageCaption">RGB input</figcaption></figure><p name="035c" id="035c" class="graf graf--p graf-after--figure">The post is about how to productize the client and server parts of the Deep Learning model, created by scientists/data scientists — when SW Engineering meets AI, like DevOps maybe DevAI (as Dev-DataSceientis is a mouthful. Hope this helps.</p><p name="ba07" id="ba07" class="graf graf--p graf-after--p">Next post I will talk about optimizing the TF Served models for different GPU’s (especially those having TensorCores) and how one can do that without knowing anything deep about the model.</p><h4 name="ed30" id="ed30" class="graf graf--h4 graf-after--p">References</h4><div name="679c" id="679c" class="graf graf--mixtapeEmbed graf-after--h4"><a href="https://github.com/alexcpn/tf_serving_clients" data-href="https://github.com/alexcpn/tf_serving_clients" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/alexcpn/tf_serving_clients"><strong class="markup--strong markup--mixtapeEmbed-strong">alexcpn/tf_serving_clients</strong><br><em class="markup--em markup--mixtapeEmbed-em">TF Serving Clients. Contribute to alexcpn/tf_serving_clients development by creating an account on GitHub.</em>github.com</a><a href="https://github.com/alexcpn/tf_serving_clients" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="fe083185ff54ffd478925d2b81e7cf29" data-thumbnail-img-id="0*9wY4zT6dD4QhyV3O" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*9wY4zT6dD4QhyV3O);"></a></div><div name="db4e" id="db4e" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed"><a href="https://medium.com/techlogs/image-pre-processing-for-tf-serving-via-opencv-pillow-tensorflow-tf-image-decode-876c8a14c67a" data-href="https://medium.com/techlogs/image-pre-processing-for-tf-serving-via-opencv-pillow-tensorflow-tf-image-decode-876c8a14c67a" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://medium.com/techlogs/image-pre-processing-for-tf-serving-via-opencv-pillow-tensorflow-tf-image-decode-876c8a14c67a"><strong class="markup--strong markup--mixtapeEmbed-strong">Image pre-processing for TF Serving via OpenCV, Pillow, TensorFlow tf.image.decode*</strong><br><em class="markup--em markup--mixtapeEmbed-em">Example Input Images</em>medium.com</a><a href="https://medium.com/techlogs/image-pre-processing-for-tf-serving-via-opencv-pillow-tensorflow-tf-image-decode-876c8a14c67a" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="435c599e020e436d8cabf7c20beeee64" data-thumbnail-img-id="1*G1brFgvQ7g13wHtl54pvqg.jpeg" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*G1brFgvQ7g13wHtl54pvqg.jpeg);"></a></div><div name="1c0d" id="1c0d" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed"><a href="https://towardsdatascience.com/using-tensorflow-serving-grpc-38a722451064" data-href="https://towardsdatascience.com/using-tensorflow-serving-grpc-38a722451064" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://towardsdatascience.com/using-tensorflow-serving-grpc-38a722451064"><strong class="markup--strong markup--mixtapeEmbed-strong">Using Tensorflow Serving GRPC</strong><br><em class="markup--em markup--mixtapeEmbed-em">Once you have your Tensorflow or Keras based model trained, one needs to think on how to use it in production. You may…</em>towardsdatascience.com</a><a href="https://towardsdatascience.com/using-tensorflow-serving-grpc-38a722451064" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="2c3a019684fbe53c06f39167c38b1273" data-thumbnail-img-id="1*4ls2-f3QSzaVu4RFGxCW8A.png" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/1*4ls2-f3QSzaVu4RFGxCW8A.png);"></a></div><div name="f968" id="f968" class="graf graf--mixtapeEmbed graf-after--mixtapeEmbed graf--trailing"><a href="https://hackernoon.com/is-object-detection-a-done-deal-59a7be913fd2" data-href="https://hackernoon.com/is-object-detection-a-done-deal-59a7be913fd2" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://hackernoon.com/is-object-detection-a-done-deal-59a7be913fd2"><strong class="markup--strong markup--mixtapeEmbed-strong">Is Object Detection a Done Deal?</strong><br><em class="markup--em markup--mixtapeEmbed-em">A few years back it was widely known that Object Detection was a hard problem to solve. The comic below was just a few…</em>hackernoon.com</a><a href="https://hackernoon.com/is-object-detection-a-done-deal-59a7be913fd2" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="73fdc6f85feb8233a4a782cad4c694e1" data-thumbnail-img-id="0*GoXh814mdyi4aDul" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*GoXh814mdyi4aDul);"></a></div></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@alexcpn" class="p-author h-card">Alex Punnen</a> on <a href="https://medium.com/p/69e191cb1f37"><time class="dt-published" datetime="2019-04-01T13:11:24.872Z">April 1, 2019</time></a>.</p><p><a href="https://medium.com/@alexcpn/productising-tensorflow-keras-models-via-tensorflow-serving-69e191cb1f37" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on August 22, 2019.</p></footer></article></body></html>